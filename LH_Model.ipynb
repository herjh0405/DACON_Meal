<<<<<<< HEAD
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LH_Model.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":["U0l0Cf0Sj6MD","yQYZdKJv-nPm","FQowLejR_SD6","c8lX6Sst_c8F","pAaGu6sRVOR1","w9LUcbkNtWaP","iUKXq4Q1fA3p","vsnRgKfHqEPv","wvg8ZOEUv8u7"],"mount_file_id":"1l2NH5yafpIojJ-f0dM0WEDv-rwEiHKNw","authorship_tag":"ABX9TyOe+rLsANBjQ5oNNgE60QLs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"SrP5riasfpzN"},"source":["!pip install catboost\n","!pip install pycaret\n","!pip install kaggler\n","!pip install pendulum\n","!pip install flaml\n","!pip install shap"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KKXMNXLhfsoG"},"source":["import pandas as pd\n","import numpy as np\n","import datetime as dt\n","import datetime\n","np.random.seed(0)\n","\n","from pycaret.regression import *\n","from kaggler.preprocessing import LabelEncoder\n","from sklearn.metrics import roc_auc_score, log_loss\n","\n","from tqdm.notebook import tqdm\n","import os, re\n","import glob\n","import calendar\n","\n","from flaml import AutoML\n","import statsmodels.api as sm\n","import pendulum"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-qUC3VbFfuP2"},"source":["# 한글 폰트 사용\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import matplotlib.font_manager as fm\n","import os\n","\n","def change_matplotlib_font(font_download_url):\n","    FONT_PATH = 'MY_FONT'\n","    \n","    font_download_cmd = f\"wget {font_download_url} -O {FONT_PATH}.zip\"\n","    unzip_cmd = f\"unzip -o {FONT_PATH}.zip -d {FONT_PATH}\"\n","    os.system(font_download_cmd)\n","    os.system(unzip_cmd)\n","    \n","    font_files = fm.findSystemFonts(fontpaths=FONT_PATH)\n","    for font_file in font_files:\n","        fm.fontManager.addfont(font_file)\n","\n","    font_name = fm.FontProperties(fname=font_files[2]).get_name()\n","    matplotlib.rc('font', family=font_name)\n","    print(\"font family: \", plt.rcParams['font.family'])\n","\n","font_download_url = \"https://fonts.google.com/download?family=Nanum%20Gothic\"\n","change_matplotlib_font(font_download_url)\n","# 마이너스 폰트 깨짐 방지\n","plt.rcParams['axes.unicode_minus'] = False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U0l0Cf0Sj6MD"},"source":["### Load Data"]},{"cell_type":"code","metadata":{"id":"p3PKLkpn7Pmj"},"source":["# path = '/content/drive/MyDrive/구내식당/water/'\n","# train = pd.read_csv(path+'train.csv')\n","# test = pd.read_csv(path+'test.csv')\n","# holiday = pd.read_csv(path+'holidays.csv', index_col=0)\n","# corona = pd.read_csv(path+'corona_data.csv')\n","\n","# df = pd.concat([train.iloc[:, :-2], test])\n","# target_df = train.iloc[:, -2:]\n","# df.columns = ['일자', '요일', '정원','휴가자', '출장자', '야근자',\\\n","#                  '재택근무자', '조식', '중식', '석식']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yQYZdKJv-nPm"},"source":["## 메뉴 관련 전처리"]},{"cell_type":"code","metadata":{"id":"5ykDEFEb-1VI"},"source":["# Menu-extracting function\n","def extractMenu(array, keywords=[], not_in_keywords={}, comm_not_in=[]):\n","  extractedMenu = []\n","  for menu_nm in array:\n","    for kw in keywords:\n","      if menu_nm.find(kw) > -1:\n","        has_not_in = False\n","        if kw in not_in_keywords:\n","          for sub_kw in not_in_keywords[kw]:\n","            if menu_nm.find(sub_kw) > -1:\n","              has_not_in = True\n","              break\n","        for sub_kw in comm_not_in:\n","            if menu_nm.find(sub_kw) > -1:\n","              has_not_in = True\n","              break\n","\n","        if not has_not_in:\n","          extractedMenu.append(menu_nm)\n","          break\n","  return(extractedMenu)\n","\n","def extractMenu2(array, keywords=[]):\n","  extractedMenu = []\n","  for menu_nm in tot_menu_arr:\n","    for kw in keywords:\n","      if menu_nm.find(kw) > -1:\n","        menu_nm_list = re.split(r'[^\\w]', menu_nm)\n","        for menu_nm_tmp in menu_nm_list:\n","          if menu_nm_tmp.find(kw) + len(kw) == len(menu_nm_tmp): # 끝에 있으면\n","            extractedMenu.append(menu_nm)\n","        break\n","  return(extractedMenu)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8MUhPAob-3kg"},"source":["lunch_menu_data = df['중식']\n","dinner_menu_data = df['석식']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"61mXHJgI-3Sz"},"source":["tot_menu_arr = []\n","pattern = r\"\\(.*\\)\"\n","for menu_data in [lunch_menu_data, dinner_menu_data]:\n","  for daily_menu in menu_data:\n","    menu_list = daily_menu.strip().split()\n","    menu_list2 = []\n","    for i, menu_nm in enumerate(menu_list):\n","      menu_nm = re.sub(pattern, '', menu_nm)\n","      if menu_nm.strip() in ['', '*']:\n","        continue\n","      if menu_nm[0] == '(' or menu_nm[-1] == ')':\n","        continue\n","      menu_list2.append(menu_nm)\n","    tot_menu_arr += menu_list2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xjBtsk_j-2xT"},"source":["tot_menu_arr = set(tot_menu_arr)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SsTj9nrV--xG"},"source":["len(tot_menu_arr)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OEjJtWFW--uV"},"source":["# 육류 분류\n","# 소고기\n","# https://namu.wiki/w/%EC%87%A0%EA%B3%A0%EA%B8%B0\n","beef = ['소고기', '쇠고기', '불고기', '떡갈비', '갈비찜', '소갈비', '육사시미', '육회', '장조림', '와규', '야키니쿠', '규동', '스테이크', '햄버그 스테이크',\n"," '함박스테이크', '함바그스테이크', '함박 스테이크', '햄버거', '로스트 디너', '비프가스밀라네사', '웰링턴', '슈하스쿠', '아사도', '우육면',\n"," '육개장', '육포', '평양냉면', '비프 스트로가노프', '설렁탕', '소고기국', '소머리국밥', '곰탕', '너비아니', '보르챠', '소꼬리']\n","# 돼지고기\n","# https://namu.wiki/w/%EB%8F%BC%EC%A7%80%EA%B3%A0%EA%B8%B0\n","pig = ['돼지', '돼지머리', '머릿고기', '뒷고기', '관자살', '콧등살', '삼각살', '설중살', '설하살', '안중살', '뽈항정살',\n"," '볼살', '두항정', '돼지코', '항정살', '목살', '가브리살', '갈비', '앞다리살', '갈매기살', '등심', '안심',\n"," '삼겹살', '오겹살', '뒷다리살', '돈족', '내장', '오소리감투', '허파', '염통', '콩팥', '새끼보', '돈낭',\n"," '돈족', '돼지꼬리', '사태', '막창', '감자탕', '돈가스', '돼지갈비', '돼지국밥', '돼지불고기', '두루치기', '순대',\n"," '순댓', '족발', '보쌈', '수육', '편육', '제육', '탕수육', '삼겹', '맥적', '차슈', '향우구육', '꿔바로우', '훙사오러우',\n"," '회과육', '동파육', '라후테', '오향장육', '슈바인스학세', '소시지', '소세지', '포크 커틀릿', '함바그 스테이크', '함바그스테이크',\n"," '함박스테이크', '살스테이크','살 스테이크', '함박 스테이크', '베이컨', '햄', '스팸', '폭립', '폭찹', '돈지루', '부타동', '바쿠테', '팟 카파오 무 쌉', '비엔나', '소떡', '육']\n","# 닭고기\n","# https://namu.wiki/w/%EB%8B%AD%EA%B3%A0%EA%B8%B0\n","chicken = ['닭', '깐풍기', '꼬꼬면', '궁보계정', '간장닭', '기스면', '계', '도빙무시', '라조기', '백숙', '영계백숙',\n"," '불닭', '삼계탕', '삼계선', '오니시메', '옻닭', '연팔기', '유린기', '육회', '좌종당계', '찜닭', '초계밀면',\n"," '치킨', '도리텐', '지파이', '치짜', '취계', '카라아게', '가라아', '파닭', '양파닭', '케밥', '코코뱅', '탕수기',\n"," '포계', '프랑구 아사두']\n","# 양고기\n","# https://namu.wiki/w/%EC%96%91%EA%B3%A0%EA%B8%B0\n","sheep = ['양고기','훠궈', '양꼬치', '케밥', '샤슬릭', '징기스칸', '셰퍼드 파이', '허르헉', '양갈비']\n","# 오리고기\n","# https://namu.wiki/w/%EC%98%A4%EB%A6%AC%EA%B3%A0%EA%B8%B0\n","dug = ['오리']\n","\n","web_keywords = beef + pig + chicken + sheep + dug\n","keywords = ['돈까스', '히레카츠', '히레까쓰', '히레가스', '포크', '부대찌개', '뒷다리', '앞다리', '돈', '순살',\n","                '소머리', '등뼈', '곱창', '도가니', '뼈해장국', '뼈다귀해장국', '목심', '채끝', '우둔', '양지', '설도', '만두', '만둣',\n","                '잡채', '류산슬', '유산슬', '고기', '고깃']\n","keywords += web_keywords\n","\n","not_in_keywords = {'오리':['아오리', '오리엔탈'], '계':['계란', '계발', '계피'], '장조림':['계란', '메추리알'], '치킨':['치킨무'], '돈':['돈나물'], '만두':['당면계란'], '만둣':['당면계란']}\n","meat_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords=not_in_keywords, comm_not_in=[])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u-I02EzT--qc"},"source":["# 돼지고기\n","keywords = ['돼지', '돼지머리', '머릿고기', '뒷고기', '관자살', '콧등살', '삼각살', '설중살', '설하살', '안중살', '뽈항정살',\n"," '볼살', '두항정', '돼지코', '항정살', '목살', '가브리살', '앞다리살', '갈매기살', '등심', '안심',\n"," '삼겹살', '오겹살', '앞다리살', '뒷다리살', '돈족', '내장', '오소리감투', '허파', '염통', '콩팥', '새끼보', '돈낭',\n"," '돈족', '돼지꼬리', '사태', '막창', '감자탕', '돈가스', '돼지갈비', '돼지국밥', '돼지불고기', '두루치기', '순대',\n"," '순댓', '족발', '보쌈', '수육', '편육', '제육', '탕수육', '삼겹', '맥적', '차슈', '향우구육', '꿔바로우', '훙사오러우',\n"," '회과육', '동파육', '라후테', '오향장육', '슈바인스학세', '소시지', '소세지', '포크 커틀릿',\n"," '목살스테이크','목살 스테이크', '베이컨', '햄', '스팸', '폭립', '폭찹', '돈지루', '부타동', '바쿠테', '팟 카파오 무 쌉', '비엔나', '소떡',\n"," '돈까스', '히레카츠', '히레까쓰', '히레가스', '포크', '돈', '등뼈', '뼈해장국', '뼈다귀해장국']\n","not_in_keywords = {'돈':['돈나물'], '만두':['당면계란'], '만둣':['당면계란']}\n","pig_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords=not_in_keywords, comm_not_in=[])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qkndYkKF--mD"},"source":["# 소고기\n","keywords = ['소고기', '쇠고기', '소불고기', '소갈비', '육사시미', '육회', '와규', '야키니쿠', '규동', '소곱창',\n","            '로스트 디너', '비프가스밀라네사', '웰링턴', '슈하스쿠', '아사도', '우육면',\n","            '육개장', '육포', '평양냉면', '비프 스트로가노프', '설렁탕', '소고기국', '소머리국밥', '곰탕', '너비아니', '보르챠', '소꼬리', '소머리', '설도', '목심', '채끝', '우둔', '양지', '도가니']\n","beef_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords={}, comm_not_in=[])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zu5btCQl_ExN"},"source":["# 닭고기\n","keywords = ['닭', '깐풍기', '꼬꼬면', '궁보계정', '간장닭', '기스면', '계', '도빙무시', '라조기', '백숙', '영계백숙',\n","          '불닭', '삼계탕', '삼계선', '오니시메', '옻닭', '연팔기', '유린기', '육회', '좌종당계', '찜닭', '초계밀면',\n","          '치킨', '도리텐', '지파이', '치짜', '취계', '카라아게', '가라아', '파닭', '양파닭', '케밥', '코코뱅', '탕수기',\n","          '포계', '프랑구 아사두']\n","not_in_keywords = {'계':['계란', '계발', '계피'], '치킨':['치킨무']}\n","\n","chicken_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords=not_in_keywords, comm_not_in=[])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EyBscZPX_Eq5"},"source":["# 양고기 - 데이터 없어서 제외\n","keywords = ['양고기','훠궈', '양꼬치', '케밥', '샤슬릭', '징기스칸', '셰퍼드 파이', '허르헉', '양갈비']\n","sheep_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords={}, comm_not_in=[])\n","sheep_menus"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"29errrks_Em8"},"source":["# 오리고기\n","keywords = ['오리']\n","not_in_keywords = {'오리':['아오리', '오리엔탈']}\n","duck_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords=not_in_keywords, comm_not_in=[])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oIiPZsJq_Eik"},"source":["#난류 (계란)\n","keywords = ['계란', '난', '란', '메추리알', '날치알', '동태알']\n","not_in_keywords = {\"란\":['토란'], '난':['커리', '카레']}\n","egg_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords=not_in_keywords, comm_not_in=[])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pCvsvlx8_Ed1"},"source":["# 죽류\n","keywords = ['죽', '누룽지']\n","juk_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords={}, comm_not_in=[])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8vfiEY18_EX2"},"source":["# 덮밥 및 국밥류\n","keywords = ['덮밥', '국밥']\n","gukbob_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords={}, comm_not_in=[])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iNL-w6Fm--hI"},"source":["# 비빔밥 및 볶음밥류\n","keywords = ['비빔밥', '볶음밥']\n","bb_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords={}, comm_not_in=[])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5i8uzCd2_NQ-"},"source":["# 국탕류\n","keywords = ['국', '탕', '찌개', '국물']\n","soup_menus = extractMenu2(tot_menu_arr, keywords=keywords)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cJtEwngK_Ndc"},"source":["# 구이류\n","keywords = ['구이']\n","gui_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords={}, comm_not_in=[])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hx1ksLEc_NlR"},"source":["# 전류\n","keywords = ['전', '부침개', '빈대떡']\n","jeon_menus = extractMenu2(tot_menu_arr, keywords=keywords)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bDXN4Jft_Nrt"},"source":["# http://yaksik.net/detail.php?number=24904\n","# 튀김류\n","keywords = ['튀김', '까스', '카츠', '가츠', '까츠', '탕수', '덴뿌라', '덴푸라', '크로켓', '고로케', '맛탕', '치킨', '통닭', '부각', '강정', '김말이', '깐풍']\n","fry_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords={}, comm_not_in=[])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FQowLejR_SD6"},"source":["## 메뉴 추가 특성 - Part1"]},{"cell_type":"code","metadata":{"id":"SCcKhNAJ_SAN"},"source":["# 곡물\n","keywords = ['현미', '밥', '쌀', '보리', '죽', '참깨', '들깨', '수수', '잡곡', '귀리', '퀴노아', '아마란스', '옥수수', '기장', '메밀', '모밀']\n","grain_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords={}, comm_not_in=[])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fxOcmQZV_R-G"},"source":["# 콩류\n","keywords = ['콩', '녹두', '팥', '완두']\n","not_in_keywords = {'콩':['콩나물']}\n","bean_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords=not_in_keywords, comm_not_in=[])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DmqzkA84_R6d"},"source":["# 묵\n","keywords = ['묵']\n","not_in_keywords = {'묵':['어묵', '묵은지']}\n","kor_jelly_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords=not_in_keywords, comm_not_in=[])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cQlE4n5R_R3D"},"source":["# 생선 및 조개류\n","# https://ko.wikipedia.org/wiki/%EC%83%9D%EC%84%A0\n","# https://namu.wiki/w/%EC%83%9D%EC%84%A0\n","# https://namu.wiki/w/%EC%A1%B0%EA%B0%9C\n","keywords = ['생선', '조개', '메기', '송어', '오징어', '굴', '멸치', '숭어', '성게', '고등어', '명태',\n","            '쏨뱅이', '연어', '틸라피아', '우럭', '이리치', '가재', '참바리', '상어', '돔',\n","            '삼치', '방어', '참치', '새우', '문어', '홍어', '농어', '붉평치', '청상아리', '황새치',\n","            '다랑어', '비막치어', '장어', '녹새치', '숭어', '굴비', '조기', '갈치', '꽁치',\n","            '전어', '명태', '노가리', '황태', '은어', '가물치', '쏘가리', '붕어', '잉어', '모래마주', '가자미',\n","            '간재미', '가오리', '박대', '양미리', '과메기', '청어', '생태',\n","            '개복치', '광어', '넙치', '기름치', '까나리', '날치','놀래미'\n","            ,'능성어','달고기','대구','도다리','도루묵','도미','독가시치'\n","            ,'만새기','망상어','문절망둑','물메기','미꾸라지','민어','방어'\n","            ,'추어탕','배스','밴댕이','뱅어','벵에돔','병어','보리멸'\n","            ,'복어','볼락','부세','부시리','붕장어','블루길'\n","            ,'빙어','산천어','서대','시샤모','쏘가리','쏠배감펭','쏨뱅이'\n","            ,'아귀','아구','임연수','전갱이','전복치','점성어','정어리'\n","            ,'준치','쥐치','청새치','청어','향어','홍어','황새치','매운탕'\n","            ,'루테피스크','게맛살','물회','회덮밥','부야베스','북엇국','세꼬시','수르스트뢰밍','식해','어묵','오뎅'\n","            ,'쥐포','추어탕','피시 앤드 칩스','피쉬 앤드 칩스','피시앤드칩스','피쉬앤드칩스','피시앤칩스','피쉬앤칩스','해물'\n","            ,'가리비', '개오지', '꼬막','대칭이','바지락','백합','홍합','소라', '골뱅이', '고둥','재첩'\n","            ,'전복','플라티케라무스', '봉골레', '클램차우더']\n","not_in_keywords = {'굴':['굴소스'], '새우':['새우젓']}\n","fish_shell_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords=not_in_keywords, comm_not_in=[])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BAKGLplP_R07"},"source":["# 채소류\n","# https://namu.wiki/w/%EC%B1%84%EC%86%8C?from=%EC%95%BC%EC%B1%84\n","keywords = ['가지', '갓', '감자', '고구마', '고사리', '고추', '페페론치노', '냉이', '근대', '깻잎', '차조기'\n","            , '당근', '더덕', '도라지', '동아', '딸기', '마', '마늘', '멜론', '무', '무청'\n","            , '바나나', '배추', '버섯', '부추', '브로콜리', '상추', '생강', '쇠비름', '나물'\n","            , '쑥', '시금치', '수박', '시호', '아스파라거스', '야콘', '양파', '여주', '연근', '열무', '오이'\n","            , '우엉', '인삼', '죽순', '청경채', '참외', '칡', '풋콩', '토란', '토마토', '쪽파', '대파', '파인애플'\n","            , '파프리카', '피망', '케일', '고수', '로즈마리', '루타바가', '바질', '박하', '산마늘', '셀러리'\n","            , '아티초크', '타임', '파슬리', '호박', '피클', '파채', '파김치', '채소', '야채']\n","not_in_keywords = {'무':['무침'], '마':'마카로니', '고추':['고추장']}\n","vegetable_menus =  extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords=not_in_keywords, comm_not_in=[])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ajuoyOO9_RxJ"},"source":["# 해조류\n","# https://namu.wiki/w/%EC%A1%B0%EB%A5%98(%EC%88%98%EC%A4%91%EC%83%9D%EB%AC%BC)?from=%ED%95%B4%EC%A1%B0%EB%A5%98\n","keywords = ['김', '우뭇가사리', '한천', '매생이', '파래', '바다포도', '해캄', '클로렐라', '청각', '마리모모스볼', '다시마', '미역', '감태', '톳']\n","not_in_keywords = {'김':['김치', '튀김', '김칫']}\n","sea_alg_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords=not_in_keywords, comm_not_in=[])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A2tDD97z_Rtq"},"source":["# 쌀케익 - 없어서 패스"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GB4-DOpW_RrS"},"source":["# 발효된 콩 상품 -> 장류\n","# https://namu.wiki/w/%EC%9E%A5%EB%A5%98\n","keywords = ['된장', '간장', '쯔유', '노추', '미소', '고추장', '청국장', '담북장', '팥장', '두부장', '비지장', '어육장', '춘장', '마장', '낫토', '두반장', '해선장', '굴소스', '게장',\n"," '장조림', '양념장', '장국', '쌈장', '초장', '*장']\n","jang_menus  = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords={}, comm_not_in=[])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lE6zUsAp_Ro6"},"source":["# 김치\n","# https://namu.wiki/w/%EA%B9%80%EC%B9%98\n","keywords = ['김치', '깍두기', '석박지', '동치미', '겉절이', '묵은지', '소박이', '섞박지', '생채', '게국지', '김칫']\n","kimchi_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords={}, comm_not_in=[])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3c1XH9S7_Rmf"},"source":["# 만두\n","# https://namu.wiki/w/%EB%A7%8C%EB%91%90\n","keywords = ['만두', '춘권', '만쥬', '사모사', '만둣']\n","mandu_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords={}, comm_not_in=[])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NCwa_vM6_cs9"},"source":["# 곡물 가루(밀가루, 쌀가루 등 전분)\n","# https://namu.wiki/w/%EB%B0%80%EA%B0%80%EB%A3%A8\n","# 미숫가루\n","keywords = [\"면\", \"수제비\", \"전\", \"부침개\", \"빵\", \"춘권\", \"튀김\", \"과자\", \"국수\", \"메밀\", \"모밀\", \"피자\", \"전병\", \"떡\", \"어묵\", \"오뎅\", \"소시지\", \"소세지\", \"햄\", \"김밥\", \n","            \"부대찌개\", \"스콘\", \"만두\", \"파이\", \"빈대떡\", \"케이크\", \"케익\", \"쿠키\", \"핫도그\", \"파스타\", \"치킨\", \"라자냐\", \"팟타이\", \"나쵸\", \"팝콘\", '스파게티', '짬뽕']\n","not_in_keywords = {'치킨':['치킨무'], '전':['전주식'], '짬뽕':['고기', '찌개', '국']}\n","powder_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords=not_in_keywords, comm_not_in=[])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BXRc_I7B_c4Y"},"source":["# 과일\n","# https://namu.wiki/w/%EA%B3%BC%EC%9D%BC\n","# https://namu.wiki/w/%EC%88%98%EC%9E%85%20%EA%B3%BC%EC%9D%BC\n","keywords = ['구기자','매실','무화과','버찌','체리','복분자','복숭아','블랙베리','블루베리','딸기','살구','앵두','자두','포도'\n","            ,'감','다래','대추','머루','모과','무화과','배','사과','석류','으름','귤','유자','레드향','천혜향','한라봉'\n","            ,'과라나','구아바','구즈베리','토마토','나랑히야','노니','노팔','니파팜','두꾸','두리안','라임','람부탄'\n","            ,'레몬','애플','루비솔트부쉬','리치','여지','마랑','마룰라','마르멜로','마프랑','망고','블랙베리','아보카도'\n","            ,'아로니아','아사이베리','아사이 베리','양초열매','오렌지','올리브','용안','롱간','자몽','바나나','딸기','수박'\n","            ,'참외','멜론','메론','여주','파인애플','토마토','코코넛','크랜베리','타마린드','파파야','패션프루트','패션후르츠']\n","not_in_keywords = {'살구':['구이', '목살', '삼겹살', '가브리살', '갈비살', '항정살'], '감':['감자'], '배':['배추', '알배기', '소배기']}\n","comm_not_in = ['주스', '쥬스', '음료', 'D', '순']\n","fruit_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords=not_in_keywords, comm_not_in=comm_not_in)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c8lX6Sst_c8F"},"source":["## 메뉴 추가 특성 - Part2"]},{"cell_type":"code","metadata":{"id":"4V-tLXEM_c_v"},"source":["# 쌀\n","# https://namu.wiki/w/%EA%B3%A1%EB%AC%BC\n","\n","keywords = ['쌀', '잡곡', '오곡', '현미', '흑미', '귀리', '차조', '렌틸콩', '강낭콩', '병아리콩', '완두콩', '기장', '보리', '수수', '호밀'] \n","not_in_keywords = {'쌀':['쌀국수', '찹쌀'], '기장':['장조림'], '수수':['옥수수', '부꾸미']} # 찹쌀은 밥 메뉴명에 쓰이지 않아 삭제\n","comm_not_in = ['스프']\n","rice_menus = extractMenu(tot_menu_arr, keywords, not_in_keywords, comm_not_in)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"er5kRW4h_dCS"},"source":["# 김밥 및 초밥\n","\n","keywords = ['김밥', '초밥'] \n","not_in_keywords = {'김밥':['볶음밥']}\n","comm_not_in = []\n","gimbab_menus = extractMenu(tot_menu_arr, keywords, not_in_keywords, comm_not_in)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kYnsISbF_dGS"},"source":["# 소금, 식초 등에 절인 해산물\n","# 해산물이 들어있지 않은 절임류도 포함시킴\n","\n","keywords = ['절임', '젓'] \n","not_in_keywords = {}\n","comm_not_in = []\n","saused_menus = extractMenu(tot_menu_arr, keywords, not_in_keywords, comm_not_in)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GCIwG9SL_dHy"},"source":["# 면류\n","# https://femiwiki.com/w/%EB%B6%84%EB%A5%98:%EC%A2%85%EB%A5%98/%EB%A9%B4%EC%9A%94%EB%A6%AC\n","keywords = ['국수', '면', '파스타', '스파게티', '짬뽕', '라면'] \n","not_in_keywords = {'짬뽕':['고기', '찌개', '국']}\n","comm_not_in = []\n","noodle_menus = extractMenu(tot_menu_arr, keywords, not_in_keywords, comm_not_in)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ryTAAXbf_dKh"},"source":["# 스튜 - 조림과 찌개의 중간단계\n","# https://namu.wiki/w/%EC%8A%A4%ED%8A%9C\n","keywords = ['스튜', '조림'] \n","not_in_keywords = {}\n","comm_not_in = []\n","stew_menus = extractMenu(tot_menu_arr, keywords, not_in_keywords, comm_not_in)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FFCcLTq5_dNi"},"source":["# 한국 전통 샐러드\n","keywords = ['나물', '무침'] \n","not_in_keywords = {'나물':['콩나물', '밥']}\n","comm_not_in = []\n","namul_menus = extractMenu(tot_menu_arr, keywords, not_in_keywords, comm_not_in)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FadGmqSj_dPu"},"source":["# 피클\n","keywords = ['피클'] \n","not_in_keywords = {}\n","comm_not_in = []\n","pickle_menus = extractMenu(tot_menu_arr, keywords, not_in_keywords, comm_not_in)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j2EvAh24_dSH"},"source":["# 뚝배기 - 없음 -> 제외\n","keywords = ['뚝배기', '돌솥'] \n","not_in_keywords = {}\n","comm_not_in = []\n","dduk_menus = extractMenu(tot_menu_arr, keywords, not_in_keywords, comm_not_in)\n","\n","dduk_menus"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"61i-6POx_dV8"},"source":["# 샐러드\n","keywords = ['샐러드'] \n","not_in_keywords = {}\n","comm_not_in = []\n","salad_menus = extractMenu(tot_menu_arr, keywords, not_in_keywords, comm_not_in)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pa9wDbZ3_dX_"},"source":["# 우유\n","# 우유가 들어간 식재료(크림, 요거트 등)종류로 변경\n","keywords = ['까르보나라', '크림', '요거트'] \n","not_in_keywords = {}\n","comm_not_in = ['샐러드', 'D', '드레싱', '소스'] # 샐러드 드레싱, 디핑소스 제외\n","milk_menus = extractMenu(tot_menu_arr, keywords, not_in_keywords, comm_not_in)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vjyIqm7P_daU"},"source":["# 빵, 쿠키\n","# https://ko.wikipedia.org/wiki/%EB%B9%B5_%EB%AA%A9%EB%A1%9D\n","keywords = ['와플', '케이크', '케잌', '바게트', '도넛', '도너츠', '핫도그', '도라야키', '베이글', '번', '비스킷', '스콘', '토스트', '브레드', '포카차', '피자', '호두과자', '쿠키'] \n","not_in_keywords = {}\n","comm_not_in = []\n","bread_menus = extractMenu(tot_menu_arr, keywords, not_in_keywords, comm_not_in)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dmnB-rio_qz0"},"source":["# 음료\n","# https://ko.wikipedia.org/wiki/%EB%B9%B5_%EB%AA%A9%EB%A1%9D\n","keywords = ['주스', '쥬스', '수정과', '식혜', '식초', '코코아', '칵테일', '스무디', '우유', '셰이크', '야쿠르트', '요구르트', '커피', '차', '탄산수', '음료'] \n","not_in_keywords = {'차':['차돌']}\n","comm_not_in = []\n","drink_menus = extractMenu(tot_menu_arr, keywords, not_in_keywords, comm_not_in)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fLFMSNCz_q2V"},"source":["def get_food_one_hot(x, menu_array):\n","  menu_list = x.strip().split()\n","  for i, menu_nm in enumerate(menu_list):\n","    menu_nm = re.sub(pattern, '', menu_nm)\n","    if menu_nm.strip() in ['', '*']:\n","      continue\n","    if menu_nm[0] == '(' or menu_nm[-1] == ')':\n","      continue\n","    try:\n","      if menu_array.index(menu_nm) > -1:\n","        return 1\n","    except Exception:\n","      pass\n","  return 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"whUqZvnZ_q4k"},"source":["# 데이터 병합\n","menu_col_nm = ['육류', '난류', '죽류', '덮밥_국밥류', '비빔밥_볶음밥류', '국탕류', '구이류', '전류', '튀김류', '곡물', '콩류',\n","               '묵', '생선_조개류', '채소류', '해조류', '장류', '김치', '만두', '곡물가루', '과일', '쌀', '김밥_초밥', '절임류',\n","               '면류', '스튜', '나물_무침류', '피클', '샐러드', '우유', '빵류', '음료', '돼지고기', '소고기', '닭고기', '오리고기']\n","menu_data_arr = [meat_menus, egg_menus, juk_menus, gukbob_menus, bb_menus, soup_menus, gui_menus, jeon_menus, fry_menus, grain_menus, bean_menus,\n","                 kor_jelly_menus, fish_shell_menus, vegetable_menus, sea_alg_menus, jang_menus, kimchi_menus, mandu_menus, powder_menus, fruit_menus, rice_menus, gimbab_menus, saused_menus,\n","                 noodle_menus, stew_menus, namul_menus, pickle_menus, salad_menus, milk_menus, bread_menus, drink_menus, pig_menus, beef_menus, chicken_menus, duck_menus]\n","\n","for col_type in ['중식메뉴', '석식메뉴']:\n","  for i, menu_arr in enumerate(menu_data_arr):\n","    train[col_type + '_' + menu_col_nm[i]] = train[col_type].apply(lambda x: get_food_one_hot(x, menu_arr))\n","    test[col_type + '_' + menu_col_nm[i]] = train[col_type].apply(lambda x: get_food_one_hot(x, menu_arr))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XgtDxinW_Nv5"},"source":["df = pd.concat([train, test], axis=0).reset_index(drop=True)\n","df = df.fillna(0)\n","df.columns = ['일자', '요일', '정원', '휴가자', '출장자', '야근자', '재택근무자', '조식', '중식', '석식',\\\n","              '중식계', '석식계', '중식메뉴_육류',\n","       '중식메뉴_난류', '중식메뉴_죽류', '중식메뉴_덮밥_국밥류', '중식메뉴_비빔밥_볶음밥류', '중식메뉴_국탕류',\n","       '중식메뉴_구이류', '중식메뉴_전류', '중식메뉴_튀김류', '중식메뉴_곡물', '중식메뉴_콩류', '중식메뉴_묵',\n","       '중식메뉴_생선_조개류', '중식메뉴_채소류', '중식메뉴_해조류', '중식메뉴_장류', '중식메뉴_김치', '중식메뉴_만두',\n","       '중식메뉴_곡물가루', '중식메뉴_과일', '중식메뉴_쌀', '중식메뉴_김밥_초밥', '중식메뉴_절임류', '중식메뉴_면류',\n","       '중식메뉴_스튜', '중식메뉴_나물_무침류', '중식메뉴_피클', '중식메뉴_샐러드', '중식메뉴_우유', '중식메뉴_빵류',\n","       '중식메뉴_음료', '중식메뉴_돼지고기', '중식메뉴_소고기', '중식메뉴_닭고기', '중식메뉴_오리고기', '석식메뉴_육류',\n","       '석식메뉴_난류', '석식메뉴_죽류', '석식메뉴_덮밥_국밥류', '석식메뉴_비빔밥_볶음밥류', '석식메뉴_국탕류',\n","       '석식메뉴_구이류', '석식메뉴_전류', '석식메뉴_튀김류', '석식메뉴_곡물', '석식메뉴_콩류', '석식메뉴_묵',\n","       '석식메뉴_생선_조개류', '석식메뉴_채소류', '석식메뉴_해조류', '석식메뉴_장류', '석식메뉴_김치', '석식메뉴_만두',\n","       '석식메뉴_곡물가루', '석식메뉴_과일', '석식메뉴_쌀', '석식메뉴_김밥_초밥', '석식메뉴_절임류', '석식메뉴_면류',\n","       '석식메뉴_스튜', '석식메뉴_나물_무침류', '석식메뉴_피클', '석식메뉴_샐러드', '석식메뉴_우유', '석식메뉴_빵류',\n","       '석식메뉴_음료', '석식메뉴_돼지고기', '석식메뉴_소고기', '석식메뉴_닭고기', '석식메뉴_오리고기']\n","df.drop(columns=['조식', '중식', '석식'], inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pAaGu6sRVOR1"},"source":["### 외부 데이터 추가"]},{"cell_type":"code","metadata":{"id":"XU9m1cWDgN4R"},"source":["dust_dir = os.path.join(path, '미세먼지_일별')\n","wdata_dir = os.path.join(path, '날씨_시간별')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-HgT1y52gZdD"},"source":["w_attrs = ['강수', '기온', '습도', '강수형태']\n","w_years = os.listdir(wdata_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MH3kshIpgwwy"},"source":["def get_wdata(data_path, dtype='num'):\n","  datetime_list = []\n","  value_list_12 = []\n","  value_list_18 = []\n","  curr_mon = ''\n","\n","  with open(data_path, 'r') as f:\n","    lines = f.readlines()\n","    for i, line in enumerate(lines):\n","      if line.strip() == '':\n","        break\n","      row_data = line.strip().split(',')\n","      row_data = [elem.strip() for elem in row_data]\n","      if i == 0:\n","        curr_mon = row_data[-1].split()[-1][:-2]\n","        continue\n","      if len(row_data) == 1:\n","        curr_mon = row_data[-1].split()[-1][:-2]\n","        continue\n","      r_day, r_hour, r_value = row_data\n","      if r_hour in [\"1200\", \"1800\"]: # 점심 12시, 저녁 6시 기준으로 처리\n","        if r_hour == \"1200\":\n","          datetime_list.append(curr_mon[:4]+'-'+curr_mon[4:]+'-'+str('%02d'%int(r_day)))\n","\n","        if dtype == 'num':\n","          if r_hour == \"1200\":\n","            value_list_12.append(float(r_value))\n","          else:\n","            value_list_18.append(float(r_value))\n","        else:\n","          if r_hour == \"1200\":\n","            value_list_12.append(str(round(float(r_value))))\n","          else:\n","            value_list_18.append(str(round(float(r_value))))\n","          \n","\n","  return datetime_list, value_list_12, value_list_18"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iUoVNeu1gxs2"},"source":["# 강수, 기온, 습도, 강수형태 데이터\n","w_data_rain_12 = []\n","w_data_temp_12 = []\n","w_data_hum_12 = []\n","w_data_rtype_12 = []\n","w_data_rain_18 = []\n","w_data_temp_18 = []\n","w_data_hum_18 = []\n","w_data_rtype_18 = []\n","w_datetime = []\n","\n","for year in w_years:\n","  w_subdir = os.path.join(wdata_dir, year)\n","  file_names = os.listdir(w_subdir)\n","  file_name = \"\"\n","  if year != '2021':\n","    file_name = f'{year}01_{year}12.csv'\n","  else:\n","    file_name = f'{year}01_{year}04.csv'\n","  file_path_rain = os.path.join(w_subdir, '충무공동_강수_'+file_name)\n","  file_path_temp = os.path.join(w_subdir, '충무공동_기온_'+file_name)\n","  file_path_hum = os.path.join(w_subdir, '충무공동_습도_'+file_name)\n","  file_path_rtype = os.path.join(w_subdir, '충무공동_강수형태_'+file_name)\n","\n","  datetime_list_rain, value_list_rain_12, value_list_rain_18 = get_wdata(file_path_rain, dtype='num') # 강수 데이터\n","  datetime_list_temp, value_list_temp_12, value_list_temp_18 = get_wdata(file_path_temp, dtype='num') # 기온 데이터\n","  datetime_list_hum, value_list_hum_12, value_list_hum_18 = get_wdata(file_path_hum, dtype='num') # 습도 데이터\n","  datetime_list_rtype, value_list_rtype_12, value_list_rtype_18 = get_wdata(file_path_rtype, dtype='cat') # 강수형태 데이터\n","  \n","  w_datetime   += datetime_list_rain\n","  w_data_rain_12  += value_list_rain_12\n","  w_data_temp_12  += value_list_temp_12\n","  w_data_hum_12   += value_list_hum_12\n","  w_data_rtype_12 += value_list_rtype_12\n","  w_data_rain_18  += value_list_rain_18\n","  w_data_temp_18  += value_list_temp_18\n","  w_data_hum_18   += value_list_hum_18\n","  w_data_rtype_18 += value_list_rtype_18"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EsORoPLdgygT"},"source":["w_df = pd.DataFrame({'일자':pd.Series(w_datetime, dtype='datetime64[ns]'),\n","                   'rain_lunch':pd.Series(w_data_rain_12, dtype='float'),\n","                   'temp_lunch':pd.Series(w_data_temp_12, dtype='float'),\n","                   'hum_lunch':pd.Series(w_data_hum_12, dtype='float'),\n","                   'rain_type_lunch':pd.Series(w_data_rtype_12, dtype='str'),\n","                   'rain_dinner':pd.Series(w_data_rain_18, dtype='float'),\n","                   'temp_dinner':pd.Series(w_data_temp_18, dtype='float'),\n","                   'hum_dinner':pd.Series(w_data_hum_18, dtype='float'),\n","                   'rain_type_dinner':pd.Series(w_data_rtype_18, dtype='str')})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PBaSMJuAgzRj"},"source":["# 불쾌지수 컬럼 추가\n","# https://dacon.io/competitions/official/235736/codeshare/2753?page=1&dtype=recent\n","w_df['discomfort_index_lunch'] = 1.8*w_df['temp_lunch'] - 0.55*(1-w_df['hum_lunch']/100)*(1.8*w_df['temp_lunch']-26) + 32\n","w_df['discomfort_index_dinner'] = 1.8*w_df['temp_dinner'] - 0.55*(1-w_df['hum_dinner']/100)*(1.8*w_df['temp_dinner']-26) + 32"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jiVnVyYfgz80"},"source":["dust_file_paths = glob.glob(os.path.join(dust_dir, '*.xls'))\n","d_datetime = []\n","d_value1 = []\n","d_value2 = []\n","\n","# 시간별 데이터의 경우 미세먼지 측정값 중 빈 값이 있는 경우가 어느 정도 있어서 배제했습니다.\n","for file_path in dust_file_paths:\n","  date_yyyymm = os.path.splitext(os.path.basename(file_path))[0] # yyyymm\n","  date_year = date_yyyymm[:4]\n","  date_mon = date_yyyymm[4:]\n","  dust_df = None\n","\n","  if date_year == '2021':\n","    dust_df = pd.read_excel(file_path, header=[0, 1], skiprows=3)\n","  else:\n","    dust_df = pd.read_excel(file_path, header=[0, 1])\n","  cols = dust_df.columns\n","  date_col = cols[0]\n","  fine_dust_col = cols[1] # 미세먼지\n","  ufine_dust_col = cols[2] # 초미세먼지\n","\n","  # 해당월의 일수 가져오기\n","  days = calendar.monthrange(int(date_year),int(date_mon))[1] \n","  for day in range(1, days+1):\n","    day_1 = '%02d'%day\n","    curr_day_df =  date_year+ '-' + date_mon + '-' + day_1\n","\n","    row_lunch = dust_df[dust_df[cols[0]] == curr_day_df]\n","    row_dinner = dust_df[dust_df[cols[0]] == curr_day_df]\n","    curr_date = date_year+'-'+date_mon+'-'+day_1\n","  \n","    d_datetime.append(curr_date)\n","    d_value1.append(row_lunch[fine_dust_col].values[0])\n","    d_value2.append(row_lunch[ufine_dust_col].values[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XE3Cg4eUg0kL"},"source":["dust_df = pd.DataFrame({'일자':pd.Series(d_datetime, dtype='datetime64[ns]'),\n","                   'fine_dust':pd.Series(d_value1, dtype='float'),\n","                   'ultra_fine_dust':pd.Series(d_value2, dtype='float')})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_vW5j6_Jg30z"},"source":["df = pd.merge(df, dust_df, on='일자')\n","df = pd.merge(df, w_df, on='일자')\n","\n","# 결측치 근처 관측치로 대체\n","df['fine_dust'][564, 1129] = [36, 23]\n","df['ultra_fine_dust'][234, 235, 564, 654, 1129] = [11, 31, 26, 5, 9]\n","\n","# 저녁 컬럼 삭제\n","# df = df.drop(columns=['rain_dinner', 'temp_dinner', 'hum_dinner', 'rain_type_dinner', 'discomfort_index_dinner'])\n","\n","# 미세먼지 명목변수화\n","df['fine_degree'] = df['fine_dust'].apply(lambda x : 0 if 0<=x<=30 else (1 if 31<=x<=80 else (2 if 81<=x<=150 else 3)))\n","df['ultra_fine_degree'] = df['ultra_fine_dust'].apply(lambda x : 0 if 0<=x<=15 else (1 if 16<=x<=35 else (2 if 36<=x<=75 else 3)))\n","df['fine_degree'] = df.apply(lambda x : max(x['fine_degree'], x['ultra_fine_degree']), axis=1)\n","\n","# 강수량 명목변수화\n","df['rain_degree_lunch'] = df['rain_lunch'].apply(lambda x : 0 if x < 2 else 1)\n","df['rain_degree_dinner'] = df['rain_dinner'].apply(lambda x : 0 if x < 2 else 1)\n","\n","# 불쾌지수 명목변수화 \n","df['discomfort_degree_lunch'] = df['discomfort_index_lunch'].apply(lambda x : 0 if x<68 else (1 if 68<=x<75 else (2 if 75<=x<80 else 3)))\n","df['discomfort_degree_dinner'] = df['discomfort_index_dinner'].apply(lambda x : 0 if x<68 else (1 if 68<=x<75 else (2 if 75<=x<80 else 3)))\n","\n","# 명목변수화하면서 삭제\n","df = df.drop(columns=['rain_lunch', 'temp_lunch', 'hum_lunch', 'rain_type_lunch', 'discomfort_index_lunch',\n","                      'rain_dinner', 'temp_dinner', 'hum_dinner', 'rain_type_dinner', 'discomfort_index_dinner',\n","                      'fine_dust', 'ultra_fine_dust', 'ultra_fine_degree',])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y3yZ6AQZV1sF"},"source":["## 중간 변수 색출"]},{"cell_type":"markdown","metadata":{"id":"pI10OVxAg7Zs"},"source":["### 파생변수"]},{"cell_type":"code","metadata":{"id":"EP2g8FEBAuk3"},"source":["path = '/content/drive/MyDrive/구내식당/water/'\n","df = pd.read_csv(path+'df.csv')\n","train = pd.read_csv(path+'train.csv')\n","# holiday = pd.read_csv(path+'holidays.csv')\n","holiday = pd.read_csv(path+'holidays_old.csv')\n","# corona = pd.read_csv(path+'corona_data.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n4OzChE4TBbs"},"source":["df = df.drop(columns=['중식메뉴_죽류', '중식메뉴_곡물', '중식메뉴_김치', '중식메뉴_쌀', '중식메뉴_나물_무침류', '중식메뉴_피클',\\\n","                      'fine_degree', 'rain_degree_lunch', 'discomfort_degree_lunch', '석식메뉴_죽류', '석식메뉴_곡물',\\\n","                      '석식메뉴_김치', '석식메뉴_쌀', '석식메뉴_나물_무침류', '석식메뉴_피클', 'rain_degree_dinner', 'discomfort_degree_dinner',\\\n","                      '중식메뉴_음료', '석식메뉴_음료'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sHN9PM4zRvhs"},"source":["### 외부 날씨 데이터 추가"]},{"cell_type":"code","metadata":{"id":"2-FngjGmRyVz"},"source":["weather_path = '/content/drive/MyDrive/구내식당/water/날씨'\n","\n","w_attrs = ['3시간기온', '강수형태', '강수확률', '습도', '6시간강수량']\n","w_years = os.listdir(weather_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8-iXSbGpR3uS"},"source":["def get_wdata(data_path) :\n","    datetime_list = []\n","    value_list_12 = []\n","    value_list_18 = []\n","    curr_mon = ''\n","\n","    with open(data_path, 'r') as f :\n","        lines = f.readlines()\n","        for i, line in enumerate(lines) :\n","            if line.strip() == '' :\n","                break\n","            row_data = line.strip().split(',')\n","            row_data = [elem.strip() for elem in row_data]\n","            if i == 0 :\n","                curr_mon = row_data[-1].split()[-1][:-2]\n","                continue\n","            if len(row_data) == 1 :\n","                curr_mon =row_data[-1].split()[-1][:-2]\n","                continue\n","            r_day, r_hour, r_fore, r_temp = row_data\n","            if '6시간' not in data_path :\n","                if r_hour == '1400' and r_fore in ['+13', '+19']:\n","                    if r_fore in '+13' :\n","                        datetime_list.append(curr_mon[:4]+'-'+curr_mon[4:]+'-'+str('%02d'%int(r_day)))\n","                    if r_fore == '+13' :\n","                        value_list_12.append(float(r_temp))\n","                    else :\n","                        value_list_18.append(float(r_temp))\n","            else :\n","                if r_hour == '1400' and r_fore in ['+13', '+19']:\n","                    if r_fore in '+13' :\n","                        datetime_list.append(curr_mon[:4]+'-'+curr_mon[4:]+'-'+str('%02d'%int(r_day)))\n","                    if r_fore == '+13' :\n","                        value_list_12.append(float(r_temp))\n","                    else :\n","                        value_list_18.append(float(r_temp))\n","\n","        if data_path == '/content/drive/MyDrive/구내식당/water/정훈/날씨/2017/충무공동_강수형태_201701_201712.csv' :\n","            nan_val = [0]*22\n","            nan_val[-7] = 1\n","            nan_val[-6] = 1\n","            nan_val[-2] = 1 \n","            value_list_12.extend(nan_val)\n","            value_list_18.extend(nan_val)\n","    return datetime_list, value_list_12, value_list_18"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2GynTZ0lR9Uf"},"source":["# ['3시간기온', '강수형태', '강수확률', '습도', '하늘상태']\n","w_data_rain_12 = []\n","w_data_rain_18 = []\n","w_data_rtype_12 = []\n","w_data_rtype_18 = []\n","w_data_hum_12 = []\n","w_data_hum_18 = []\n","w_data_temp_12 = []\n","w_data_temp_18 = []\n","w_data_rain_total_12 = []\n","w_data_rain_total_18 = []\n","w_datetime = []\n","\n","for year in w_years :\n","    w_subdir = os.path.join(weather_path, year)\n","    file_names = os.listdir(w_subdir)\n","    file_name = \"\"\n","    if year != '2021' :\n","        file_name = f'{year}01_{year}12.csv'\n","    else :\n","        file_name = f'{year}01_{year}06.csv'\n","    file_path_rain = os.path.join(w_subdir, '충무공동_강수확률_'+file_name)\n","    file_path_temp = os.path.join(w_subdir, '충무공동_3시간기온_'+file_name)\n","    file_path_hum = os.path.join(w_subdir, '충무공동_습도_'+file_name)\n","    file_path_rtype = os.path.join(w_subdir, '충무공동_강수형태_'+file_name)\n","    file_path_rain_total = os.path.join(w_subdir, '충무공동_6시간강수량_'+file_name)\n","\n","    datetime_list_temp, value_list_temp_12, value_list_temp_18 = get_wdata(file_path_temp) # 3시간기온 데이터\n","    datetime_list_rain, value_list_rain_12, value_list_rain_18 = get_wdata(file_path_rain) # 강수확률 데이터\n","    datetime_list_hum, value_list_hum_12, value_list_hum_18 = get_wdata(file_path_hum) # 습도 데이터\n","    datetime_list_rtype, value_list_rtype_12, value_list_rtype_18 = get_wdata(file_path_rtype) # 강수형태 데이터\n","    datetime_list_rain_total, value_list_rain_total_12, value_list_rain_total_18 = get_wdata(file_path_rain_total) # 강수량 데이터\n","\n","    w_datetime   += datetime_list_temp\n","    w_data_rain_12  += value_list_rain_12\n","    w_data_temp_12  += value_list_temp_12\n","    w_data_hum_12   += value_list_hum_12\n","    w_data_rtype_12 += value_list_rtype_12\n","    w_data_rain_18  += value_list_rain_18\n","    w_data_temp_18  += value_list_temp_18\n","    w_data_hum_18   += value_list_hum_18\n","    w_data_rtype_18 += value_list_rtype_18\n","    w_data_rain_total_12 += value_list_rain_total_12\n","    w_data_rain_total_18 += value_list_rain_total_18"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d4HZBR6HR-3n"},"source":["w_df = pd.DataFrame({'일자':pd.Series(w_datetime, dtype='datetime64[ns]'),\n","                   'rain_prob_lunch':pd.Series(w_data_rain_12, dtype='float'),\n","                   'temp_lunch':pd.Series(w_data_temp_12, dtype='float'),\n","                   'hum_lunch':pd.Series(w_data_hum_12, dtype='float'),\n","                   'rain_total_lunch':pd.Series(w_data_rain_total_12, dtype='float'),\n","                   'rain_type_lunch':pd.Series(w_data_rtype_12, dtype='float'),\n","                   'rain_prob_dinner':pd.Series(w_data_rain_18, dtype='float'),\n","                   'temp_dinner':pd.Series(w_data_temp_18, dtype='float'),\n","                   'hum_dinner':pd.Series(w_data_hum_18, dtype='float'),\n","                   'rain_total_dinner':pd.Series(w_data_rain_total_18, dtype='float'),\n","                   'rain_type_dinner':pd.Series(w_data_rtype_18, dtype='float'),}).sort_values('일자').reset_index(drop=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NsFFBSAwSL1s"},"source":["# 불쾌지수 컬럼 추가\n","# https://dacon.io/competitions/official/235736/codeshare/2753?page=1&dtype=recent\n","w_df['discomfort_lunch'] = 1.8*w_df['temp_lunch'] - 0.55*(1-w_df['hum_lunch']/100)*(1.8*w_df['temp_lunch']-26) + 32\n","w_df['discomfort_dinner'] = 1.8*w_df['temp_dinner'] - 0.55*(1-w_df['hum_dinner']/100)*(1.8*w_df['temp_dinner']-26) + 32\n","\n","# 불쾌지수 명목변수화 \n","w_df['discomfort_lunch'] = w_df['discomfort_lunch'].apply(lambda x : 0 if x<68 else (1 if 68<=x<75 else (2 if 75<=x<80 else 3)))\n","w_df['discomfort_dinner'] = w_df['discomfort_dinner'].apply(lambda x : 0 if x<68 else (1 if 68<=x<75 else (2 if 75<=x<80 else 3)))\n","\n","# 진주에는 눈이 거의 내리지 않음 -> 비로 고쳐주겠음\n","w_df['rain_type_lunch'] = w_df['rain_type_lunch'].apply(lambda x : 0 if x==0 else 1)\n","w_df['rain_type_dinner'] = w_df['rain_type_dinner'].apply(lambda x : 0 if x==0 else 1)\n","\n","w_df['rain_prob_lunch'] = w_df['rain_prob_lunch'].apply(lambda x : np.round(x/100, 1))\n","w_df['rain_prob_dinner'] = w_df['rain_prob_dinner'].apply(lambda x : np.round(x/100, 1))\n","w_df.drop(columns=['temp_lunch', 'temp_dinner', 'hum_lunch', 'hum_dinner'], inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R_PFyx4LSNGp"},"source":["w_df['일자'] = w_df['일자'].shift(-1)\n","df['일자'] = df['일자'].astype('datetime64')\n","df = pd.merge(df, w_df, on='일자', how='left')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_-sRKn88z5do"},"source":["# df['expect_lunch'] = df['rain_prob_lunch']*df['rain_total_lunch']\n","# df['expect_dinner'] = df['rain_prob_dinner']*df['rain_total_dinner']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SpUeZDvyKDiG"},"source":["# df.drop(columns=['rain_total_lunch', 'rain_total_dinner'], inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jxsw5--drBg0"},"source":["* 복날, 연말, 명절 전 영입일 - 긴 연후 전후\n","* 국정감사 \n","    * 컬럼 체크 필요"]},{"cell_type":"code","metadata":{"id":"fNyg8enap-kO"},"source":["def feature_pipeline(df) :\n","    df['식사가능인원'] = df['정원']-(df['휴가자']+df['출장자']+df['재택근무자'])+df['야근자']\n","    df['년'] = df['일자'].dt.year\n","    df['월'] = df['일자'].dt.month\n","    df['일'] = df['일자'].dt.day\n","    df['년월'] = df['년'].astype('str')+'_'+df['월'].astype('str')\n","\n","    first_dayofmonth = []\n","    last_dayofmonth = []\n","    for i in df['년월'].unique() :\n","        first_dayofmonth.append(df[df['년월']==i].iloc[0].name)\n","        last_dayofmonth.append(df[df['년월']==i].iloc[-1].name)\n","    df['첫_출근일'] = df.apply(lambda x : 1 if x.name in first_dayofmonth else 0, axis=1)\n","    df['마지막_출근일'] = df.apply(lambda x : 1 if x.name in last_dayofmonth else 0, axis=1)\n","\n","    # 월_주차 컬럼 추가\n","    df['주차'] = df['일자'].apply(lambda x: pendulum.parse(str(x)).week_of_month)\n","    repair_2017 = df[(df['년']==2017)&(df['주차']<0)]['일자'].dt.week\n","    repair_2021 = df[(df['년']==2021)&(df['주차']<0)]['일자'].dt.week\n","    df['주차'][list(repair_2017.index)] = repair_2017.values\n","    df['주차'][list(repair_2021.index)] = repair_2021.values\n","    df['주차'][[709, 954, 955]] = np.array([6, 5, 5])\n","\n","    month_to_season = {1: 3,2: 3,3:0,4:0,5:0,6:1,7:1,8:1,9:2,10:2,11:2,12: 3}\n","    df['계절'] = df['월'].apply(lambda x : month_to_season[x])\n","    df['요일'] = df['일자'].dt.weekday\n","    df['야근_가능'] = df['요일'].apply(lambda x : 1 if (x==2) or (x==4) else 0)\n","    df['is_corona'] = df['일자'].apply(lambda x : 0 if x < pd.to_datetime('2020-01-06') else 1)\n","    df['연기준몇주째']= df['일자'].dt.weekofyear\n","    df['월마지막일여부'] =df['일자'].dt.is_month_end\n","    df['월일수']= df['일자'].dt.days_in_month\n","\n","    # 공휴일 데이터 추가\n","    holiday['date'] = pd.to_datetime(holiday['date'])\n","    df['before_holiday'] = df['일자'].apply(lambda x : 1 if (x+dt.timedelta(1) in holiday['date'].tolist()) else 0)\n","    df['after_holiday'] = df['일자'].apply(lambda x : 1 if (x-dt.timedelta(1) in holiday['date'].tolist()) else 0)\n","    \n","    # 탄력근무제 적용 여부\n","    test_date=\"20180701\"\n","    convert_date = datetime.datetime.strptime(test_date, \"%Y%m%d\").date()\n","    df['탄력근무_여부'] = df['일자'].apply(lambda x : 1 if x >= convert_date else 0)\n","\n","    # # 이벤트 데이터 고민 - 복날 / 연말\n","    bok = pd.to_datetime(['2016-08-16', '2016-07-27', '2016-07-18', '2017-08-11', '2017-07-21','2017-07-12', '2018-08-16', '2018-07-27', '2018-07-17', '2019-08-12', '2019-07-22', '2019-07-12', '2020-07-27', '2020-07-16'])\n","    df['복날'] = df['일자'].apply(lambda x : 1 if x in bok else 0)\n","\n","    # end_year = df[(df['월']==12)&(df['일']>=21)].index\n","    # df['연말'] = df.apply(lambda x : 1 if  x.name in end_year else 0, axis=1)\n","\n","    # 명절 전 영업일 여부\n","    event = pd.to_datetime(['2016-02-05', '2019-09-13', '2017-01-26','2017-09-29', '2018-02-14', '2018-09-21', '2019-02-01', '2019-09-11', '2020-01-23', '2020-09-30', '2021-02-10'])\n","    df['명절_이전_영업일'] = df['일자'].apply(lambda x : 1 if x in event else 0)\n","\n","    # 명절 전이나 둘 중에 하나만 써야함 / 특히 10월달이 이상함\n","    # df['긴_연휴'] = df['일자'].diff().dt.days.fillna(0).astype('int')\n","    # df['연휴_뒤'] = df['긴_연휴'].apply(lambda x: 1 if x>=4 else 0)\n","    # df['연휴_앞'] = df.apply(lambda x : 1 if x.name in df[df['연휴_뒤']==1].index-1 else 0, axis=1)\n","    # df.drop(columns=['긴_연휴'], inplace=True)\n","\n","    # 국정 감사 기간\n","    gukgam = pd.to_datetime(['2016-10-04', '2016-10-05', '2016-10-06', '2017-10-10', '2017-10-11', '2017-10-12', '2017-10-13', '2018-10-08', '2018-10-10', '2018-10-11',\\\n","                             '2019-10-02', '2019-10-04'])\n","    df['국정감사'] = df['일자'].apply(lambda x : 1 if x in gukgam else 0)\n","\n","    # 인원 변화\n","    # df['인원변화'] = df['정원'].diff()\n","    # df['인원변화'][0] = 0\n","\n","    df.drop(columns=['정원', '년월'], inplace=True)\n","    return df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bFVPTKq2mpmt"},"source":["#### 고민하는 변수"]},{"cell_type":"code","metadata":{"id":"2sbY-BF5UkhR"},"source":["# check_corona = corona[['일자', '누적검사자']]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zM-X4KHMgX2r"},"source":["# 고민해본 파생변수\n","# 식사인원\n","# df['휴가출장'] = df['정원']-(df['휴가자']+df['출장자'])\n","# df['휴가출장재택'] = df['정원']-(df['휴가자']+df['출장자']+df['재택근무자'])\n","# df['휴가출장재택야근'] = df['정원']-(df['휴가자']+df['출장자']+df['재택근무자'])+df['야근자']\n","# df['휴가'] = df['정원']-(df['휴가자'])\n","# df['휴가야근'] = df['정원']-(df['휴가자'])+df['야근자']\n","# df['휴가재택야근'] = df['정원']-(df['휴가자']+df['재택근무자'])+df['야근자']\n","# df['휴가재택'] = df['정원']-(df['휴가자']+df['재택근무자'])\n","# df['휴가비율'] = df['휴가자']/df['정원']\n","# df['출장비율'] = df['출장자']/df['정원']\n","# df['야근비율'] = df['야근자']/df['출근인원']\n","# df['재택비율'] = df['재택근무자']/df['정원']\n","# df['휴가_출장'] = df['휴가자']+df['출장자']\n","\n","# etc\n","# df['is_monday'] = df['요일'].apply(lambda x : 1 if (x==0) else 0) \n","# df['정책_변화'] = df['일자'].apply(lambda x : 0 if x < pd.to_datetime('2019-01-04') else 1)\n","# df['월마지막일여부'] =df['일자'].dt.is_month_end\n","# df['연기준몇일째']= df['일자'].dt.dayofyear\n","\n","# maybe not\n","# df['출장자제외'] = df['정원'] - df['출장자']\n","# df['재택근무제외'] = df['정원'] - df['재택근무자']\n","# df['윤년여부'] = df['일자'].dt.is_leap_year\n","# df['월시작일여부'] = df['일자'].dt.is_month_start\n","# df['월마지막일여부'] =df['일자'].dt.is_month_end\n","# df['분기시작일여부'] =df['일자'].dt.is_quarter_start\n","# df['분기마지막일여부'] =df['일자'].dt.is_quarter_end\n","# df['연시작일여부'] =df['일자'].dt.is_year_start\n","# df['연마지막일여부'] =df['일자'].dt.is_year_end"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B7whoF48rOKX"},"source":["### 데이터 정규화"]},{"cell_type":"code","metadata":{"id":"FhrGZeRNrTaN"},"source":["df = feature_pipeline(df)\n","\n","# 코로나 데이터 추가\n","# corona = corona.drop_duplicates(['일자'])\n","# corona['일자'] = corona['일자'].astype('datetime64')\n","# df = pd.merge(df,corona[['일자', '일일검사자']], on='일자', how='left')\n","# df = df.fillna(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2z2CUjMo3WlY"},"source":["df['중식비율'] = df['중식계']/df['식사가능인원']\n","df['석식비율'] = df['석식계']/df['식사가능인원']\n","# df.drop(columns=[i for i in df.columns if ('메뉴' in i) or ('degree' in i)], inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eTc_3wc4AOjE"},"source":["## 이상치 제거\n","* 수요일 야근자가 100명 이상, 정책 변경 후 금요일 100명 이상 제거\n","* 식사비율이 이상치에 해당하는 값 제거\n","    * 1.5*(제 3분위 수 - 제 1분위 수)\n"]},{"cell_type":"code","metadata":{"id":"D72MMCwrOC5I"},"source":["df = df[~df.index.isin(df[(df['요일']==2)&(df['야근자']>=100)].index.append(df[(df['요일']==4)&(df['일자']>='2019-01-01')&(df['야근자']>100)].index))]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MJanA6ghM47y"},"source":["train = df.iloc[:-50]\n","train['월마지막일여부'] = train['월마지막일여부'].astype(int)\n","test = df.iloc[-50:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8bOFtebI8bfn"},"source":["train_2 = train[train['석식계']!=0]\n","iqr_lunch = 1.5*(train['중식비율'].quantile(0.75) - train['중식비율'].quantile(0.25))\n","iqr_dinner = 1.5*(train_2['석식비율'].quantile(0.75) - train_2['석식비율'].quantile(0.25))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CtefFzD096t3"},"source":["plt.plot(train_2['일자'], train_2['석식비율'], marker='o')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4FFfmCLv8uZG"},"source":["train_1 = train[(train['중식비율']>=train['중식비율'].quantile(0.25)-iqr_lunch)|(train['중식비율']<=train['중식비율'].quantile(0.75)+iqr_lunch)]\n","train_2 = train_2[train_2['일자']!='2016-10-05']\n","# train_2 = train_2[(train_2['석식비율']>=train_2['석식비율'].quantile(0.25)-iqr_dinner)|(train_2['석식비율']<=train_2['석식비율'].quantile(0.75)+iqr_dinner)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PE6cH0Pm5a-t"},"source":["def get_one_hot(x, target_val):\n","  if x == target_val:\n","    return 1\n","  else:\n","    return 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3EDdvFp75bwl"},"source":["from sklearn.preprocessing import LabelEncoder\n","onehot_col = ['년', '월', '요일', '계절']\n","# onehot_col = ['월', '요일', '계절']\n","# df_tmp = df.copy()\n","# df = pd.concat([df[list((set(df.columns)-set(onehot_col)))],\\\n","#                 pd.get_dummies(df[onehot_col])], axis=1)\n","\n","sub_types = [[2016, 2017, 2018, 2019, 2020, 2021], [1,2,3,4,5,6,7,8,9,10,11,12], [0,1,2,3,4], [0,1,2,3]]\n","# sub_types = [[1,2,3,4,5,6,7,8,9,10,11,12], [0,1,2,3,4], [0,1,2,3]]\n","\n","train_lun = train_1.copy()\n","train_din = train_2.copy()\n","# for i, col_type in enumerate(onehot_col):\n","#   for j, class_nm in enumerate(sub_types[i]):\n","#     df_tmp[col_type + '_' + str(class_nm)] = df_tmp[col_type].apply(lambda x: get_one_hot(x, class_nm))\n","\n","for i, col_type in enumerate(onehot_col):\n","  for j, class_nm in enumerate(sub_types[i]):\n","    train_lun[col_type + '_' + str(class_nm)] = train_lun[col_type].apply(lambda x: get_one_hot(x, class_nm))\n","for i, col_type in enumerate(onehot_col):\n","  for j, class_nm in enumerate(sub_types[i]):\n","    train_din[col_type + '_' + str(class_nm)] = train_din[col_type].apply(lambda x: get_one_hot(x, class_nm))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8s6vRKynS4TX"},"source":["train_1 = train_lun[[i for i in train_lun.columns if ('석식' not in i) and ('dinner' not in i) and ('탄력근무' not in i) and ('명절' not in i)]]\n","train_2 = train_din[[i for i in train_din.columns if ('중식' not in i) and ('lunch' not in i)]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"56bzQdB_AIwm"},"source":["# train = train.drop(columns=['일자', '년', '월', '요일',  '계절'])\n","# train_1 =train[[i for i in train.columns if ('석식' not in i) and ('dinner' not in i) and ('탄력근무' not in i) and ('명절' not in i)]]\n","# train_2 =train[[i for i in train.columns if ('중식' not in i) and ('lunch' not in i)]]\n","\n","# 이상치 제거\n","# train_2 = train_2.loc[train_2['석식계'] != 0.0]\n","# train_2 = train_2.loc[(train_2['년_2016'] != 1) | (train_2['월_10'] != 1) | (train_2['일'] != 5)]\n","# # train_2 = train_2.loc[(train_2['년_2019'] != 1) | (train_2['월_9'] != 1) | (train_2['일'] != 11)]\n","# # train_2 = train_2.loc[(train_2['년_2019'] != 1) | (train_2['월_12'] != 1) | (train_2['일'] != 23)]\n","# # train_2 = train_2.loc[(train_2['년_2019'] != 1) | (train_2['월_12'] != 1) | (train_2['일'] != 30)]\n","# # train_2 = train_2.loc[(train_2['년_2020'] != 1) | (train_2['월_1'] != 1) | (train_2['일'] != 23)]\n","\n","# train_1 = train_1.loc[(train_1['년_2016'] != 1) | (train_1['월_10'] != 1) | (train_1['일'] != 5)]\n","# train_1 = train_1.loc[(train_1['년_2017'] != 1) | (train_1['월_12'] != 1) | (train_1['일'] != 28)]\n","# train_1 = train_1.loc[(train_1['년_2020'] != 1) | (train_1['월_12'] != 1) | (train_1['일'] != 2)]\n","# train_1 = train_1.loc[(train_1['년_2018'] != 1) | (train_1['월_9'] != 1) | (train_1['일'] != 14)]\n","# train_1 = train_1.loc[(train_1['년_2018'] != 1) | (train_1['월_12'] != 1) | (train_1['일'] != 24)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"73IbKbNpS4fN"},"source":["for i, col_type in enumerate(onehot_col):\n","  for j, class_nm in enumerate(sub_types[i]):\n","    test[col_type + '_' + str(class_nm)] = test[col_type].apply(lambda x: get_one_hot(x, class_nm))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PAzHnB0nA_uH"},"source":["train_1 = train_1.drop(columns=['일자', '요일', '월', '월마지막일여부', '계절', '월일수','복날', '중식비율', '년'])\n","train_2 = train_2.drop(columns=['일자', '요일', '월', '월마지막일여부', '월일수','계절', '복날', 'after_holiday', '석식비율', '년', '첫_출근일'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2_INS-fArpY1"},"source":["# train_1.drop(columns=['첫_출근일', '마지막_출근일'], inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R9SNEwOuuFdH"},"source":["train_1.drop(columns=['국정감사'], inplace=True)\n","train_2.drop(columns=['국정감사'], inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iGR2r9EhV0RZ"},"source":["train_1.columns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Fsy_siBV2b5"},"source":["train_2.columns"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fYnxhThaC3QF"},"source":["## AutoML"]},{"cell_type":"code","metadata":{"id":"mdluKTtVtbcy"},"source":["!pip install optuna"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3UzXhQB2tkPV"},"source":["import optuna\n","from optuna import Trial\n","from optuna.samplers import TPESampler\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.model_selection import train_test_split\n","from catboost import CatBoostRegressor\n","from lightgbm import LGBMRegressor"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w9LUcbkNtWaP"},"source":["## Catboost 모델링"]},{"cell_type":"markdown","metadata":{"id":"iUKXq4Q1fA3p"},"source":["### 점심"]},{"cell_type":"code","metadata":{"id":"sQZKNlDruept"},"source":["# def objective(trial: Trial) -> float:\n","# #     params_catboost = {\n","# #     'iterations': trial.suggest_int(\"iterations\", 1, 100000),\n","# #     'learning_rate': trial.suggest_float(\"learning_rate\", 0.001, 0.01),\n","# #     'eval_metric': 'MAE',\n","# #     'random_seed': 42,\n","# #     'logging_level': 'Silent',\n","# #     'use_best_model': True,\n","# #     'loss_function': 'MAE',\n","# #     'od_type': 'Iter',\n","# #     'od_wait': trial.suggest_int(\"od_wait\", 1, 10000),\n","# #     'one_hot_max_size': trial.suggest_int(\"one_hot_max_size\", 1, 10000),\n","# #     'l2_leaf_reg': trial.suggest_int(\"l2_leaf_reg\", 1, 15),\n","# #     'depth': trial.suggest_int(\"l2_leaf_reg\", 1, 15),\n","# #     'rsm': trial.suggest_float(\"rsm\", 0.1, 0.9),\n","# #     'random_strength': trial.suggest_int(\"random_strength\", 1, 10000),\n","# #     'bagging_temperature': trial.suggest_int(\"bagging_temperature\", 1, 10000),\n","# # }\n","#     params_catboost = {\n","#         # \"objective\": 'Regression',\n","#         'eval_metric' : 'MAE',\n","#         'loss_function' : 'MAE',\n","#         \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n","#         \"depth\": trial.suggest_int(\"depth\", 1, 12),\n","#         \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n","#         \"bootstrap_type\": trial.suggest_categorical(\n","#             \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n","#         ),\n","\n","#         'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-1, log = True),\n","#         'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-5, 1e-1, log = True),\n","#         'min_child_samples': trial.suggest_int('min_child_samples', 2, 20),\n","#         'random_strength': trial.suggest_float('random_strength', 0.05, 1, log = True)\n","#     }\n","\n","#     if params_catboost[\"bootstrap_type\"] == \"Bayesian\":\n","#         params_catboost[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n","#     elif params_catboost[\"bootstrap_type\"] == \"Bernoulli\":\n","#         params_catboost[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n","    \n","#     X_train, X_valid, y_train, y_valid = train_test_split(train_1[lunch_col].drop(columns=['중식계']), train_1['중식계'], test_size=0.2)\n","\n","#     model = CatBoostRegressor(**params_catboost)\n","#     model.fit(\n","#         X_train,\n","#         y_train,\n","#         cat_features=[],\n","#         eval_set=(X_valid, y_valid),\n","#         early_stopping_rounds=100,\n","#         verbose=True,\n","#     )\n","\n","#     from sklearn.metrics import mean_absolute_error\n","#     catboost_pred = model.predict(X_valid)\n","#     mean_absolute_error = (mean_absolute_error(y_valid, catboost_pred))\n","    \n","#     return mean_absolute_error"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ItKdzwzuvDGO"},"source":["# sampler = TPESampler(seed=42)\n","# study = optuna.create_study(\n","#     study_name=\"catboost_parameter_opt\",\n","#     direction=\"minimize\",\n","#     sampler=sampler,\n","# )\n","# study.optimize(objective, n_trials=50)\n","# print(\"Best Score:\", study.best_value)\n","# print(\"Best trial:\", study.best_trial.params)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k5CNlnT0jDfH"},"source":["# catboost params\n","params = {'colsample_bylevel': 0.09657164370923056, 'depth': 8, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bernoulli',\n","          'learning_rate': 0.06770318888990769, 'l2_leaf_reg': 0.00021708988251157392, 'min_child_samples': 15, 'random_strength': 0.1618545184779159, 'subsample': 0.8477676851569896}\n","\n","X_train, X_valid, y_train, y_valid = train_test_split(train_1[lunch_col].drop(columns=['중식계']), train_1['중식계'], test_size=0.2)\n","model = CatBoostRegressor(**params)\n","model.fit(\n","        X_train,\n","        y_train,\n","        cat_features=[],\n","        eval_set=(X_valid, y_valid),\n","        early_stopping_rounds=100,\n","        verbose=True,\n","    )\n","\n","catboost_pred_lunch = model.predict(test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"annUBspDjk1N"},"source":["# optuna.visualization.plot_optimization_history(study)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NTjw_Uw4mzsJ"},"source":["# optuna.visualization.plot_parallel_coordinate(study)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XbBQHfcOmz5m"},"source":["# optuna.visualization.plot_param_importances(study)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"apcZ571Zm0EM"},"source":["# from catboost import Pool\n","# feature_score = pd.DataFrame(list(zip(train_1[lunch_col].drop(columns=['중식계']).dtypes.index, model.get_feature_importance(Pool(train_1[lunch_col].drop(columns=['중식계']), label=train_1['중식계'])))),\n","#                 columns=['Feature','Score'])\n","\n","# feature_score = feature_score.sort_values(by='Score', ascending=False, inplace=False, kind='quicksort', na_position='last')\n","# plt.rcParams[\"figure.figsize\"] = (12,7)\n","# ax = feature_score.plot('Feature', 'Score', kind='bar', color='c')\n","# ax.set_title(\"Catboost Feature Importance Ranking\", fontsize = 14)\n","# ax.set_xlabel('')\n","\n","# rects = ax.patches\n","\n","# labels = feature_score['Score'].round(2)\n","\n","# for rect, label in zip(rects, labels):\n","#     height = rect.get_height()\n","#     ax.text(rect.get_x() + rect.get_width()/2, height + 0.35, label, ha='center', va='bottom')\n","\n","# plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ThZ8S_DFfDbk"},"source":["### 저녁"]},{"cell_type":"code","metadata":{"id":"RQjRzzWSfE8s"},"source":["dinner_col = ['휴가자', '출장자', '야근자', '재택근무자', '석식계', '석식메뉴_육류', '석식메뉴_난류', '석식메뉴_덮밥_국밥류', '석식메뉴_비빔밥_볶음밥류', '석식메뉴_국탕류', '석식메뉴_구이류', '석식메뉴_전류', '석식메뉴_튀김류', '석식메뉴_콩류', '석식메뉴_묵', '석식메뉴_생선_조개류', '석식메뉴_채소류', '석식메뉴_해조류', '석식메뉴_장류', '석식메뉴_만두', '석식메뉴_곡물가루', '석식메뉴_과일', '석식메뉴_김밥_초밥', '석식메뉴_절임류', '석식메뉴_면류', '석식메뉴_스튜', '석식메뉴_샐러드', '석식메뉴_우유', '석식메뉴_빵류', '석식메뉴_돼지고기', '석식메뉴_소고기', '석식메뉴_닭고기', '석식메뉴_오리고기', '식사가능인원', '일', '야근_가능', '연기준몇주째', '첫_출근일', '마지막_출근일', '주차', 'is_corona', 'fine_degree', 'rain_degree_dinner', 'discomfort_degree_dinner', 'before_holiday', 'after_holiday', '월일수','명절_이전_영업일', '국정감사','탄력근무_여부', '년_2016', '년_2017', '년_2018', '년_2019', '년_2020', '년_2021', '월_1', '월_2', '월_3', '월_4', '월_5', '월_6', '월_7', '월_8', '월_9', '월_10', '월_11', '월_12', '요일_0', '요일_1', '요일_2', '요일_3', '요일_4', '계절_0', '계절_1', '계절_2', '계절_3']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MegfZyz7cH7q"},"source":["# def objective(trial: Trial) -> float:\n","# #     params_catboost = {\n","# #     'iterations': trial.suggest_int(\"iterations\", 1, 100000),\n","# #     'learning_rate': trial.suggest_float(\"learning_rate\", 0.001, 0.01),\n","# #     'eval_metric': 'MAE',\n","# #     'random_seed': 42,\n","# #     'logging_level': 'Silent',\n","# #     'use_best_model': True,\n","# #     'loss_function': 'MAE',\n","# #     'od_type': 'Iter',\n","# #     'od_wait': trial.suggest_int(\"od_wait\", 1, 10000),\n","# #     'one_hot_max_size': trial.suggest_int(\"one_hot_max_size\", 1, 10000),\n","# #     'l2_leaf_reg': trial.suggest_int(\"l2_leaf_reg\", 1, 15),\n","# #     'depth': trial.suggest_int(\"l2_leaf_reg\", 1, 15),\n","# #     'rsm': trial.suggest_float(\"rsm\", 0.1, 0.9),\n","# #     'random_strength': trial.suggest_int(\"random_strength\", 1, 10000),\n","# #     'bagging_temperature': trial.suggest_int(\"bagging_temperature\", 1, 10000),\n","# # }\n","#     params_catboost = {\n","#         # \"objective\": 'Regression',\n","#         'eval_metric' : 'MAE',\n","#         'loss_function' : 'MAE',\n","#         \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n","#         \"depth\": trial.suggest_int(\"depth\", 1, 12),\n","#         \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n","#         \"bootstrap_type\": trial.suggest_categorical(\n","#             \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n","#         ),\n","\n","#         'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-1, log = True),\n","#         'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-5, 1e-1, log = True),\n","#         'min_child_samples': trial.suggest_int('min_child_samples', 2, 20),\n","#         'random_strength': trial.suggest_float('random_strength', 0.05, 1, log = True)\n","#     }\n","\n","#     if params_catboost[\"bootstrap_type\"] == \"Bayesian\":\n","#         params_catboost[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n","#     elif params_catboost[\"bootstrap_type\"] == \"Bernoulli\":\n","#         params_catboost[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n","    \n","#     X_train, X_valid, y_train, y_valid = train_test_split(train_2[dinner_col].drop(columns=['석식계']), train_2['석식계'], test_size=0.2)\n","\n","#     model = CatBoostRegressor(**params_catboost)\n","#     model.fit(\n","#         X_train,\n","#         y_train,\n","#         cat_features=[],\n","#         eval_set=(X_valid, y_valid),\n","#         early_stopping_rounds=100,\n","#         verbose=True,\n","#     )\n","\n","#     from sklearn.metrics import mean_absolute_error\n","#     catboost_pred = model.predict(X_valid)\n","#     mean_absolute_error = (mean_absolute_error(y_valid, catboost_pred))\n","    \n","#     return mean_absolute_error"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ETHjYhBtfm-S"},"source":["# def objective(trial: Trial) -> float:\n","#     params_catboost = {\n","#     'iterations': trial.suggest_int(\"iterations\", 1, 100000),\n","#     'learning_rate': trial.suggest_float(\"learning_rate\", 0.001, 0.01),\n","#     'eval_metric': 'MAE',\n","#     'loss_function': 'MAE',\n","#     'random_seed': 42,\n","#     'logging_level': 'Silent',\n","#     'use_best_model': True,\n","#     'od_type': 'Iter',\n","#     'od_wait': trial.suggest_int(\"od_wait\", 1, 10000),\n","#     'one_hot_max_size': trial.suggest_int(\"one_hot_max_size\", 1, 10000),\n","#     'l2_leaf_reg': trial.suggest_int(\"l2_leaf_reg\", 1, 15),\n","#     'depth': trial.suggest_int(\"l2_leaf_reg\", 1, 15),\n","#     'rsm': trial.suggest_float(\"rsm\", 0.1, 0.9),\n","#     'random_strength': trial.suggest_int(\"random_strength\", 1, 10000),\n","#     'bagging_temperature': trial.suggest_int(\"bagging_temperature\", 1, 10000),\n","# }\n","    \n","#     X_train, X_valid, y_train, y_valid = train_test_split(train_2[dinner_col].drop(columns=['석식계']), train_2['석식계'], test_size=0.2)\n","\n","#     model = CatBoostRegressor(**params_catboost)\n","#     model.fit(\n","#         X_train,\n","#         y_train,\n","#         cat_features=[],\n","#         eval_set=(X_valid, y_valid),\n","#         early_stopping_rounds=100,\n","#         verbose=True,\n","#     )\n","\n","#     from sklearn.metrics import mean_absolute_error\n","#     catboost_pred = model.predict(X_valid)\n","#     mean_absolute_error = (mean_absolute_error(y_valid, catboost_pred))\n","    \n","#     return mean_absolute_error"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dJwiyeeTftoR"},"source":["# sampler = TPESampler(seed=42)\n","# study = optuna.create_study(\n","#     study_name=\"catboost_parameter_opt\",\n","#     direction=\"minimize\",\n","#     sampler=sampler,\n","# )\n","# study.optimize(objective, n_trials=50)\n","# print(\"Best Score:\", study.best_value)\n","# print(\"Best trial:\", study.best_trial.params)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5z8Gg_8Of7t-"},"source":["# catboost params\n","params = {'colsample_bylevel': 0.07483210682368195, 'depth': 8, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bernoulli',\n","          'learning_rate': 0.01866828671939659, 'l2_leaf_reg': 0.031061104466436423, 'min_child_samples': 2, 'random_strength': 0.24750133579738517, 'subsample': 0.5114408837171173}\n","\n","X_train, X_valid, y_train, y_valid = train_test_split(train_2[dinner_col].drop(columns=['석식계']), train_2['석식계'], test_size=0.2)\n","model = CatBoostRegressor(**params)\n","model.fit(\n","        X_train,\n","        y_train,\n","        cat_features=[],\n","        eval_set=(X_valid, y_valid),\n","        early_stopping_rounds=100,\n","        verbose=True,\n","    )\n","\n","catboost_pred_dinner = model.predict(test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HagjY0XJ3mkh"},"source":["## LightGBM 모델링"]},{"cell_type":"markdown","metadata":{"id":"cBxZyCYsAgii"},"source":["### 점심"]},{"cell_type":"code","metadata":{"id":"S9KWl98kDcSS"},"source":["import lightgbm as lgb\n","from sklearn.metrics import cohen_kappa_score\n","from sklearn.model_selection import KFold, StratifiedKFold\n","import gc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A1pUKSlHIskx"},"source":["def objective(trial: Trial) -> float:\n","    params_lgb = {\n","        \"random_state\": 42,\n","        \"verbosity\": -1,\n","        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.5),\n","        \"n_estimators\": 10000,\n","        \"objective\": \"regression\",\n","        \"metric\": \"MAE\",\n","        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 3e-5),\n","        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 9e-2),\n","        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 20),\n","        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n","        'feature_fraction':trial.suggest_discrete_uniform('feature_fraction',0.5, 1, 0.01),\n","        'bagging_fraction':trial.suggest_discrete_uniform('bagging_fraction',0.5, 1, 0.01),\n","        \"subsample_freq\": trial.suggest_int(\"subsample_freq\", 1, 10),\n","        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n","        \"max_bin\": trial.suggest_int(\"max_bin\", 200, 500),\n","    }\n","    X_train, X_valid, y_train, y_valid = train_test_split(train_1.drop(columns=['중식계']), train_1['중식계'], test_size=0.2)\n","    \n","    model = LGBMRegressor(**params_lgb)\n","    model.fit(\n","        X_train,\n","        y_train,\n","        eval_set=[(X_train, y_train), (X_valid, y_valid)],\n","        early_stopping_rounds=100,\n","        verbose=False,\n","    )\n","    from sklearn.metrics import mean_absolute_error\n","    lgbm_pred = model.predict(X_valid)\n","    mean_absolute_error = (mean_absolute_error(y_valid, lgbm_pred))\n","    \n","    return mean_absolute_error"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mrp2lCE23c2a"},"source":["sampler = TPESampler(seed=42)\n","study = optuna.create_study(\n","    study_name=\"lgbm_parameter_opt\",\n","    direction=\"minimize\",\n","    sampler=sampler,\n",")\n","study.optimize(objective, n_trials=200)\n","print(\"Best Score:\", study.best_value)\n","print(\"Best trial:\", study.best_trial.params)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0pObWaWW5IVE"},"source":["params = {'learning_rate': 0.21957568357884244, 'reg_alpha': 2.21053202708411e-05, 'reg_lambda': 0.0548990854608358, 'max_depth': 2,\n","          'num_leaves': 84, 'feature_fraction': 0.51, 'bagging_fraction': 0.8400000000000001, 'subsample_freq': 1, 'min_child_samples': 9, 'max_bin': 329}\n","\n","X_train, X_valid, y_train, y_valid = train_test_split(train_1.drop(columns=['중식계']), train_1['중식계'], test_size=0.2)\n","model = LGBMRegressor(**params)\n","model.fit(\n","        X_train,\n","        y_train,\n","        eval_set=(X_valid, y_valid),\n","        early_stopping_rounds=100,\n","        verbose=False,\n","    )\n","\n","lgbm_pred_lunch = model.predict(test[train_1.drop(columns='중식계').columns])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dhgJQepwAkEb"},"source":["### 저녁"]},{"cell_type":"code","metadata":{"id":"Xv9xpPauLNaW"},"source":["def objective(trial: Trial) -> float:\n","    params_lgb = {\n","        \"random_state\": 42,\n","        \"verbosity\": -1,\n","        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.5),\n","        \"n_estimators\": 10000,\n","        \"objective\": \"regression\",\n","        \"metric\": \"MAE\",\n","        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 3e-5),\n","        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 9e-2),\n","        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 20),\n","        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n","        'feature_fraction':trial.suggest_discrete_uniform('feature_fraction',0.5, 1, 0.01),\n","        'bagging_fraction':trial.suggest_discrete_uniform('bagging_fraction',0.5, 1, 0.01),\n","        \"subsample_freq\": trial.suggest_int(\"subsample_freq\", 1, 10),\n","        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n","        \"max_bin\": trial.suggest_int(\"max_bin\", 200, 500),\n","    }\n","    X_train, X_valid, y_train, y_valid = train_test_split(train_2.drop(columns=['석식계']), train_2['석식계'], test_size=0.2)\n","    \n","    model = LGBMRegressor(**params_lgb)\n","    model.fit(\n","        X_train,\n","        y_train,\n","        eval_set=[(X_train, y_train), (X_valid, y_valid)],\n","        early_stopping_rounds=100,\n","        verbose=False,\n","    )\n","    from sklearn.metrics import mean_absolute_error\n","    lgbm_pred = model.predict(X_valid)\n","    mean_absolute_error = (mean_absolute_error(y_valid, lgbm_pred))\n","    \n","    return mean_absolute_error"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7OvGHDu5e3mX"},"source":["sampler = TPESampler(seed=42)\n","study = optuna.create_study(\n","    study_name=\"lgbm_parameter_opt\",\n","    direction=\"minimize\",\n","    sampler=sampler,\n",")\n","study.optimize(objective, n_trials=200)\n","print(\"Best Score:\", study.best_value)\n","print(\"Best trial:\", study.best_trial.params)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PVsAI_1Ye3hg"},"source":["params = {'learning_rate': 0.05494273949627791, 'reg_alpha': 2.3332952479619423e-05, 'reg_lambda': 0.027388545403756952,\n","          'max_depth': 14, 'num_leaves': 24, 'feature_fraction': 0.5, 'bagging_fraction': 0.9199999999999999, 'subsample_freq': 1, 'min_child_samples': 21, 'max_bin': 208}\n","\n","X_train, X_valid, y_train, y_valid = train_test_split(train_2.drop(columns=['석식계']), train_2['석식계'], test_size=0.2)\n","model = LGBMRegressor(**params)\n","model.fit(\n","        X_train,\n","        y_train,\n","        eval_set=(X_valid, y_valid),\n","        early_stopping_rounds=100,\n","        verbose=False,\n","    )\n","\n","lgbm_pred_dinner = model.predict(test[train_2.drop(columns='석식계').columns])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I_8IMY2ze3Yf"},"source":["## Xgboost"]},{"cell_type":"markdown","metadata":{"id":"Wp1mvs85NAPO"},"source":["### 점심"]},{"cell_type":"code","metadata":{"id":"1ZQzrVBmAwiC"},"source":["from xgboost import XGBRegressor"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AiY0tsaoe28s"},"source":["def objective(trial: Trial) -> float :\n","    tree_method = ['exact','approx','hist']\n","    boosting_list = ['gbtree', 'gblinear']\n","    objective_list_reg = ['reg:linear', 'reg:gamma', 'reg:tweedie']\n","    param_xgboost = {\n","        'boosting':trial.suggest_categorical('boosting', boosting_list),\n","        'tree_method':trial.suggest_categorical('tree_method', tree_method),\n","        'max_depth':trial.suggest_int('max_depth', 2, 25),\n","        'reg_alpha':trial.suggest_int('reg_alpha', 0, 5),\n","        'reg_lambda':trial.suggest_int('reg_lambda', 0, 5),\n","        'min_child_weight':trial.suggest_int('min_child_weight', 0, 5),\n","        'gamma':trial.suggest_int('gamma', 0, 5),\n","        'learning_rate':trial.suggest_loguniform('learning_rate',0.005,0.5),\n","        'eval_metric': \"mae\",\n","        'objective':trial.suggest_categorical('objective', objective_list_reg),\n","        'colsample_bytree':trial.suggest_discrete_uniform('colsample_bytree', 0.1, 1, 0.01),\n","        'colsample_bynode':trial.suggest_discrete_uniform('colsample_bynode', 0.1, 1, 0.01),\n","        'colsample_bylevel':trial.suggest_discrete_uniform('colsample_bylevel', 0.1, 1, 0.01),\n","        'subsample':trial.suggest_discrete_uniform('subsample', 0.5, 1, 0.05),\n","        'nthread' : -1  \n","      }\n","    X_train, X_valid, y_train, y_valid = train_test_split(train_1.drop(columns=['중식계']), train_1['중식계'], test_size=0.2)\n","    \n","    model = XGBRegressor(**param_xgboost)\n","    model.fit(\n","        X_train,\n","        y_train,\n","        eval_set=[(X_train, y_train), (X_valid, y_valid)],\n","        early_stopping_rounds=100,\n","        verbose=False,\n","    )\n","    from sklearn.metrics import mean_absolute_error\n","    xgboost_pred = model.predict(X_valid)\n","    mean_absolute_error = (mean_absolute_error(y_valid, xgboost_pred))\n","    \n","    return mean_absolute_error"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KCIH8jGUAtwt"},"source":["sampler = TPESampler(seed=42)\n","study = optuna.create_study(\n","    study_name=\"xgboost_parameter_opt\",\n","    direction=\"minimize\",\n","    sampler=sampler,\n",")\n","study.optimize(objective, n_trials=200)\n","print(\"Best Score:\", study.best_value)\n","print(\"Best trial:\", study.best_trial.params)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h5a4vo8KAvQq"},"source":["params = {'boosting': 'gblinear', 'tree_method': 'exact', 'max_depth': 4, 'reg_alpha': 2, 'reg_lambda': 4, 'min_child_weight': 1, 'gamma': 2, 'learning_rate': 0.09347190266835348,\n","          'objective': 'reg:linear', 'colsample_bytree': 0.88, 'colsample_bynode': 0.98, 'colsample_bylevel': 0.86, 'subsample': 0.65}\n","\n","X_train, X_valid, y_train, y_valid = train_test_split(train_1.drop(columns=['중식계']), train_1['중식계'], test_size=0.2)\n","model = XGBRegressor(**params)\n","model.fit(\n","        X_train,\n","        y_train,\n","        eval_set=[(X_valid, y_valid)],\n","        early_stopping_rounds=100,\n","        verbose=False,\n","    )\n","\n","xgboost_pred_lunch = model.predict(test[train_1.drop(columns='중식계').columns])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JFii1mkyRiP_"},"source":["### 저녁"]},{"cell_type":"code","metadata":{"id":"YQ7CtWkFAvY_"},"source":["def huber_approx_obj(preds, dtrain):\n","    d = preds - dtrain #remove .get_labels() for sklearn\n","    h = 1  #h is delta in the graphic\n","    scale = 1 + (d / h) ** 2\n","    scale_sqrt = np.sqrt(scale)\n","    grad = d / scale_sqrt\n","    hess = 1 / scale / scale_sqrt\n","    return grad, hess\n","\n","def objective(trial: Trial) -> float :\n","    tree_method = ['exact','approx','hist']\n","    boosting_list = ['gbtree', 'gblinear']\n","    objective_list_reg = ['reg:linear', 'reg:gamma', 'reg:tweedie', huber_approx_obj]\n","    param_xgboost = {\n","        'boosting':trial.suggest_categorical('boosting', boosting_list),\n","        'tree_method':trial.suggest_categorical('tree_method', tree_method),\n","        'max_depth':trial.suggest_int('max_depth', 2, 25),\n","        'reg_alpha':trial.suggest_int('reg_alpha', 0, 5),\n","        'reg_lambda':trial.suggest_int('reg_lambda', 0, 5),\n","        'min_child_weight':trial.suggest_int('min_child_weight', 0, 5),\n","        'gamma':trial.suggest_int('gamma', 0, 5),\n","        'learning_rate':trial.suggest_loguniform('learning_rate',0.005,0.5),\n","        'eval_metric': \"mae\",\n","        'objective':trial.suggest_categorical('objective', objective_list_reg),\n","        'colsample_bytree':trial.suggest_discrete_uniform('colsample_bytree', 0.1, 1, 0.01),\n","        'colsample_bynode':trial.suggest_discrete_uniform('colsample_bynode', 0.1, 1, 0.01),\n","        'colsample_bylevel':trial.suggest_discrete_uniform('colsample_bylevel', 0.1, 1, 0.01),\n","        'subsample':trial.suggest_discrete_uniform('subsample', 0.5, 1, 0.05),\n","        'nthread' : -1  \n","      }\n","    X_train, X_valid, y_train, y_valid = train_test_split(train_2.drop(columns=['석식계']), train_2['석식계'], test_size=0.2)\n","    \n","    model = XGBRegressor(**param_xgboost)\n","    model.fit(\n","        X_train,\n","        y_train,\n","        eval_set=[(X_train, y_train), (X_valid, y_valid)],\n","        early_stopping_rounds=100,\n","        verbose=False,\n","    )\n","    from sklearn.metrics import mean_absolute_error\n","    xgboost_pred = model.predict(X_valid)\n","    mean_absolute_error = (mean_absolute_error(y_valid, xgboost_pred))\n","    \n","    return mean_absolute_error"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XfZJIMWZAvbZ"},"source":["sampler = TPESampler(seed=42)\n","study = optuna.create_study(\n","    study_name=\"xgboost_parameter_opt\",\n","    direction=\"minimize\",\n","    sampler=sampler,\n",")\n","study.optimize(objective, n_trials=200)\n","print(\"Best Score:\", study.best_value)\n","print(\"Best trial:\", study.best_trial.params)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tek2ajvZAvea"},"source":["params = {'boosting': 'gbtree', 'tree_method': 'hist', 'max_depth': 13, 'reg_alpha': 2, 'reg_lambda': 2, 'min_child_weight': 4, 'gamma': 0,\n","          'learning_rate': 0.05505247994150589, 'objective': 'reg:linear', 'colsample_bytree': 0.49, 'colsample_bynode': 0.85, 'colsample_bylevel': 0.71, 'subsample': 1.0}\n","        \n","X_train, X_valid, y_train, y_valid = train_test_split(train_2.drop(columns=['석식계']), train_2['석식계'], test_size=0.2)\n","model = XGBRegressor(**params)\n","model.fit(\n","        X_train,\n","        y_train,\n","        eval_set=[(X_valid, y_valid)],\n","        early_stopping_rounds=100,\n","        verbose=False,\n","    )\n","\n","xgboost_pred_dinner = model.predict(test[train_2.drop(columns='석식계').columns])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vsnRgKfHqEPv"},"source":["## RandomForest"]},{"cell_type":"markdown","metadata":{"id":"q5FatEy6uL5D"},"source":["### 점심"]},{"cell_type":"code","metadata":{"id":"-5B_Hq3vpEA-"},"source":["from sklearn.ensemble import RandomForestRegressor"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XIxG9hbapFbK"},"source":["# def objective(trial: Trial) -> float :\n","#     params_rf = {\n","#         'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n","#         'max_depth': trial.suggest_int('max_depth', 4, 50),\n","#         'min_samples_split': trial.suggest_int('min_samples_split', 2, 150),\n","#         'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 60),\n","#         'random_state' : 42,\n","#     }\n","#     X_train, X_valid, y_train, y_valid = train_test_split(train_1[lunch_col].drop(columns=['중식계']), train_1['중식계'], test_size=0.2)\n","    \n","#     model = RandomForestRegressor(**params_rf)\n","#     model.fit(\n","#         X_train,\n","#         y_train,\n","#         # eval_set=[(X_train, y_train), (X_valid, y_valid)],\n","#         # early_stopping_rounds=100,\n","#         # verbose=False,\n","#     )\n","#     from sklearn.metrics import mean_absolute_error\n","#     rf_pred = model.predict(X_valid)\n","#     mean_absolute_error = (mean_absolute_error(y_valid, rf_pred))\n","    \n","#     return mean_absolute_error"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vM93vu_xpFXU"},"source":["# sampler = TPESampler(seed=42)\n","# study = optuna.create_study(\n","#     study_name=\"rf_parameter_opt\",\n","#     direction=\"minimize\",\n","#     sampler=sampler,\n","# )\n","# study.optimize(objective, n_trials=100)\n","# print(\"Best Score:\", study.best_value)\n","# print(\"Best trial:\", study.best_trial.params)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Zmcg4ZnpFTj"},"source":["params = {'n_estimators': 900, 'max_depth': 32, 'min_samples_split': 6, 'min_samples_leaf': 3}\n","\n","X_train, X_valid, y_train, y_valid = train_test_split(train_1[lunch_col].drop(columns=['중식계']), train_1['중식계'], test_size=0.2)\n","model = RandomForestRegressor(**params)\n","model.fit(\n","        X_train,\n","        y_train,\n","        # eval_set=[(X_valid, y_valid)],\n","        # early_stopping_rounds=100,\n","        # verbose=False,\n","    )\n","\n","rf_pred_lunch = model.predict(test[lunch_col].drop(columns=['중식계']))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0pRUBQ4-uO3D"},"source":["### 저녁"]},{"cell_type":"code","metadata":{"id":"ubYB6BkHpFL-"},"source":["# def objective(trial: Trial) -> float :\n","#     params_rf = {\n","#         'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n","#         'max_depth': trial.suggest_int('max_depth', 4, 50),\n","#         'min_samples_split': trial.suggest_int('min_samples_split', 2, 150),\n","#         'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 60),\n","#         'random_state' : 42,\n","#     }\n","#     X_train, X_valid, y_train, y_valid = train_test_split(train_2[dinner_col].drop(columns=['석식계']), train_2['석식계'], test_size=0.2)\n","    \n","#     model = RandomForestRegressor(**params_rf)\n","#     model.fit(\n","#         X_train,\n","#         y_train,\n","#         # eval_set=[(X_train, y_train), (X_valid, y_valid)],\n","#         # early_stopping_rounds=100,\n","#         # verbose=False,\n","#     )\n","#     from sklearn.metrics import mean_absolute_error\n","#     rf_pred = model.predict(X_valid)\n","#     mean_absolute_error = (mean_absolute_error(y_valid, rf_pred))\n","    \n","#     return mean_absolute_error"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"op5Q3RV2pFJH"},"source":["# sampler = TPESampler(seed=42)\n","# study = optuna.create_study(\n","#     study_name=\"rf_parameter_opt\",\n","#     direction=\"minimize\",\n","#     sampler=sampler,\n","# )\n","# study.optimize(objective, n_trials=100)\n","# print(\"Best Score:\", study.best_value)\n","# print(\"Best trial:\", study.best_trial.params)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YMhAMaKzpFFx"},"source":["params = {'n_estimators': 974, 'max_depth': 19, 'min_samples_split': 17, 'min_samples_leaf': 1}\n","\n","X_train, X_valid, y_train, y_valid = train_test_split(train_2[dinner_col].drop(columns=['석식계']), train_2['석식계'], test_size=0.2)\n","model = RandomForestRegressor(**params)\n","model.fit(\n","        X_train,\n","        y_train,\n","        # eval_set=[(X_valid, y_valid)],\n","        # early_stopping_rounds=100,\n","        # verbose=False,\n","    )\n","\n","rf_pred_dinner = model.predict(test[dinner_col].drop(columns=['석식계']))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wvg8ZOEUv8u7"},"source":["## Extra Tree"]},{"cell_type":"markdown","metadata":{"id":"Eo7NZwZVv-iU"},"source":["### 점심"]},{"cell_type":"code","metadata":{"id":"jTYHb-oOv-tg"},"source":["from sklearn.ensemble import ExtraTreesRegressor"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oCXc8ziqv-3Q"},"source":["def objective(trial: Trial) -> float :\n","    params_extra = {\n","        'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n","        'max_depth': trial.suggest_int('max_depth', 4, 50),\n","        'min_samples_split': trial.suggest_int('min_samples_split', 2, 150),\n","        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 60),\n","        'random_state' : 42,\n","    }\n","    X_train, X_valid, y_train, y_valid = train_test_split(train_1.drop(columns=['중식계']), train_1['중식계'], test_size=0.2)\n","    \n","    model = ExtraTreesRegressor(**params_extra)\n","    model.fit(\n","        X_train,\n","        y_train,\n","        # eval_set=[(X_train, y_train), (X_valid, y_valid)],\n","        # early_stopping_rounds=100,\n","        # verbose=False,\n","    )\n","    from sklearn.metrics import mean_absolute_error\n","    extra_pred = model.predict(X_valid)\n","    mean_absolute_error = (mean_absolute_error(y_valid, extra_pred))\n","    \n","    return mean_absolute_error"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HJF3CMkTv_Eb"},"source":["sampler = TPESampler(seed=42)\n","study = optuna.create_study(\n","    study_name=\"extra_parameter_opt\",\n","    direction=\"minimize\",\n","    sampler=sampler,\n",")\n","study.optimize(objective, n_trials=100)\n","print(\"Best Score:\", study.best_value)\n","print(\"Best trial:\", study.best_trial.params)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eAEWAwa8v8qy"},"source":["params = {'n_estimators': 908, 'max_depth': 24, 'min_samples_split': 20, 'min_samples_leaf': 12}\n","\n","X_train, X_valid, y_train, y_valid = train_test_split(train_1[lunch_col].drop(columns=['중식계']), train_1['중식계'], test_size=0.2)\n","model = ExtraTreesRegressor(**params)\n","model.fit(\n","        X_train,\n","        y_train,\n","        # eval_set=[(X_valid, y_valid)],\n","        # early_stopping_rounds=100,\n","        # verbose=False,\n","    )\n","\n","extra_pred_lunch = model.predict(test[lunch_col].drop(columns=['중식계']))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cYmVA-6jxTUn"},"source":["### 저녁"]},{"cell_type":"code","metadata":{"id":"zdHZeH9BxTQm"},"source":["# def objective(trial: Trial) -> float :\n","#     params_extra = {\n","#         'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n","#         'max_depth': trial.suggest_int('max_depth', 4, 50),\n","#         'min_samples_split': trial.suggest_int('min_samples_split', 2, 150),\n","#         'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 60),\n","#         'random_state' : 42,\n","#     }\n","#     X_train, X_valid, y_train, y_valid = train_test_split(train_2[dinner_col].drop(columns=['석식계']), train_2['석식계'], test_size=0.2)\n","    \n","#     model = ExtraTreesRegressor(**params_extra)\n","#     model.fit(\n","#         X_train,\n","#         y_train,\n","#         # eval_set=[(X_train, y_train), (X_valid, y_valid)],\n","#         # early_stopping_rounds=100,\n","#         # verbose=False,\n","#     )\n","#     from sklearn.metrics import mean_absolute_error\n","#     extra_pred = model.predict(X_valid)\n","#     mean_absolute_error = (mean_absolute_error(y_valid, extra_pred))\n","    \n","#     return mean_absolute_error"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9U3HCNvixTMf"},"source":["# sampler = TPESampler(seed=42)\n","# study = optuna.create_study(\n","#     study_name=\"extra_parameter_opt\",\n","#     direction=\"minimize\",\n","#     sampler=sampler,\n","# )\n","# study.optimize(objective, n_trials=100)\n","# print(\"Best Score:\", study.best_value)\n","# print(\"Best trial:\", study.best_trial.params)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rgmer0wQxTHt"},"source":["params = {'n_estimators': 383, 'max_depth': 49, 'min_samples_split': 16, 'min_samples_leaf': 6}\n","\n","X_train, X_valid, y_train, y_valid = train_test_split(train_2[dinner_col].drop(columns=['석식계']), train_2['석식계'], test_size=0.2)\n","model = ExtraTreesRegressor(**params)\n","model.fit(\n","        X_train,\n","        y_train,\n","        # eval_set=[(X_valid, y_valid)],\n","        # early_stopping_rounds=100,\n","        # verbose=False,\n","    )\n","\n","extra_pred_dinner = model.predict(test[dinner_col].drop(columns=['석식계']))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XlhoYSUxUUKe"},"source":["## 제출 - Stacking Ensemble"]},{"cell_type":"code","metadata":{"id":"Dv55_aRnUiIb"},"source":["sample_submission = pd.read_csv(path+'sample_submission.csv')\n","submission = sample_submission.copy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5YZh0LtM8Coc"},"source":["submission['중식계'] = lgbm_pred_lunch\n","submission['석식계'] = lgbm_pred_dinner"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VQbaKYOPUmJH"},"source":["submission['중식계'] = (lgbm_pred_lunch*0.6+xgboost_pred_lunch*0.4)\n","submission['석식계'] = (lgbm_pred_dinner*0.6+xgboost_pred_dinner*0.4)\n","\n","# submission['중식계'] = (lgbm_pred_lunch+xgboost_pred_lunch)/2\n","# submission['석식계'] = (lgbm_pred_dinner+xgboost_pred_dinner)/2\n","\n","# submission['중식계'] = (catboost_pred_lunch+lgbm_pred_lunch+xgboost_pred_lunch+rf_pred_lunch+extra_pred_lunch)/5\n","# submission['석식계'] = (catboost_pred_dinner+lgbm_pred_dinner+xgboost_pred_dinner+rf_pred_dinner+extra_pred_dinner)/5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6QmGsIMAUNQ-"},"source":["from datetime import datetime\n","now_tm = datetime.today().strftime('%Y-%m-%d-%H:%M:%S')\n","\n","sub_path = '/content/drive/MyDrive/DACON/Dacon_Industry_Meal/submit/'\n","submission.to_csv(sub_path+now_tm+'.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gjoU2-DLRwUu"},"source":["## Flaml"]},{"cell_type":"code","metadata":{"id":"J7yTWBAq56_4"},"source":["import numpy as np\n","import shap\n","def get_columns(model, X_test, train, del_col_nm='석식계'):\n","  explainer = shap.Explainer(model._model)\n","  shap_values = explainer(X_test)\n","  vals = np.abs(shap_values.values).mean(0)\n","  cols = list(train.columns)\n","  cols.remove(del_col_nm)\n","  feature_importance = pd.DataFrame(list(zip(cols, vals)), columns=['col_name','feature_importance_vals'])\n","  feature_importance.sort_values(by=['feature_importance_vals'], ascending=False, inplace=True)\n","  return feature_importance[feature_importance['feature_importance_vals'] > 1]['col_name'].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JARYWbqt6jvM"},"source":["from sklearn.model_selection import KFold\n","from sklearn.preprocessing import StandardScaler\n","\n","def run_kfold(train, test, selected_cols=[], target_col='중식계'):\n","    folds=KFold(n_splits=5, shuffle=True, random_state=2021)\n","    outcomes=[]\n","    model = None\n","    sub=[]\n","\n","    # X = None\n","    # X_test = None\n","    # if len(selected_cols) > 0:\n","    #   X = np.array(train[selected_cols])\n","    #   X_test = np.array(test[selected_cols])\n","    # else:\n","    #   X = np.array(train.drop(columns=[target_col]))\n","    #   cols = list(train.columns)\n","    #   cols.remove(target_col)\n","    #   X_test = np.array(test[cols])\n","\n","    # y = np.array(train[target_col])\n","    X = np.array(train.drop(columns=[target_col]))\n","    cols = list(train.columns)\n","    cols.remove(target_col)\n","    X_test = np.array(test[cols])\n","    y = np.array(train[target_col])\n","\n","    # 정규화\n","    standardScaler = StandardScaler()\n","    standardScaler.fit(X)\n","    X = standardScaler.transform(X)\n","\n","    X_test = standardScaler.transform(X_test) # 정규화\n","    cols_import = []\n","\n","    for n_fold, (train_index, val_index) in enumerate(folds.split(X)):\n","        print(n_fold, 'fold started =========================================')\n","        # X_train1, X_val = , X_train.loc[val_index]\n","        y_train = y[train_index]\n","        X_train = X[train_index]\n","        # y_train1, y_val = y_train.loc[train_index], y_train.loc[val_index]\n","        y_val = y[val_index]\n","        X_val = X[val_index]\n","        if n_fold == 0:\n","          automl = AutoML()\n","          automl_settings = {\n","              \"time_budget\": 120,  # in seconds\n","              \"metric\": 'mae',\n","              \"task\": 'regression'\n","          }\n","          automl.fit(X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val, **automl_settings)\n","          model = automl.model\n","        else:\n","          model.fit(X_train, y_train, eval_set=[(X_val, y_val)],\n","                        eval_metric='mae', early_stopping_rounds=10) \n","        \n","        pred1 = model.predict(X_test)\n","        if len(selected_cols) == 0:\n","          cols_import += list(get_columns(model, X_test, train=train, del_col_nm=target_col))\n","        sub.append(pred1)\n","    return sub, model, X_test, set(cols_import)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3rqhMdnC7qD4"},"source":["my_submission, model, X_test, cols_import = run_kfold(train_1, test, selected_cols=[], target_col='중식계')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uKBj0moG5Q5B"},"source":["pred1 = np.mean(my_submission, axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OQ9AE_4e5S3n"},"source":["my_submission, model, X_test, cols_import = run_kfold(train_2, test, selected_cols=[], target_col='석식계')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oCkQhcmU5XF0"},"source":["pred2 = np.mean(my_submission, axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0zicAX8FQHz_"},"source":["# def run_kfold2(train, test):\n","#     folds=KFold(n_splits=5, shuffle=True, random_state=2021)\n","#     outcomes=[]\n","#     model = None\n","#     sub=[]\n","\n","#     X = np.array(train.drop(columns=['석식계']))\n","#     y = np.array(train['석식계'])\n","#     # 정규화\n","#     standardScaler = StandardScaler()\n","#     standardScaler.fit(X)\n","#     X = standardScaler.transform(X)\n","\n","#     cols = list(train.columns)\n","#     cols.remove('석식계')\n","#     X_test = np.array(test[cols])\n","#     X_test = standardScaler.transform(X_test) # 정규화\n","\n","#     # X = np.array(X)\n","\n","#     for n_fold, (train_index, val_index) in enumerate(folds.split(X)):\n","#         print(n_fold, 'fold started =========================================')\n","#         # X_train1, X_val = , X_train.loc[val_index]\n","#         y_train = y[train_index]\n","#         X_train = X[train_index]\n","#         # y_train1, y_val = y_train.loc[train_index], y_train.loc[val_index]\n","#         y_val = y[val_index]\n","#         X_val = X[val_index]\n","#         if n_fold == 0:\n","#           automl = AutoML()\n","#           automl_settings = {\n","#               \"time_budget\": 240,  # in seconds\n","#               \"metric\": 'mae',\n","#               \"task\": 'regression'\n","#           }\n","#           # print(len(X_train1))\n","#           # print(len(y_train1.to_numpy()))\n","#           automl.fit(X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val, **automl_settings)\n","#           model = automl.model\n","#         else:\n","#           model.fit(X_train, y_train, eval_set=[(X_val, y_val)],\n","#                         eval_metric='mae', early_stopping_rounds=10) \n","        \n","#         pred1 = model.predict(X_test)\n","#         sub.append(pred1)\n","#     return sub\n","\n","# my_submission = run_kfold2(train_2, test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2jt2ARcrQLdU"},"source":["sample_submission = pd.read_csv(path+'sample_submission.csv')\n","submission = sample_submission.copy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DgN_9TE0QML8"},"source":["submission['중식계'] = pred1\n","submission['석식계'] = pred2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ej4FbP7QNXF"},"source":["from datetime import datetime\n","now_tm = datetime.today().strftime('%Y-%m-%d-%H:%M:%S')\n","\n","sub_path = '/content/drive/MyDrive/DACON/Dacon_Industry_Meal/submit/'\n","submission.to_csv(sub_path+now_tm+'.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6U1sTx358yj-"},"source":["!pip install /content/drive/MyDrive/구내식당/water/dacon_submit_api-0.0.4-py3-none-any.whl"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zO4bqf739GsW"},"source":["from dacon_submit_api import dacon_submit_api \n","\n","result = dacon_submit_api.post_submission_file(\n","'/content/drive/MyDrive/DACON/Dacon_Industry_Meal/submit/2021-07-22-02:37:54.csv', \n","'c7eb6960dc46f188d012c09ac2fbde1a0f90af05eeef92249719473834bb981e', \n","'235743', \n","'datu', \n","'optuna')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LE_WTj5OCvsW"},"source":["## Pycaret"]},{"cell_type":"markdown","metadata":{"id":"_9H957l5l16L"},"source":["feature_selection - 전진 단계별 선택법"]},{"cell_type":"code","metadata":{"id":"o5lUYMDml50w"},"source":["import statsmodels.api as sm\n","## 전진 단계별 선택법 - 중식\n","lunch_cols = train_1.columns.tolist()\n","lunch_cols.remove('중식계')\n","lunch_cols.remove('일자')\n","variables = lunch_cols ## 설명 변수 리스트\n"," \n","y = train_1['중식계'] ## 반응 변수\n","selected_variables = [] ## 선택된 변수들\n","sl_enter = 0.05\n","sl_remove = 0.05\n"," \n","sv_per_step = [] ## 각 스텝별로 선택된 변수들\n","adjusted_r_squared = [] ## 각 스텝별 수정된 결정계수\n","steps = [] ## 스텝\n","step = 0\n","while len(variables) > 0:\n","    remainder = list(set(variables) - set(selected_variables))\n","    pval = pd.Series(index=remainder) ## 변수의 p-value\n","    ## 기존에 포함된 변수와 새로운 변수 하나씩 돌아가면서 \n","    ## 선형 모형을 적합한다.\n","    for col in remainder: \n","        X = train_1[selected_variables+[col]]\n","        X = sm.add_constant(X)\n","        model = sm.OLS(y,X).fit()\n","\n","        pval[col] = model.pvalues[col]\n"," \n","    min_pval = pval.min()\n","    if min_pval < sl_enter: ## 최소 p-value 값이 기준 값보다 작으면 포함\n","        selected_variables.append(pval.idxmin())\n","        ## 선택된 변수들에대해서\n","        ## 어떤 변수를 제거할지 고른다.\n","        while len(selected_variables) > 0:\n","            selected_X = train_1[selected_variables]\n","            selected_X = sm.add_constant(selected_X)\n","            selected_pval = sm.OLS(y,selected_X).fit().pvalues[1:] ## 절편항의 p-value는 뺀다\n","            max_pval = selected_pval.max()\n","            if max_pval >= sl_remove: ## 최대 p-value값이 기준값보다 크거나 같으면 제외\n","                remove_variable = selected_pval.idxmax()\n","                selected_variables.remove(remove_variable)\n","            else:\n","                break\n","        \n","        step += 1\n","        steps.append(step)\n","        adj_r_squared = sm.OLS(y,sm.add_constant(train_1[selected_variables])).fit().rsquared_adj\n","        adjusted_r_squared.append(adj_r_squared)\n","        sv_per_step.append(selected_variables.copy())\n","    else:\n","        break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s7FQXO5kl6HA"},"source":["fig = plt.figure(figsize=(10,10))\n","fig.set_facecolor('white')\n"," \n","font_size = 15\n","plt.xticks(steps,[f'step {s}\\n'+'\\n'.join(sv_per_step[i]) for i,s in enumerate(steps)], fontsize=12)\n","plt.plot(steps, adjusted_r_squared, marker='o')\n","    \n","plt.ylabel('Adjusted R Squared',fontsize=font_size)\n","plt.grid(True)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rJP7mDNAl6KH"},"source":["train_1_tmp = train_1[selected_variables]\n","train_1_tmp['중식계'] = train_1['중식계']\n","train_1 = train_1_tmp"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oWqcYAsjl6OC"},"source":["## 전진 단계별 선택법 - 석식\n","dinner_cols = train_2.columns.tolist()\n","dinner_cols.remove('석식계')\n","dinner_cols.remove('일자')\n","variables = dinner_cols ## 설명 변수 리스트\n"," \n","y = train_2['석식계'] ## 반응 변수\n","selected_variables_dinner = [] ## 선택된 변수들\n","sl_enter = 0.05\n","sl_remove = 0.05\n"," \n","sv_per_step = [] ## 각 스텝별로 선택된 변수들\n","adjusted_r_squared = [] ## 각 스텝별 수정된 결정계수\n","steps = [] ## 스텝\n","step = 0\n","while len(variables) > 0:\n","    remainder = list(set(variables) - set(selected_variables_dinner))\n","    pval = pd.Series(index=remainder) ## 변수의 p-value\n","    ## 기존에 포함된 변수와 새로운 변수 하나씩 돌아가면서 \n","    ## 선형 모형을 적합한다.\n","    for col in remainder: \n","        X = train_2[selected_variables_dinner+[col]]\n","        X = sm.add_constant(X)\n","        model = sm.OLS(y,X).fit()\n","\n","        pval[col] = model.pvalues[col]\n"," \n","    min_pval = pval.min()\n","    if min_pval < sl_enter: ## 최소 p-value 값이 기준 값보다 작으면 포함\n","        selected_variables_dinner.append(pval.idxmin())\n","        ## 선택된 변수들에대해서\n","        ## 어떤 변수를 제거할지 고른다.\n","        while len(selected_variables_dinner) > 0:\n","            selected_X = train_2[selected_variables_dinner]\n","            selected_X = sm.add_constant(selected_X)\n","            selected_pval = sm.OLS(y,selected_X).fit().pvalues[1:] ## 절편항의 p-value는 뺀다\n","            max_pval = selected_pval.max()\n","            if max_pval >= sl_remove: ## 최대 p-value값이 기준값보다 크거나 같으면 제외\n","                remove_variable = selected_pval.idxmax()\n","                selected_variables_dinner.remove(remove_variable)\n","            else:\n","                break\n","        \n","        step += 1\n","        steps.append(step)\n","        adj_r_squared = sm.OLS(y,sm.add_constant(train_2[selected_variables_dinner])).fit().rsquared_adj\n","        adjusted_r_squared.append(adj_r_squared)\n","        sv_per_step.append(selected_variables_dinner.copy())\n","    else:\n","        break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jc4NgqVEl-wE"},"source":["fig = plt.figure(figsize=(10,10))\n","fig.set_facecolor('white')\n"," \n","font_size = 15\n","plt.xticks(steps,[f'step {s}\\n'+'\\n'.join(sv_per_step[i]) for i,s in enumerate(steps)], fontsize=12)\n","plt.plot(steps, adjusted_r_squared, marker='o')\n","    \n","plt.ylabel('Adjusted R Squared',fontsize=font_size)\n","plt.grid(True)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4P4WL--4mAbw"},"source":["train_2_tmp = train_2[selected_variables_dinner]\n","train_2_tmp['석식계'] = train_2['석식계']\n","train_2 = train_2_tmp"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fh5eAD5nmBud"},"source":["# test 데이터\n","cols_lunch = selected_variables\n","cols_dinner = selected_variables_dinner\n","test_lunch = test[cols_lunch]\n","test_dinner = test[cols_dinner]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sLHhl6zpTxUy"},"source":["reg = setup(data=train_1,\n","            target='중식계',\n","            numeric_imputation = 'mean',\n","            normalize = True,\n","            silent= True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lDOqdaKWT6xm"},"source":["best_5 = compare_models(sort='MAE', n_select=5)\n","blended = blend_models(estimator_list= best_5, fold=5, optimize='MAE')\n","pred_holdout = predict_model(blended)\n","final_model = finalize_model(blended)\n","pred1 = predict_model(final_model, test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BCdlhjgnU39F"},"source":["sample_submission = pd.read_csv(path+'sample_submission.csv')\n","submission = sample_submission.copy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J1fj6sjNVInL"},"source":["submission['중식계'] = pred1.reset_index()['Label']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0KblKSlEIUTm"},"source":["reg = setup(data=train_2,\n","            target='석식계',\n","            numeric_imputation = 'mean',\n","            normalize = True,\n","            silent= True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y42zsb9KIXPE"},"source":["best_5 = compare_models(sort='MAE', n_select=5)\n","blended = blend_models(estimator_list= best_5, fold=5, optimize='MAE')\n","pred_holdout = predict_model(blended)\n","final_model = finalize_model(blended)\n","pred2 = predict_model(final_model, test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IPmlmWLBIXGG"},"source":["submission['석식계'] = pred2.reset_index()['Label']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XFfBt8X5VKNr"},"source":["sub_path = '/content/drive/MyDrive/DACON/Dacon_Industry_Meal/submit/'\n","best_submit = pd.read_csv(sub_path+'20210605_01_79.csv')\n","df_82 = pd.read_csv(sub_path+'20210608_01_holiday_82.csv')\n","from sklearn.metrics import mean_absolute_error\n","def show_mae(data) : \n","    result = mean_absolute_error(best_submit['중식계'], data['중식계'])+mean_absolute_error(best_submit['석식계'], data['석식계'])\n","    return display(result)\n","\n","show_mae(submission)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n-RXYMOBZA3O"},"source":["submission.to_csv(sub_path+'/20210619_02.csv', index=False)"],"execution_count":null,"outputs":[]}]}
=======
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LH_Model.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1l2NH5yafpIojJ-f0dM0WEDv-rwEiHKNw",
      "authorship_tag": "ABX9TyNcu/GqcQWGeB6FBVBivy3Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/herjh0405/DACON_Meal/blob/master/LH_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrP5riasfpzN"
      },
      "source": [
        "!pip install catboost\n",
        "!pip install pycaret\n",
        "!pip install kaggler\n",
        "!pip install pendulum\n",
        "!pip install flaml\n",
        "!pip install shap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKXMNXLhfsoG"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "import datetime\n",
        "np.random.seed(0)\n",
        "\n",
        "from pycaret.regression import *\n",
        "from kaggler.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import roc_auc_score, log_loss\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import os, re\n",
        "import glob\n",
        "import calendar\n",
        "\n",
        "from flaml import AutoML\n",
        "import statsmodels.api as sm\n",
        "import pendulum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qUC3VbFfuP2"
      },
      "source": [
        "# 한글 폰트 사용\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "import os\n",
        "\n",
        "def change_matplotlib_font(font_download_url):\n",
        "    FONT_PATH = 'MY_FONT'\n",
        "    \n",
        "    font_download_cmd = f\"wget {font_download_url} -O {FONT_PATH}.zip\"\n",
        "    unzip_cmd = f\"unzip -o {FONT_PATH}.zip -d {FONT_PATH}\"\n",
        "    os.system(font_download_cmd)\n",
        "    os.system(unzip_cmd)\n",
        "    \n",
        "    font_files = fm.findSystemFonts(fontpaths=FONT_PATH)\n",
        "    for font_file in font_files:\n",
        "        fm.fontManager.addfont(font_file)\n",
        "\n",
        "    font_name = fm.FontProperties(fname=font_files[2]).get_name()\n",
        "    matplotlib.rc('font', family=font_name)\n",
        "    print(\"font family: \", plt.rcParams['font.family'])\n",
        "\n",
        "font_download_url = \"https://fonts.google.com/download?family=Nanum%20Gothic\"\n",
        "change_matplotlib_font(font_download_url)\n",
        "# 마이너스 폰트 깨짐 방지\n",
        "plt.rcParams['axes.unicode_minus'] = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0l0Cf0Sj6MD"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3PKLkpn7Pmj"
      },
      "source": [
        "# path = '/content/drive/MyDrive/구내식당/water/'\n",
        "# train = pd.read_csv(path+'train.csv')\n",
        "# test = pd.read_csv(path+'test.csv')\n",
        "# holiday = pd.read_csv(path+'holidays.csv', index_col=0)\n",
        "# corona = pd.read_csv(path+'corona_data.csv')\n",
        "\n",
        "# df = pd.concat([train.iloc[:, :-2], test])\n",
        "# target_df = train.iloc[:, -2:]\n",
        "# df.columns = ['일자', '요일', '정원','휴가자', '출장자', '야근자',\\\n",
        "#                  '재택근무자', '조식', '중식', '석식']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQYZdKJv-nPm"
      },
      "source": [
        "## 메뉴 관련 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ykDEFEb-1VI"
      },
      "source": [
        "# Menu-extracting function\n",
        "def extractMenu(array, keywords=[], not_in_keywords={}, comm_not_in=[]):\n",
        "  extractedMenu = []\n",
        "  for menu_nm in array:\n",
        "    for kw in keywords:\n",
        "      if menu_nm.find(kw) > -1:\n",
        "        has_not_in = False\n",
        "        if kw in not_in_keywords:\n",
        "          for sub_kw in not_in_keywords[kw]:\n",
        "            if menu_nm.find(sub_kw) > -1:\n",
        "              has_not_in = True\n",
        "              break\n",
        "        for sub_kw in comm_not_in:\n",
        "            if menu_nm.find(sub_kw) > -1:\n",
        "              has_not_in = True\n",
        "              break\n",
        "\n",
        "        if not has_not_in:\n",
        "          extractedMenu.append(menu_nm)\n",
        "          break\n",
        "  return(extractedMenu)\n",
        "\n",
        "def extractMenu2(array, keywords=[]):\n",
        "  extractedMenu = []\n",
        "  for menu_nm in tot_menu_arr:\n",
        "    for kw in keywords:\n",
        "      if menu_nm.find(kw) > -1:\n",
        "        menu_nm_list = re.split(r'[^\\w]', menu_nm)\n",
        "        for menu_nm_tmp in menu_nm_list:\n",
        "          if menu_nm_tmp.find(kw) + len(kw) == len(menu_nm_tmp): # 끝에 있으면\n",
        "            extractedMenu.append(menu_nm)\n",
        "        break\n",
        "  return(extractedMenu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MUhPAob-3kg"
      },
      "source": [
        "lunch_menu_data = df['중식']\n",
        "dinner_menu_data = df['석식']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61mXHJgI-3Sz"
      },
      "source": [
        "tot_menu_arr = []\n",
        "pattern = r\"\\(.*\\)\"\n",
        "for menu_data in [lunch_menu_data, dinner_menu_data]:\n",
        "  for daily_menu in menu_data:\n",
        "    menu_list = daily_menu.strip().split()\n",
        "    menu_list2 = []\n",
        "    for i, menu_nm in enumerate(menu_list):\n",
        "      menu_nm = re.sub(pattern, '', menu_nm)\n",
        "      if menu_nm.strip() in ['', '*']:\n",
        "        continue\n",
        "      if menu_nm[0] == '(' or menu_nm[-1] == ')':\n",
        "        continue\n",
        "      menu_list2.append(menu_nm)\n",
        "    tot_menu_arr += menu_list2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjBtsk_j-2xT"
      },
      "source": [
        "tot_menu_arr = set(tot_menu_arr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsTj9nrV--xG"
      },
      "source": [
        "len(tot_menu_arr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEjJtWFW--uV"
      },
      "source": [
        "# 육류 분류\n",
        "# 소고기\n",
        "# https://namu.wiki/w/%EC%87%A0%EA%B3%A0%EA%B8%B0\n",
        "beef = ['소고기', '쇠고기', '불고기', '떡갈비', '갈비찜', '소갈비', '육사시미', '육회', '장조림', '와규', '야키니쿠', '규동', '스테이크', '햄버그 스테이크',\n",
        " '함박스테이크', '함바그스테이크', '함박 스테이크', '햄버거', '로스트 디너', '비프가스밀라네사', '웰링턴', '슈하스쿠', '아사도', '우육면',\n",
        " '육개장', '육포', '평양냉면', '비프 스트로가노프', '설렁탕', '소고기국', '소머리국밥', '곰탕', '너비아니', '보르챠', '소꼬리']\n",
        "# 돼지고기\n",
        "# https://namu.wiki/w/%EB%8F%BC%EC%A7%80%EA%B3%A0%EA%B8%B0\n",
        "pig = ['돼지', '돼지머리', '머릿고기', '뒷고기', '관자살', '콧등살', '삼각살', '설중살', '설하살', '안중살', '뽈항정살',\n",
        " '볼살', '두항정', '돼지코', '항정살', '목살', '가브리살', '갈비', '앞다리살', '갈매기살', '등심', '안심',\n",
        " '삼겹살', '오겹살', '뒷다리살', '돈족', '내장', '오소리감투', '허파', '염통', '콩팥', '새끼보', '돈낭',\n",
        " '돈족', '돼지꼬리', '사태', '막창', '감자탕', '돈가스', '돼지갈비', '돼지국밥', '돼지불고기', '두루치기', '순대',\n",
        " '순댓', '족발', '보쌈', '수육', '편육', '제육', '탕수육', '삼겹', '맥적', '차슈', '향우구육', '꿔바로우', '훙사오러우',\n",
        " '회과육', '동파육', '라후테', '오향장육', '슈바인스학세', '소시지', '소세지', '포크 커틀릿', '함바그 스테이크', '함바그스테이크',\n",
        " '함박스테이크', '살스테이크','살 스테이크', '함박 스테이크', '베이컨', '햄', '스팸', '폭립', '폭찹', '돈지루', '부타동', '바쿠테', '팟 카파오 무 쌉', '비엔나', '소떡', '육']\n",
        "# 닭고기\n",
        "# https://namu.wiki/w/%EB%8B%AD%EA%B3%A0%EA%B8%B0\n",
        "chicken = ['닭', '깐풍기', '꼬꼬면', '궁보계정', '간장닭', '기스면', '계', '도빙무시', '라조기', '백숙', '영계백숙',\n",
        " '불닭', '삼계탕', '삼계선', '오니시메', '옻닭', '연팔기', '유린기', '육회', '좌종당계', '찜닭', '초계밀면',\n",
        " '치킨', '도리텐', '지파이', '치짜', '취계', '카라아게', '가라아', '파닭', '양파닭', '케밥', '코코뱅', '탕수기',\n",
        " '포계', '프랑구 아사두']\n",
        "# 양고기\n",
        "# https://namu.wiki/w/%EC%96%91%EA%B3%A0%EA%B8%B0\n",
        "sheep = ['양고기','훠궈', '양꼬치', '케밥', '샤슬릭', '징기스칸', '셰퍼드 파이', '허르헉', '양갈비']\n",
        "# 오리고기\n",
        "# https://namu.wiki/w/%EC%98%A4%EB%A6%AC%EA%B3%A0%EA%B8%B0\n",
        "dug = ['오리']\n",
        "\n",
        "web_keywords = beef + pig + chicken + sheep + dug\n",
        "keywords = ['돈까스', '히레카츠', '히레까쓰', '히레가스', '포크', '부대찌개', '뒷다리', '앞다리', '돈', '순살',\n",
        "                '소머리', '등뼈', '곱창', '도가니', '뼈해장국', '뼈다귀해장국', '목심', '채끝', '우둔', '양지', '설도', '만두', '만둣',\n",
        "                '잡채', '류산슬', '유산슬', '고기', '고깃']\n",
        "keywords += web_keywords\n",
        "\n",
        "not_in_keywords = {'오리':['아오리', '오리엔탈'], '계':['계란', '계발', '계피'], '장조림':['계란', '메추리알'], '치킨':['치킨무'], '돈':['돈나물'], '만두':['당면계란'], '만둣':['당면계란']}\n",
        "meat_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords=not_in_keywords, comm_not_in=[])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-I02EzT--qc"
      },
      "source": [
        "# 돼지고기\n",
        "keywords = ['돼지', '돼지머리', '머릿고기', '뒷고기', '관자살', '콧등살', '삼각살', '설중살', '설하살', '안중살', '뽈항정살',\n",
        " '볼살', '두항정', '돼지코', '항정살', '목살', '가브리살', '앞다리살', '갈매기살', '등심', '안심',\n",
        " '삼겹살', '오겹살', '앞다리살', '뒷다리살', '돈족', '내장', '오소리감투', '허파', '염통', '콩팥', '새끼보', '돈낭',\n",
        " '돈족', '돼지꼬리', '사태', '막창', '감자탕', '돈가스', '돼지갈비', '돼지국밥', '돼지불고기', '두루치기', '순대',\n",
        " '순댓', '족발', '보쌈', '수육', '편육', '제육', '탕수육', '삼겹', '맥적', '차슈', '향우구육', '꿔바로우', '훙사오러우',\n",
        " '회과육', '동파육', '라후테', '오향장육', '슈바인스학세', '소시지', '소세지', '포크 커틀릿',\n",
        " '목살스테이크','목살 스테이크', '베이컨', '햄', '스팸', '폭립', '폭찹', '돈지루', '부타동', '바쿠테', '팟 카파오 무 쌉', '비엔나', '소떡',\n",
        " '돈까스', '히레카츠', '히레까쓰', '히레가스', '포크', '돈', '등뼈', '뼈해장국', '뼈다귀해장국']\n",
        "not_in_keywords = {'돈':['돈나물'], '만두':['당면계란'], '만둣':['당면계란']}\n",
        "pig_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords=not_in_keywords, comm_not_in=[])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkndYkKF--mD"
      },
      "source": [
        "# 소고기\n",
        "keywords = ['소고기', '쇠고기', '소불고기', '소갈비', '육사시미', '육회', '와규', '야키니쿠', '규동', '소곱창',\n",
        "            '로스트 디너', '비프가스밀라네사', '웰링턴', '슈하스쿠', '아사도', '우육면',\n",
        "            '육개장', '육포', '평양냉면', '비프 스트로가노프', '설렁탕', '소고기국', '소머리국밥', '곰탕', '너비아니', '보르챠', '소꼬리', '소머리', '설도', '목심', '채끝', '우둔', '양지', '도가니']\n",
        "beef_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords={}, comm_not_in=[])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zu5btCQl_ExN"
      },
      "source": [
        "# 닭고기\n",
        "keywords = ['닭', '깐풍기', '꼬꼬면', '궁보계정', '간장닭', '기스면', '계', '도빙무시', '라조기', '백숙', '영계백숙',\n",
        "          '불닭', '삼계탕', '삼계선', '오니시메', '옻닭', '연팔기', '유린기', '육회', '좌종당계', '찜닭', '초계밀면',\n",
        "          '치킨', '도리텐', '지파이', '치짜', '취계', '카라아게', '가라아', '파닭', '양파닭', '케밥', '코코뱅', '탕수기',\n",
        "          '포계', '프랑구 아사두']\n",
        "not_in_keywords = {'계':['계란', '계발', '계피'], '치킨':['치킨무']}\n",
        "\n",
        "chicken_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords=not_in_keywords, comm_not_in=[])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyBscZPX_Eq5"
      },
      "source": [
        "# 양고기 - 데이터 없어서 제외\n",
        "keywords = ['양고기','훠궈', '양꼬치', '케밥', '샤슬릭', '징기스칸', '셰퍼드 파이', '허르헉', '양갈비']\n",
        "sheep_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords={}, comm_not_in=[])\n",
        "sheep_menus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29errrks_Em8"
      },
      "source": [
        "# 오리고기\n",
        "keywords = ['오리']\n",
        "not_in_keywords = {'오리':['아오리', '오리엔탈']}\n",
        "duck_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords=not_in_keywords, comm_not_in=[])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIiPZsJq_Eik"
      },
      "source": [
        "#난류 (계란)\n",
        "keywords = ['계란', '난', '란', '메추리알', '날치알', '동태알']\n",
        "not_in_keywords = {\"란\":['토란'], '난':['커리', '카레']}\n",
        "egg_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords=not_in_keywords, comm_not_in=[])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCvsvlx8_Ed1"
      },
      "source": [
        "# 죽류\n",
        "keywords = ['죽', '누룽지']\n",
        "juk_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords={}, comm_not_in=[])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vfiEY18_EX2"
      },
      "source": [
        "# 덮밥 및 국밥류\n",
        "keywords = ['덮밥', '국밥']\n",
        "gukbob_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords={}, comm_not_in=[])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNL-w6Fm--hI"
      },
      "source": [
        "# 비빔밥 및 볶음밥류\n",
        "keywords = ['비빔밥', '볶음밥']\n",
        "bb_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords={}, comm_not_in=[])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5i8uzCd2_NQ-"
      },
      "source": [
        "# 국탕류\n",
        "keywords = ['국', '탕', '찌개', '국물']\n",
        "soup_menus = extractMenu2(tot_menu_arr, keywords=keywords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJtEwngK_Ndc"
      },
      "source": [
        "# 구이류\n",
        "keywords = ['구이']\n",
        "gui_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords={}, comm_not_in=[])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hx1ksLEc_NlR"
      },
      "source": [
        "# 전류\n",
        "keywords = ['전', '부침개', '빈대떡']\n",
        "jeon_menus = extractMenu2(tot_menu_arr, keywords=keywords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDXN4Jft_Nrt"
      },
      "source": [
        "# http://yaksik.net/detail.php?number=24904\n",
        "# 튀김류\n",
        "keywords = ['튀김', '까스', '카츠', '가츠', '까츠', '탕수', '덴뿌라', '덴푸라', '크로켓', '고로케', '맛탕', '치킨', '통닭', '부각', '강정', '김말이', '깐풍']\n",
        "fry_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords={}, comm_not_in=[])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQowLejR_SD6"
      },
      "source": [
        "## 메뉴 추가 특성 - Part1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCcKhNAJ_SAN"
      },
      "source": [
        "# 곡물\n",
        "keywords = ['현미', '밥', '쌀', '보리', '죽', '참깨', '들깨', '수수', '잡곡', '귀리', '퀴노아', '아마란스', '옥수수', '기장', '메밀', '모밀']\n",
        "grain_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords={}, comm_not_in=[])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxOcmQZV_R-G"
      },
      "source": [
        "# 콩류\n",
        "keywords = ['콩', '녹두', '팥', '완두']\n",
        "not_in_keywords = {'콩':['콩나물']}\n",
        "bean_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords=not_in_keywords, comm_not_in=[])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmqzkA84_R6d"
      },
      "source": [
        "# 묵\n",
        "keywords = ['묵']\n",
        "not_in_keywords = {'묵':['어묵', '묵은지']}\n",
        "kor_jelly_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords=not_in_keywords, comm_not_in=[])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQlE4n5R_R3D"
      },
      "source": [
        "# 생선 및 조개류\n",
        "# https://ko.wikipedia.org/wiki/%EC%83%9D%EC%84%A0\n",
        "# https://namu.wiki/w/%EC%83%9D%EC%84%A0\n",
        "# https://namu.wiki/w/%EC%A1%B0%EA%B0%9C\n",
        "keywords = ['생선', '조개', '메기', '송어', '오징어', '굴', '멸치', '숭어', '성게', '고등어', '명태',\n",
        "            '쏨뱅이', '연어', '틸라피아', '우럭', '이리치', '가재', '참바리', '상어', '돔',\n",
        "            '삼치', '방어', '참치', '새우', '문어', '홍어', '농어', '붉평치', '청상아리', '황새치',\n",
        "            '다랑어', '비막치어', '장어', '녹새치', '숭어', '굴비', '조기', '갈치', '꽁치',\n",
        "            '전어', '명태', '노가리', '황태', '은어', '가물치', '쏘가리', '붕어', '잉어', '모래마주', '가자미',\n",
        "            '간재미', '가오리', '박대', '양미리', '과메기', '청어', '생태',\n",
        "            '개복치', '광어', '넙치', '기름치', '까나리', '날치','놀래미'\n",
        "            ,'능성어','달고기','대구','도다리','도루묵','도미','독가시치'\n",
        "            ,'만새기','망상어','문절망둑','물메기','미꾸라지','민어','방어'\n",
        "            ,'추어탕','배스','밴댕이','뱅어','벵에돔','병어','보리멸'\n",
        "            ,'복어','볼락','부세','부시리','붕장어','블루길'\n",
        "            ,'빙어','산천어','서대','시샤모','쏘가리','쏠배감펭','쏨뱅이'\n",
        "            ,'아귀','아구','임연수','전갱이','전복치','점성어','정어리'\n",
        "            ,'준치','쥐치','청새치','청어','향어','홍어','황새치','매운탕'\n",
        "            ,'루테피스크','게맛살','물회','회덮밥','부야베스','북엇국','세꼬시','수르스트뢰밍','식해','어묵','오뎅'\n",
        "            ,'쥐포','추어탕','피시 앤드 칩스','피쉬 앤드 칩스','피시앤드칩스','피쉬앤드칩스','피시앤칩스','피쉬앤칩스','해물'\n",
        "            ,'가리비', '개오지', '꼬막','대칭이','바지락','백합','홍합','소라', '골뱅이', '고둥','재첩'\n",
        "            ,'전복','플라티케라무스', '봉골레', '클램차우더']\n",
        "not_in_keywords = {'굴':['굴소스'], '새우':['새우젓']}\n",
        "fish_shell_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords=not_in_keywords, comm_not_in=[])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAKGLplP_R07"
      },
      "source": [
        "# 채소류\n",
        "# https://namu.wiki/w/%EC%B1%84%EC%86%8C?from=%EC%95%BC%EC%B1%84\n",
        "keywords = ['가지', '갓', '감자', '고구마', '고사리', '고추', '페페론치노', '냉이', '근대', '깻잎', '차조기'\n",
        "            , '당근', '더덕', '도라지', '동아', '딸기', '마', '마늘', '멜론', '무', '무청'\n",
        "            , '바나나', '배추', '버섯', '부추', '브로콜리', '상추', '생강', '쇠비름', '나물'\n",
        "            , '쑥', '시금치', '수박', '시호', '아스파라거스', '야콘', '양파', '여주', '연근', '열무', '오이'\n",
        "            , '우엉', '인삼', '죽순', '청경채', '참외', '칡', '풋콩', '토란', '토마토', '쪽파', '대파', '파인애플'\n",
        "            , '파프리카', '피망', '케일', '고수', '로즈마리', '루타바가', '바질', '박하', '산마늘', '셀러리'\n",
        "            , '아티초크', '타임', '파슬리', '호박', '피클', '파채', '파김치', '채소', '야채']\n",
        "not_in_keywords = {'무':['무침'], '마':'마카로니', '고추':['고추장']}\n",
        "vegetable_menus =  extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords=not_in_keywords, comm_not_in=[])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajuoyOO9_RxJ"
      },
      "source": [
        "# 해조류\n",
        "# https://namu.wiki/w/%EC%A1%B0%EB%A5%98(%EC%88%98%EC%A4%91%EC%83%9D%EB%AC%BC)?from=%ED%95%B4%EC%A1%B0%EB%A5%98\n",
        "keywords = ['김', '우뭇가사리', '한천', '매생이', '파래', '바다포도', '해캄', '클로렐라', '청각', '마리모모스볼', '다시마', '미역', '감태', '톳']\n",
        "not_in_keywords = {'김':['김치', '튀김', '김칫']}\n",
        "sea_alg_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords=not_in_keywords, comm_not_in=[])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2tDD97z_Rtq"
      },
      "source": [
        "# 쌀케익 - 없어서 패스"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GB4-DOpW_RrS"
      },
      "source": [
        "# 발효된 콩 상품 -> 장류\n",
        "# https://namu.wiki/w/%EC%9E%A5%EB%A5%98\n",
        "keywords = ['된장', '간장', '쯔유', '노추', '미소', '고추장', '청국장', '담북장', '팥장', '두부장', '비지장', '어육장', '춘장', '마장', '낫토', '두반장', '해선장', '굴소스', '게장',\n",
        " '장조림', '양념장', '장국', '쌈장', '초장', '*장']\n",
        "jang_menus  = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords={}, comm_not_in=[])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE6zUsAp_Ro6"
      },
      "source": [
        "# 김치\n",
        "# https://namu.wiki/w/%EA%B9%80%EC%B9%98\n",
        "keywords = ['김치', '깍두기', '석박지', '동치미', '겉절이', '묵은지', '소박이', '섞박지', '생채', '게국지', '김칫']\n",
        "kimchi_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords={}, comm_not_in=[])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c1XH9S7_Rmf"
      },
      "source": [
        "# 만두\n",
        "# https://namu.wiki/w/%EB%A7%8C%EB%91%90\n",
        "keywords = ['만두', '춘권', '만쥬', '사모사', '만둣']\n",
        "mandu_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords={}, comm_not_in=[])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCwa_vM6_cs9"
      },
      "source": [
        "# 곡물 가루(밀가루, 쌀가루 등 전분)\n",
        "# https://namu.wiki/w/%EB%B0%80%EA%B0%80%EB%A3%A8\n",
        "# 미숫가루\n",
        "keywords = [\"면\", \"수제비\", \"전\", \"부침개\", \"빵\", \"춘권\", \"튀김\", \"과자\", \"국수\", \"메밀\", \"모밀\", \"피자\", \"전병\", \"떡\", \"어묵\", \"오뎅\", \"소시지\", \"소세지\", \"햄\", \"김밥\", \n",
        "            \"부대찌개\", \"스콘\", \"만두\", \"파이\", \"빈대떡\", \"케이크\", \"케익\", \"쿠키\", \"핫도그\", \"파스타\", \"치킨\", \"라자냐\", \"팟타이\", \"나쵸\", \"팝콘\", '스파게티', '짬뽕']\n",
        "not_in_keywords = {'치킨':['치킨무'], '전':['전주식'], '짬뽕':['고기', '찌개', '국']}\n",
        "powder_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords=not_in_keywords, comm_not_in=[])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXRc_I7B_c4Y"
      },
      "source": [
        "# 과일\n",
        "# https://namu.wiki/w/%EA%B3%BC%EC%9D%BC\n",
        "# https://namu.wiki/w/%EC%88%98%EC%9E%85%20%EA%B3%BC%EC%9D%BC\n",
        "keywords = ['구기자','매실','무화과','버찌','체리','복분자','복숭아','블랙베리','블루베리','딸기','살구','앵두','자두','포도'\n",
        "            ,'감','다래','대추','머루','모과','무화과','배','사과','석류','으름','귤','유자','레드향','천혜향','한라봉'\n",
        "            ,'과라나','구아바','구즈베리','토마토','나랑히야','노니','노팔','니파팜','두꾸','두리안','라임','람부탄'\n",
        "            ,'레몬','애플','루비솔트부쉬','리치','여지','마랑','마룰라','마르멜로','마프랑','망고','블랙베리','아보카도'\n",
        "            ,'아로니아','아사이베리','아사이 베리','양초열매','오렌지','올리브','용안','롱간','자몽','바나나','딸기','수박'\n",
        "            ,'참외','멜론','메론','여주','파인애플','토마토','코코넛','크랜베리','타마린드','파파야','패션프루트','패션후르츠']\n",
        "not_in_keywords = {'살구':['구이', '목살', '삼겹살', '가브리살', '갈비살', '항정살'], '감':['감자'], '배':['배추', '알배기', '소배기']}\n",
        "comm_not_in = ['주스', '쥬스', '음료', 'D', '순']\n",
        "fruit_menus = extractMenu(tot_menu_arr, keywords=keywords, not_in_keywords=not_in_keywords, comm_not_in=comm_not_in)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8lX6Sst_c8F"
      },
      "source": [
        "## 메뉴 추가 특성 - Part2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V-tLXEM_c_v"
      },
      "source": [
        "# 쌀\n",
        "# https://namu.wiki/w/%EA%B3%A1%EB%AC%BC\n",
        "\n",
        "keywords = ['쌀', '잡곡', '오곡', '현미', '흑미', '귀리', '차조', '렌틸콩', '강낭콩', '병아리콩', '완두콩', '기장', '보리', '수수', '호밀'] \n",
        "not_in_keywords = {'쌀':['쌀국수', '찹쌀'], '기장':['장조림'], '수수':['옥수수', '부꾸미']} # 찹쌀은 밥 메뉴명에 쓰이지 않아 삭제\n",
        "comm_not_in = ['스프']\n",
        "rice_menus = extractMenu(tot_menu_arr, keywords, not_in_keywords, comm_not_in)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "er5kRW4h_dCS"
      },
      "source": [
        "# 김밥 및 초밥\n",
        "\n",
        "keywords = ['김밥', '초밥'] \n",
        "not_in_keywords = {'김밥':['볶음밥']}\n",
        "comm_not_in = []\n",
        "gimbab_menus = extractMenu(tot_menu_arr, keywords, not_in_keywords, comm_not_in)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYnsISbF_dGS"
      },
      "source": [
        "# 소금, 식초 등에 절인 해산물\n",
        "# 해산물이 들어있지 않은 절임류도 포함시킴\n",
        "\n",
        "keywords = ['절임', '젓'] \n",
        "not_in_keywords = {}\n",
        "comm_not_in = []\n",
        "saused_menus = extractMenu(tot_menu_arr, keywords, not_in_keywords, comm_not_in)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCIwG9SL_dHy"
      },
      "source": [
        "# 면류\n",
        "# https://femiwiki.com/w/%EB%B6%84%EB%A5%98:%EC%A2%85%EB%A5%98/%EB%A9%B4%EC%9A%94%EB%A6%AC\n",
        "keywords = ['국수', '면', '파스타', '스파게티', '짬뽕', '라면'] \n",
        "not_in_keywords = {'짬뽕':['고기', '찌개', '국']}\n",
        "comm_not_in = []\n",
        "noodle_menus = extractMenu(tot_menu_arr, keywords, not_in_keywords, comm_not_in)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryTAAXbf_dKh"
      },
      "source": [
        "# 스튜 - 조림과 찌개의 중간단계\n",
        "# https://namu.wiki/w/%EC%8A%A4%ED%8A%9C\n",
        "keywords = ['스튜', '조림'] \n",
        "not_in_keywords = {}\n",
        "comm_not_in = []\n",
        "stew_menus = extractMenu(tot_menu_arr, keywords, not_in_keywords, comm_not_in)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFCcLTq5_dNi"
      },
      "source": [
        "# 한국 전통 샐러드\n",
        "keywords = ['나물', '무침'] \n",
        "not_in_keywords = {'나물':['콩나물', '밥']}\n",
        "comm_not_in = []\n",
        "namul_menus = extractMenu(tot_menu_arr, keywords, not_in_keywords, comm_not_in)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FadGmqSj_dPu"
      },
      "source": [
        "# 피클\n",
        "keywords = ['피클'] \n",
        "not_in_keywords = {}\n",
        "comm_not_in = []\n",
        "pickle_menus = extractMenu(tot_menu_arr, keywords, not_in_keywords, comm_not_in)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2EvAh24_dSH"
      },
      "source": [
        "# 뚝배기 - 없음 -> 제외\n",
        "keywords = ['뚝배기', '돌솥'] \n",
        "not_in_keywords = {}\n",
        "comm_not_in = []\n",
        "dduk_menus = extractMenu(tot_menu_arr, keywords, not_in_keywords, comm_not_in)\n",
        "\n",
        "dduk_menus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61i-6POx_dV8"
      },
      "source": [
        "# 샐러드\n",
        "keywords = ['샐러드'] \n",
        "not_in_keywords = {}\n",
        "comm_not_in = []\n",
        "salad_menus = extractMenu(tot_menu_arr, keywords, not_in_keywords, comm_not_in)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pa9wDbZ3_dX_"
      },
      "source": [
        "# 우유\n",
        "# 우유가 들어간 식재료(크림, 요거트 등)종류로 변경\n",
        "keywords = ['까르보나라', '크림', '요거트'] \n",
        "not_in_keywords = {}\n",
        "comm_not_in = ['샐러드', 'D', '드레싱', '소스'] # 샐러드 드레싱, 디핑소스 제외\n",
        "milk_menus = extractMenu(tot_menu_arr, keywords, not_in_keywords, comm_not_in)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjyIqm7P_daU"
      },
      "source": [
        "# 빵, 쿠키\n",
        "# https://ko.wikipedia.org/wiki/%EB%B9%B5_%EB%AA%A9%EB%A1%9D\n",
        "keywords = ['와플', '케이크', '케잌', '바게트', '도넛', '도너츠', '핫도그', '도라야키', '베이글', '번', '비스킷', '스콘', '토스트', '브레드', '포카차', '피자', '호두과자', '쿠키'] \n",
        "not_in_keywords = {}\n",
        "comm_not_in = []\n",
        "bread_menus = extractMenu(tot_menu_arr, keywords, not_in_keywords, comm_not_in)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmnB-rio_qz0"
      },
      "source": [
        "# 음료\n",
        "# https://ko.wikipedia.org/wiki/%EB%B9%B5_%EB%AA%A9%EB%A1%9D\n",
        "keywords = ['주스', '쥬스', '수정과', '식혜', '식초', '코코아', '칵테일', '스무디', '우유', '셰이크', '야쿠르트', '요구르트', '커피', '차', '탄산수', '음료'] \n",
        "not_in_keywords = {'차':['차돌']}\n",
        "comm_not_in = []\n",
        "drink_menus = extractMenu(tot_menu_arr, keywords, not_in_keywords, comm_not_in)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLFMSNCz_q2V"
      },
      "source": [
        "def get_food_one_hot(x, menu_array):\n",
        "  menu_list = x.strip().split()\n",
        "  for i, menu_nm in enumerate(menu_list):\n",
        "    menu_nm = re.sub(pattern, '', menu_nm)\n",
        "    if menu_nm.strip() in ['', '*']:\n",
        "      continue\n",
        "    if menu_nm[0] == '(' or menu_nm[-1] == ')':\n",
        "      continue\n",
        "    try:\n",
        "      if menu_array.index(menu_nm) > -1:\n",
        "        return 1\n",
        "    except Exception:\n",
        "      pass\n",
        "  return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whUqZvnZ_q4k"
      },
      "source": [
        "# 데이터 병합\n",
        "menu_col_nm = ['육류', '난류', '죽류', '덮밥_국밥류', '비빔밥_볶음밥류', '국탕류', '구이류', '전류', '튀김류', '곡물', '콩류',\n",
        "               '묵', '생선_조개류', '채소류', '해조류', '장류', '김치', '만두', '곡물가루', '과일', '쌀', '김밥_초밥', '절임류',\n",
        "               '면류', '스튜', '나물_무침류', '피클', '샐러드', '우유', '빵류', '음료', '돼지고기', '소고기', '닭고기', '오리고기']\n",
        "menu_data_arr = [meat_menus, egg_menus, juk_menus, gukbob_menus, bb_menus, soup_menus, gui_menus, jeon_menus, fry_menus, grain_menus, bean_menus,\n",
        "                 kor_jelly_menus, fish_shell_menus, vegetable_menus, sea_alg_menus, jang_menus, kimchi_menus, mandu_menus, powder_menus, fruit_menus, rice_menus, gimbab_menus, saused_menus,\n",
        "                 noodle_menus, stew_menus, namul_menus, pickle_menus, salad_menus, milk_menus, bread_menus, drink_menus, pig_menus, beef_menus, chicken_menus, duck_menus]\n",
        "\n",
        "for col_type in ['중식메뉴', '석식메뉴']:\n",
        "  for i, menu_arr in enumerate(menu_data_arr):\n",
        "    train[col_type + '_' + menu_col_nm[i]] = train[col_type].apply(lambda x: get_food_one_hot(x, menu_arr))\n",
        "    test[col_type + '_' + menu_col_nm[i]] = train[col_type].apply(lambda x: get_food_one_hot(x, menu_arr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgtDxinW_Nv5"
      },
      "source": [
        "df = pd.concat([train, test], axis=0).reset_index(drop=True)\n",
        "df = df.fillna(0)\n",
        "df.columns = ['일자', '요일', '정원', '휴가자', '출장자', '야근자', '재택근무자', '조식', '중식', '석식',\\\n",
        "              '중식계', '석식계', '중식메뉴_육류',\n",
        "       '중식메뉴_난류', '중식메뉴_죽류', '중식메뉴_덮밥_국밥류', '중식메뉴_비빔밥_볶음밥류', '중식메뉴_국탕류',\n",
        "       '중식메뉴_구이류', '중식메뉴_전류', '중식메뉴_튀김류', '중식메뉴_곡물', '중식메뉴_콩류', '중식메뉴_묵',\n",
        "       '중식메뉴_생선_조개류', '중식메뉴_채소류', '중식메뉴_해조류', '중식메뉴_장류', '중식메뉴_김치', '중식메뉴_만두',\n",
        "       '중식메뉴_곡물가루', '중식메뉴_과일', '중식메뉴_쌀', '중식메뉴_김밥_초밥', '중식메뉴_절임류', '중식메뉴_면류',\n",
        "       '중식메뉴_스튜', '중식메뉴_나물_무침류', '중식메뉴_피클', '중식메뉴_샐러드', '중식메뉴_우유', '중식메뉴_빵류',\n",
        "       '중식메뉴_음료', '중식메뉴_돼지고기', '중식메뉴_소고기', '중식메뉴_닭고기', '중식메뉴_오리고기', '석식메뉴_육류',\n",
        "       '석식메뉴_난류', '석식메뉴_죽류', '석식메뉴_덮밥_국밥류', '석식메뉴_비빔밥_볶음밥류', '석식메뉴_국탕류',\n",
        "       '석식메뉴_구이류', '석식메뉴_전류', '석식메뉴_튀김류', '석식메뉴_곡물', '석식메뉴_콩류', '석식메뉴_묵',\n",
        "       '석식메뉴_생선_조개류', '석식메뉴_채소류', '석식메뉴_해조류', '석식메뉴_장류', '석식메뉴_김치', '석식메뉴_만두',\n",
        "       '석식메뉴_곡물가루', '석식메뉴_과일', '석식메뉴_쌀', '석식메뉴_김밥_초밥', '석식메뉴_절임류', '석식메뉴_면류',\n",
        "       '석식메뉴_스튜', '석식메뉴_나물_무침류', '석식메뉴_피클', '석식메뉴_샐러드', '석식메뉴_우유', '석식메뉴_빵류',\n",
        "       '석식메뉴_음료', '석식메뉴_돼지고기', '석식메뉴_소고기', '석식메뉴_닭고기', '석식메뉴_오리고기']\n",
        "df.drop(columns=['조식', '중식', '석식'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAaGu6sRVOR1"
      },
      "source": [
        "### 외부 데이터 추가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XU9m1cWDgN4R"
      },
      "source": [
        "dust_dir = os.path.join(path, '미세먼지_일별')\n",
        "wdata_dir = os.path.join(path, '날씨_시간별')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HgT1y52gZdD"
      },
      "source": [
        "w_attrs = ['강수', '기온', '습도', '강수형태']\n",
        "w_years = os.listdir(wdata_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MH3kshIpgwwy"
      },
      "source": [
        "def get_wdata(data_path, dtype='num'):\n",
        "  datetime_list = []\n",
        "  value_list_12 = []\n",
        "  value_list_18 = []\n",
        "  curr_mon = ''\n",
        "\n",
        "  with open(data_path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    for i, line in enumerate(lines):\n",
        "      if line.strip() == '':\n",
        "        break\n",
        "      row_data = line.strip().split(',')\n",
        "      row_data = [elem.strip() for elem in row_data]\n",
        "      if i == 0:\n",
        "        curr_mon = row_data[-1].split()[-1][:-2]\n",
        "        continue\n",
        "      if len(row_data) == 1:\n",
        "        curr_mon = row_data[-1].split()[-1][:-2]\n",
        "        continue\n",
        "      r_day, r_hour, r_value = row_data\n",
        "      if r_hour in [\"1200\", \"1800\"]: # 점심 12시, 저녁 6시 기준으로 처리\n",
        "        if r_hour == \"1200\":\n",
        "          datetime_list.append(curr_mon[:4]+'-'+curr_mon[4:]+'-'+str('%02d'%int(r_day)))\n",
        "\n",
        "        if dtype == 'num':\n",
        "          if r_hour == \"1200\":\n",
        "            value_list_12.append(float(r_value))\n",
        "          else:\n",
        "            value_list_18.append(float(r_value))\n",
        "        else:\n",
        "          if r_hour == \"1200\":\n",
        "            value_list_12.append(str(round(float(r_value))))\n",
        "          else:\n",
        "            value_list_18.append(str(round(float(r_value))))\n",
        "          \n",
        "\n",
        "  return datetime_list, value_list_12, value_list_18"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUoVNeu1gxs2"
      },
      "source": [
        "# 강수, 기온, 습도, 강수형태 데이터\n",
        "w_data_rain_12 = []\n",
        "w_data_temp_12 = []\n",
        "w_data_hum_12 = []\n",
        "w_data_rtype_12 = []\n",
        "w_data_rain_18 = []\n",
        "w_data_temp_18 = []\n",
        "w_data_hum_18 = []\n",
        "w_data_rtype_18 = []\n",
        "w_datetime = []\n",
        "\n",
        "for year in w_years:\n",
        "  w_subdir = os.path.join(wdata_dir, year)\n",
        "  file_names = os.listdir(w_subdir)\n",
        "  file_name = \"\"\n",
        "  if year != '2021':\n",
        "    file_name = f'{year}01_{year}12.csv'\n",
        "  else:\n",
        "    file_name = f'{year}01_{year}04.csv'\n",
        "  file_path_rain = os.path.join(w_subdir, '충무공동_강수_'+file_name)\n",
        "  file_path_temp = os.path.join(w_subdir, '충무공동_기온_'+file_name)\n",
        "  file_path_hum = os.path.join(w_subdir, '충무공동_습도_'+file_name)\n",
        "  file_path_rtype = os.path.join(w_subdir, '충무공동_강수형태_'+file_name)\n",
        "\n",
        "  datetime_list_rain, value_list_rain_12, value_list_rain_18 = get_wdata(file_path_rain, dtype='num') # 강수 데이터\n",
        "  datetime_list_temp, value_list_temp_12, value_list_temp_18 = get_wdata(file_path_temp, dtype='num') # 기온 데이터\n",
        "  datetime_list_hum, value_list_hum_12, value_list_hum_18 = get_wdata(file_path_hum, dtype='num') # 습도 데이터\n",
        "  datetime_list_rtype, value_list_rtype_12, value_list_rtype_18 = get_wdata(file_path_rtype, dtype='cat') # 강수형태 데이터\n",
        "  \n",
        "  w_datetime   += datetime_list_rain\n",
        "  w_data_rain_12  += value_list_rain_12\n",
        "  w_data_temp_12  += value_list_temp_12\n",
        "  w_data_hum_12   += value_list_hum_12\n",
        "  w_data_rtype_12 += value_list_rtype_12\n",
        "  w_data_rain_18  += value_list_rain_18\n",
        "  w_data_temp_18  += value_list_temp_18\n",
        "  w_data_hum_18   += value_list_hum_18\n",
        "  w_data_rtype_18 += value_list_rtype_18"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsORoPLdgygT"
      },
      "source": [
        "w_df = pd.DataFrame({'일자':pd.Series(w_datetime, dtype='datetime64[ns]'),\n",
        "                   'rain_lunch':pd.Series(w_data_rain_12, dtype='float'),\n",
        "                   'temp_lunch':pd.Series(w_data_temp_12, dtype='float'),\n",
        "                   'hum_lunch':pd.Series(w_data_hum_12, dtype='float'),\n",
        "                   'rain_type_lunch':pd.Series(w_data_rtype_12, dtype='str'),\n",
        "                   'rain_dinner':pd.Series(w_data_rain_18, dtype='float'),\n",
        "                   'temp_dinner':pd.Series(w_data_temp_18, dtype='float'),\n",
        "                   'hum_dinner':pd.Series(w_data_hum_18, dtype='float'),\n",
        "                   'rain_type_dinner':pd.Series(w_data_rtype_18, dtype='str')})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBaSMJuAgzRj"
      },
      "source": [
        "# 불쾌지수 컬럼 추가\n",
        "# https://dacon.io/competitions/official/235736/codeshare/2753?page=1&dtype=recent\n",
        "w_df['discomfort_index_lunch'] = 1.8*w_df['temp_lunch'] - 0.55*(1-w_df['hum_lunch']/100)*(1.8*w_df['temp_lunch']-26) + 32\n",
        "w_df['discomfort_index_dinner'] = 1.8*w_df['temp_dinner'] - 0.55*(1-w_df['hum_dinner']/100)*(1.8*w_df['temp_dinner']-26) + 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiVnVyYfgz80"
      },
      "source": [
        "dust_file_paths = glob.glob(os.path.join(dust_dir, '*.xls'))\n",
        "d_datetime = []\n",
        "d_value1 = []\n",
        "d_value2 = []\n",
        "\n",
        "# 시간별 데이터의 경우 미세먼지 측정값 중 빈 값이 있는 경우가 어느 정도 있어서 배제했습니다.\n",
        "for file_path in dust_file_paths:\n",
        "  date_yyyymm = os.path.splitext(os.path.basename(file_path))[0] # yyyymm\n",
        "  date_year = date_yyyymm[:4]\n",
        "  date_mon = date_yyyymm[4:]\n",
        "  dust_df = None\n",
        "\n",
        "  if date_year == '2021':\n",
        "    dust_df = pd.read_excel(file_path, header=[0, 1], skiprows=3)\n",
        "  else:\n",
        "    dust_df = pd.read_excel(file_path, header=[0, 1])\n",
        "  cols = dust_df.columns\n",
        "  date_col = cols[0]\n",
        "  fine_dust_col = cols[1] # 미세먼지\n",
        "  ufine_dust_col = cols[2] # 초미세먼지\n",
        "\n",
        "  # 해당월의 일수 가져오기\n",
        "  days = calendar.monthrange(int(date_year),int(date_mon))[1] \n",
        "  for day in range(1, days+1):\n",
        "    day_1 = '%02d'%day\n",
        "    curr_day_df =  date_year+ '-' + date_mon + '-' + day_1\n",
        "\n",
        "    row_lunch = dust_df[dust_df[cols[0]] == curr_day_df]\n",
        "    row_dinner = dust_df[dust_df[cols[0]] == curr_day_df]\n",
        "    curr_date = date_year+'-'+date_mon+'-'+day_1\n",
        "  \n",
        "    d_datetime.append(curr_date)\n",
        "    d_value1.append(row_lunch[fine_dust_col].values[0])\n",
        "    d_value2.append(row_lunch[ufine_dust_col].values[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XE3Cg4eUg0kL"
      },
      "source": [
        "dust_df = pd.DataFrame({'일자':pd.Series(d_datetime, dtype='datetime64[ns]'),\n",
        "                   'fine_dust':pd.Series(d_value1, dtype='float'),\n",
        "                   'ultra_fine_dust':pd.Series(d_value2, dtype='float')})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vW5j6_Jg30z"
      },
      "source": [
        "df = pd.merge(df, dust_df, on='일자')\n",
        "df = pd.merge(df, w_df, on='일자')\n",
        "\n",
        "# 결측치 근처 관측치로 대체\n",
        "df['fine_dust'][564, 1129] = [36, 23]\n",
        "df['ultra_fine_dust'][234, 235, 564, 654, 1129] = [11, 31, 26, 5, 9]\n",
        "\n",
        "# 저녁 컬럼 삭제\n",
        "# df = df.drop(columns=['rain_dinner', 'temp_dinner', 'hum_dinner', 'rain_type_dinner', 'discomfort_index_dinner'])\n",
        "\n",
        "# 미세먼지 명목변수화\n",
        "df['fine_degree'] = df['fine_dust'].apply(lambda x : 0 if 0<=x<=30 else (1 if 31<=x<=80 else (2 if 81<=x<=150 else 3)))\n",
        "df['ultra_fine_degree'] = df['ultra_fine_dust'].apply(lambda x : 0 if 0<=x<=15 else (1 if 16<=x<=35 else (2 if 36<=x<=75 else 3)))\n",
        "df['fine_degree'] = df.apply(lambda x : max(x['fine_degree'], x['ultra_fine_degree']), axis=1)\n",
        "\n",
        "# 강수량 명목변수화\n",
        "df['rain_degree_lunch'] = df['rain_lunch'].apply(lambda x : 0 if x < 2 else 1)\n",
        "df['rain_degree_dinner'] = df['rain_dinner'].apply(lambda x : 0 if x < 2 else 1)\n",
        "\n",
        "# 불쾌지수 명목변수화 \n",
        "df['discomfort_degree_lunch'] = df['discomfort_index_lunch'].apply(lambda x : 0 if x<68 else (1 if 68<=x<75 else (2 if 75<=x<80 else 3)))\n",
        "df['discomfort_degree_dinner'] = df['discomfort_index_dinner'].apply(lambda x : 0 if x<68 else (1 if 68<=x<75 else (2 if 75<=x<80 else 3)))\n",
        "\n",
        "# 명목변수화하면서 삭제\n",
        "df = df.drop(columns=['rain_lunch', 'temp_lunch', 'hum_lunch', 'rain_type_lunch', 'discomfort_index_lunch',\n",
        "                      'rain_dinner', 'temp_dinner', 'hum_dinner', 'rain_type_dinner', 'discomfort_index_dinner',\n",
        "                      'fine_dust', 'ultra_fine_dust', 'ultra_fine_degree',])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3yZ6AQZV1sF"
      },
      "source": [
        "## 중간 변수 색출"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pI10OVxAg7Zs"
      },
      "source": [
        "### 파생변수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EP2g8FEBAuk3"
      },
      "source": [
        "path = '/content/drive/MyDrive/구내식당/water/'\n",
        "df = pd.read_csv(path+'df.csv')\n",
        "train = pd.read_csv(path+'train.csv')\n",
        "# holiday = pd.read_csv(path+'holidays.csv')\n",
        "holiday = pd.read_csv(path+'holidays_old.csv')\n",
        "corona = pd.read_csv(path+'corona_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4OzChE4TBbs"
      },
      "source": [
        "df = df.drop(columns=['중식메뉴_죽류', '중식메뉴_곡물', '중식메뉴_김치', '중식메뉴_쌀', '중식메뉴_나물_무침류', '중식메뉴_피클',\\\n",
        "                      'fine_degree', 'rain_degree_lunch', 'discomfort_degree_lunch', '석식메뉴_죽류', '석식메뉴_곡물',\\\n",
        "                      '석식메뉴_김치', '석식메뉴_쌀', '석식메뉴_나물_무침류', '석식메뉴_피클', 'rain_degree_dinner', 'discomfort_degree_dinner',\\\n",
        "                      '중식메뉴_음료', '석식메뉴_음료'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jxsw5--drBg0"
      },
      "source": [
        "* 복날, 연말, 명절 전 영입일 - 긴 연후 전후\n",
        "* 국정감사 \n",
        "    * 컬럼 체크 필요"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNyg8enap-kO"
      },
      "source": [
        "def feature_pipeline(df) :\n",
        "    df['일자'] = df['일자'].astype('datetime64')\n",
        "    df['식사가능인원'] = df['정원']-(df['휴가자']+df['출장자']+df['재택근무자'])+df['야근자']\n",
        "    df['년'] = df['일자'].dt.year\n",
        "    df['월'] = df['일자'].dt.month\n",
        "    df['일'] = df['일자'].dt.day\n",
        "    df['년월'] = df['년'].astype('str')+'_'+df['월'].astype('str')\n",
        "\n",
        "    first_dayofmonth = []\n",
        "    last_dayofmonth = []\n",
        "    for i in df['년월'].unique() :\n",
        "        first_dayofmonth.append(df[df['년월']==i].iloc[0].name)\n",
        "        last_dayofmonth.append(df[df['년월']==i].iloc[-1].name)\n",
        "    df['첫_출근일'] = df.apply(lambda x : 1 if x.name in first_dayofmonth else 0, axis=1)\n",
        "    df['마지막_출근일'] = df.apply(lambda x : 1 if x.name in last_dayofmonth else 0, axis=1)\n",
        "\n",
        "    # 월_주차 컬럼 추가\n",
        "    df['주차'] = df['일자'].apply(lambda x: pendulum.parse(str(x)).week_of_month)\n",
        "    repair_2017 = df[(df['년']==2017)&(df['주차']<0)]['일자'].dt.week\n",
        "    repair_2021 = df[(df['년']==2021)&(df['주차']<0)]['일자'].dt.week\n",
        "    df['주차'][list(repair_2017.index)] = repair_2017.values\n",
        "    df['주차'][list(repair_2021.index)] = repair_2021.values\n",
        "    df['주차'][[709, 954, 955]] = np.array([6, 5, 5])\n",
        "\n",
        "    month_to_season = {1: 3,2: 3,3:0,4:0,5:0,6:1,7:1,8:1,9:2,10:2,11:2,12: 3}\n",
        "    df['계절'] = df['월'].apply(lambda x : month_to_season[x])\n",
        "    df['요일'] = df['일자'].dt.weekday\n",
        "    df['야근_가능'] = df['요일'].apply(lambda x : 1 if (x==2) or (x==4) else 0)\n",
        "    df['is_corona'] = df['일자'].apply(lambda x : 0 if x < pd.to_datetime('2020-01-06') else 1)\n",
        "    df['연기준몇주째']= df['일자'].dt.weekofyear\n",
        "    df['월마지막일여부'] =df['일자'].dt.is_month_end\n",
        "    df['월일수']= df['일자'].dt.days_in_month\n",
        "\n",
        "    # 공휴일 데이터 추가\n",
        "    holiday['date'] = pd.to_datetime(holiday['date'])\n",
        "    df['before_holiday'] = df['일자'].apply(lambda x : 1 if (x+dt.timedelta(1) in holiday['date'].tolist()) else 0)\n",
        "    df['after_holiday'] = df['일자'].apply(lambda x : 1 if (x-dt.timedelta(1) in holiday['date'].tolist()) else 0)\n",
        "    \n",
        "    # 탄력근무제 적용 여부\n",
        "    test_date=\"20180701\"\n",
        "    convert_date = datetime.datetime.strptime(test_date, \"%Y%m%d\").date()\n",
        "    df['탄력근무_여부'] = df['일자'].apply(lambda x : 1 if x >= convert_date else 0)\n",
        "\n",
        "    # # 이벤트 데이터 고민 - 복날 / 연말\n",
        "    bok = pd.to_datetime(['2016-08-16', '2016-07-27', '2016-07-18', '2017-08-11', '2017-07-21','2017-07-12', '2018-08-16', '2018-07-27', '2018-07-17', '2019-08-12', '2019-07-22', '2019-07-12', '2020-07-27', '2020-07-16'])\n",
        "    df['복날'] = df['일자'].apply(lambda x : 1 if x in bok else 0)\n",
        "\n",
        "    # end_year = df[(df['월']==12)&(df['일']>=21)].index\n",
        "    # df['연말'] = df.apply(lambda x : 1 if  x.name in end_year else 0, axis=1)\n",
        "\n",
        "    # 명절 전 영업일 여부\n",
        "    event = pd.to_datetime(['2016-02-05', '2019-09-13', '2017-01-26','2017-09-29', '2018-02-14', '2018-09-21', '2019-02-01', '2019-09-11', '2020-01-23', '2020-09-30', '2021-02-10'])\n",
        "    df['명절_이전_영업일'] = df['일자'].apply(lambda x : 1 if x in event else 0)\n",
        "\n",
        "    # 명절 전이나 둘 중에 하나만 써야함 / 특히 10월달이 이상함\n",
        "    # df['긴_연휴'] = df['일자'].diff().dt.days.fillna(0).astype('int')\n",
        "    # df['연휴_뒤'] = df['긴_연휴'].apply(lambda x: 1 if x>=4 else 0)\n",
        "    # df['연휴_앞'] = df.apply(lambda x : 1 if x.name in df[df['연휴_뒤']==1].index-1 else 0, axis=1)\n",
        "    # df.drop(columns=['긴_연휴'], inplace=True)\n",
        "\n",
        "    # 국정 감사 기간\n",
        "    gukgam = pd.to_datetime(['2016-10-04', '2016-10-05', '2016-10-06', '2017-10-10', '2017-10-11', '2017-10-12', '2017-10-13', '2018-10-08', '2018-10-10', '2018-10-11',\\\n",
        "                             '2019-10-02', '2019-10-04'])\n",
        "    df['국정감사'] = df['일자'].apply(lambda x : 1 if x in gukgam else 0)\n",
        "\n",
        "    # 인원 변화\n",
        "    # df['인원변화'] = df['정원'].diff()\n",
        "    # df['인원변화'][0] = 0\n",
        "\n",
        "    df.drop(columns=['정원', '년월', '년'], inplace=True)\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFVPTKq2mpmt"
      },
      "source": [
        "#### 고민하는 변수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sbY-BF5UkhR"
      },
      "source": [
        "# check_corona = corona[['일자', '누적검사자']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zM-X4KHMgX2r"
      },
      "source": [
        "# 고민해본 파생변수\n",
        "# 식사인원\n",
        "# df['휴가출장'] = df['정원']-(df['휴가자']+df['출장자'])\n",
        "# df['휴가출장재택'] = df['정원']-(df['휴가자']+df['출장자']+df['재택근무자'])\n",
        "# df['휴가출장재택야근'] = df['정원']-(df['휴가자']+df['출장자']+df['재택근무자'])+df['야근자']\n",
        "# df['휴가'] = df['정원']-(df['휴가자'])\n",
        "# df['휴가야근'] = df['정원']-(df['휴가자'])+df['야근자']\n",
        "# df['휴가재택야근'] = df['정원']-(df['휴가자']+df['재택근무자'])+df['야근자']\n",
        "# df['휴가재택'] = df['정원']-(df['휴가자']+df['재택근무자'])\n",
        "# df['휴가비율'] = df['휴가자']/df['정원']\n",
        "# df['출장비율'] = df['출장자']/df['정원']\n",
        "# df['야근비율'] = df['야근자']/df['출근인원']\n",
        "# df['재택비율'] = df['재택근무자']/df['정원']\n",
        "# df['휴가_출장'] = df['휴가자']+df['출장자']\n",
        "\n",
        "# etc\n",
        "# df['is_monday'] = df['요일'].apply(lambda x : 1 if (x==0) else 0) \n",
        "# df['정책_변화'] = df['일자'].apply(lambda x : 0 if x < pd.to_datetime('2019-01-04') else 1)\n",
        "# df['월마지막일여부'] =df['일자'].dt.is_month_end\n",
        "# df['연기준몇일째']= df['일자'].dt.dayofyear\n",
        "\n",
        "# maybe not\n",
        "# df['출장자제외'] = df['정원'] - df['출장자']\n",
        "# df['재택근무제외'] = df['정원'] - df['재택근무자']\n",
        "# df['윤년여부'] = df['일자'].dt.is_leap_year\n",
        "# df['월시작일여부'] = df['일자'].dt.is_month_start\n",
        "# df['월마지막일여부'] =df['일자'].dt.is_month_end\n",
        "# df['분기시작일여부'] =df['일자'].dt.is_quarter_start\n",
        "# df['분기마지막일여부'] =df['일자'].dt.is_quarter_end\n",
        "# df['연시작일여부'] =df['일자'].dt.is_year_start\n",
        "# df['연마지막일여부'] =df['일자'].dt.is_year_end"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7whoF48rOKX"
      },
      "source": [
        "### 데이터 정규화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhrGZeRNrTaN"
      },
      "source": [
        "df = feature_pipeline(df)\n",
        "\n",
        "# 코로나 데이터 추가\n",
        "corona = corona.drop_duplicates(['일자'])\n",
        "corona['일자'] = corona['일자'].astype('datetime64')\n",
        "df = pd.merge(df,corona[['일자', '일일검사자']], on='일자', how='left')\n",
        "df = df.fillna(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2z2CUjMo3WlY"
      },
      "source": [
        "df['중식비율'] = df['중식계']/df['식사가능인원']\n",
        "df['석식비율'] = df['석식계']/df['식사가능인원']\n",
        "# df.drop(columns=[i for i in df.columns if ('메뉴' in i) or ('degree' in i)], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTc_3wc4AOjE"
      },
      "source": [
        "## 이상치 제거\n",
        "* 수요일 야근자가 100명 이상, 정책 변경 후 금요일 100명 이상 제거\n",
        "* 식사비율이 이상치에 해당하는 값 제거\n",
        "    * 1.5*(제 3분위 수 - 제 1분위 수)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D72MMCwrOC5I"
      },
      "source": [
        "df = df[~df.index.isin(df[(df['요일']==2)&(df['야근자']>=100)].index.append(df[(df['요일']==4)&(df['일자']>='2019-01-01')&(df['야근자']>100)].index))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJanA6ghM47y"
      },
      "source": [
        "train = df.iloc[:-50]\n",
        "train['월마지막일여부'] = train['월마지막일여부'].astype(int)\n",
        "test = df.iloc[-50:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bOFtebI8bfn"
      },
      "source": [
        "train_2 = train[train['석식계']!=0]\n",
        "iqr_lunch = 1.5*(train['중식비율'].quantile(0.75) - train['중식비율'].quantile(0.25))\n",
        "iqr_dinner = 1.5*(train_2['석식비율'].quantile(0.75) - train_2['석식비율'].quantile(0.25))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtefFzD096t3"
      },
      "source": [
        "plt.plot(train_2['일자'], train_2['석식비율'], marker='o')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FFfmCLv8uZG"
      },
      "source": [
        "train_1 = train[(train['중식비율']>=train['중식비율'].quantile(0.25)-iqr_lunch)|(train['중식비율']<=train['중식비율'].quantile(0.75)+iqr_lunch)]\n",
        "train_2 = train_2[train_2['일자']!='2016-10-05']\n",
        "# train_2 = train_2[(train_2['석식비율']>=train_2['석식비율'].quantile(0.25)-iqr_dinner)|(train_2['석식비율']<=train_2['석식비율'].quantile(0.75)+iqr_dinner)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PE6cH0Pm5a-t"
      },
      "source": [
        "def get_one_hot(x, target_val):\n",
        "  if x == target_val:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EDdvFp75bwl"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "# onehot_col = ['년', '월', '요일', '계절']\n",
        "onehot_col = ['월', '요일', '계절']\n",
        "# df_tmp = df.copy()\n",
        "# df = pd.concat([df[list((set(df.columns)-set(onehot_col)))],\\\n",
        "#                 pd.get_dummies(df[onehot_col])], axis=1)\n",
        "\n",
        "# sub_types = [[2016, 2017, 2018, 2019, 2020, 2021], [1,2,3,4,5,6,7,8,9,10,11,12], [0,1,2,3,4], [0,1,2,3]]\n",
        "sub_types = [[1,2,3,4,5,6,7,8,9,10,11,12], [0,1,2,3,4], [0,1,2,3]]\n",
        "\n",
        "train_lun = train_1.copy()\n",
        "train_din = train_2.copy()\n",
        "# for i, col_type in enumerate(onehot_col):\n",
        "#   for j, class_nm in enumerate(sub_types[i]):\n",
        "#     df_tmp[col_type + '_' + str(class_nm)] = df_tmp[col_type].apply(lambda x: get_one_hot(x, class_nm))\n",
        "\n",
        "for i, col_type in enumerate(onehot_col):\n",
        "  for j, class_nm in enumerate(sub_types[i]):\n",
        "    train_lun[col_type + '_' + str(class_nm)] = train_lun[col_type].apply(lambda x: get_one_hot(x, class_nm))\n",
        "for i, col_type in enumerate(onehot_col):\n",
        "  for j, class_nm in enumerate(sub_types[i]):\n",
        "    train_din[col_type + '_' + str(class_nm)] = train_din[col_type].apply(lambda x: get_one_hot(x, class_nm))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8s6vRKynS4TX"
      },
      "source": [
        "train_1 = train_lun[[i for i in train_lun.columns if ('석식' not in i) and ('dinner' not in i) and ('탄력근무' not in i) and ('명절' not in i)]]\n",
        "train_2 = train_din[[i for i in train_din.columns if ('중식' not in i) and ('lunch' not in i)]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56bzQdB_AIwm"
      },
      "source": [
        "# train = train.drop(columns=['일자', '년', '월', '요일',  '계절'])\n",
        "# train_1 =train[[i for i in train.columns if ('석식' not in i) and ('dinner' not in i) and ('탄력근무' not in i) and ('명절' not in i)]]\n",
        "# train_2 =train[[i for i in train.columns if ('중식' not in i) and ('lunch' not in i)]]\n",
        "\n",
        "# 이상치 제거\n",
        "# train_2 = train_2.loc[train_2['석식계'] != 0.0]\n",
        "# train_2 = train_2.loc[(train_2['년_2016'] != 1) | (train_2['월_10'] != 1) | (train_2['일'] != 5)]\n",
        "# # train_2 = train_2.loc[(train_2['년_2019'] != 1) | (train_2['월_9'] != 1) | (train_2['일'] != 11)]\n",
        "# # train_2 = train_2.loc[(train_2['년_2019'] != 1) | (train_2['월_12'] != 1) | (train_2['일'] != 23)]\n",
        "# # train_2 = train_2.loc[(train_2['년_2019'] != 1) | (train_2['월_12'] != 1) | (train_2['일'] != 30)]\n",
        "# # train_2 = train_2.loc[(train_2['년_2020'] != 1) | (train_2['월_1'] != 1) | (train_2['일'] != 23)]\n",
        "\n",
        "# train_1 = train_1.loc[(train_1['년_2016'] != 1) | (train_1['월_10'] != 1) | (train_1['일'] != 5)]\n",
        "# train_1 = train_1.loc[(train_1['년_2017'] != 1) | (train_1['월_12'] != 1) | (train_1['일'] != 28)]\n",
        "# train_1 = train_1.loc[(train_1['년_2020'] != 1) | (train_1['월_12'] != 1) | (train_1['일'] != 2)]\n",
        "# train_1 = train_1.loc[(train_1['년_2018'] != 1) | (train_1['월_9'] != 1) | (train_1['일'] != 14)]\n",
        "# train_1 = train_1.loc[(train_1['년_2018'] != 1) | (train_1['월_12'] != 1) | (train_1['일'] != 24)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73IbKbNpS4fN"
      },
      "source": [
        "for i, col_type in enumerate(onehot_col):\n",
        "  for j, class_nm in enumerate(sub_types[i]):\n",
        "    test[col_type + '_' + str(class_nm)] = test[col_type].apply(lambda x: get_one_hot(x, class_nm))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAzHnB0nA_uH"
      },
      "source": [
        "train_1 = train_1.drop(columns=['일자', '요일', '월', '첫_출근일', '마지막_출근일', '계절', '월일수', '복날', '일일검사자', '중식비율'])\n",
        "train_2 = train_2.drop(columns=['일자', '요일', '월', '첫_출근일', '마지막_출근일', '계절', '월일수', '복날', '일일검사자', '석식비율'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGR2r9EhV0RZ"
      },
      "source": [
        "train_1.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Fsy_siBV2b5"
      },
      "source": [
        "train_2.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYnxhThaC3QF"
      },
      "source": [
        "## AutoML"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdluKTtVtbcy"
      },
      "source": [
        "!pip install optuna"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UzXhQB2tkPV"
      },
      "source": [
        "import optuna\n",
        "from optuna import Trial\n",
        "from optuna.samplers import TPESampler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from catboost import CatBoostRegressor\n",
        "from lightgbm import LGBMRegressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9LUcbkNtWaP"
      },
      "source": [
        "## Catboost 모델링"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUKXq4Q1fA3p"
      },
      "source": [
        "### 점심"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQZKNlDruept"
      },
      "source": [
        "# def objective(trial: Trial) -> float:\n",
        "# #     params_catboost = {\n",
        "# #     'iterations': trial.suggest_int(\"iterations\", 1, 100000),\n",
        "# #     'learning_rate': trial.suggest_float(\"learning_rate\", 0.001, 0.01),\n",
        "# #     'eval_metric': 'MAE',\n",
        "# #     'random_seed': 42,\n",
        "# #     'logging_level': 'Silent',\n",
        "# #     'use_best_model': True,\n",
        "# #     'loss_function': 'MAE',\n",
        "# #     'od_type': 'Iter',\n",
        "# #     'od_wait': trial.suggest_int(\"od_wait\", 1, 10000),\n",
        "# #     'one_hot_max_size': trial.suggest_int(\"one_hot_max_size\", 1, 10000),\n",
        "# #     'l2_leaf_reg': trial.suggest_int(\"l2_leaf_reg\", 1, 15),\n",
        "# #     'depth': trial.suggest_int(\"l2_leaf_reg\", 1, 15),\n",
        "# #     'rsm': trial.suggest_float(\"rsm\", 0.1, 0.9),\n",
        "# #     'random_strength': trial.suggest_int(\"random_strength\", 1, 10000),\n",
        "# #     'bagging_temperature': trial.suggest_int(\"bagging_temperature\", 1, 10000),\n",
        "# # }\n",
        "#     params_catboost = {\n",
        "#         # \"objective\": 'Regression',\n",
        "#         'eval_metric' : 'MAE',\n",
        "#         'loss_function' : 'MAE',\n",
        "#         \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n",
        "#         \"depth\": trial.suggest_int(\"depth\", 1, 12),\n",
        "#         \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n",
        "#         \"bootstrap_type\": trial.suggest_categorical(\n",
        "#             \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n",
        "#         ),\n",
        "\n",
        "#         'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-1, log = True),\n",
        "#         'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-5, 1e-1, log = True),\n",
        "#         'min_child_samples': trial.suggest_int('min_child_samples', 2, 20),\n",
        "#         'random_strength': trial.suggest_float('random_strength', 0.05, 1, log = True)\n",
        "#     }\n",
        "\n",
        "#     if params_catboost[\"bootstrap_type\"] == \"Bayesian\":\n",
        "#         params_catboost[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n",
        "#     elif params_catboost[\"bootstrap_type\"] == \"Bernoulli\":\n",
        "#         params_catboost[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n",
        "    \n",
        "#     X_train, X_valid, y_train, y_valid = train_test_split(train_1[lunch_col].drop(columns=['중식계']), train_1['중식계'], test_size=0.2)\n",
        "\n",
        "#     model = CatBoostRegressor(**params_catboost)\n",
        "#     model.fit(\n",
        "#         X_train,\n",
        "#         y_train,\n",
        "#         cat_features=[],\n",
        "#         eval_set=(X_valid, y_valid),\n",
        "#         early_stopping_rounds=100,\n",
        "#         verbose=True,\n",
        "#     )\n",
        "\n",
        "#     from sklearn.metrics import mean_absolute_error\n",
        "#     catboost_pred = model.predict(X_valid)\n",
        "#     mean_absolute_error = (mean_absolute_error(y_valid, catboost_pred))\n",
        "    \n",
        "#     return mean_absolute_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItKdzwzuvDGO"
      },
      "source": [
        "# sampler = TPESampler(seed=42)\n",
        "# study = optuna.create_study(\n",
        "#     study_name=\"catboost_parameter_opt\",\n",
        "#     direction=\"minimize\",\n",
        "#     sampler=sampler,\n",
        "# )\n",
        "# study.optimize(objective, n_trials=50)\n",
        "# print(\"Best Score:\", study.best_value)\n",
        "# print(\"Best trial:\", study.best_trial.params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5CNlnT0jDfH"
      },
      "source": [
        "# catboost params\n",
        "params = {'colsample_bylevel': 0.09657164370923056, 'depth': 8, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bernoulli',\n",
        "          'learning_rate': 0.06770318888990769, 'l2_leaf_reg': 0.00021708988251157392, 'min_child_samples': 15, 'random_strength': 0.1618545184779159, 'subsample': 0.8477676851569896}\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(train_1[lunch_col].drop(columns=['중식계']), train_1['중식계'], test_size=0.2)\n",
        "model = CatBoostRegressor(**params)\n",
        "model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        cat_features=[],\n",
        "        eval_set=(X_valid, y_valid),\n",
        "        early_stopping_rounds=100,\n",
        "        verbose=True,\n",
        "    )\n",
        "\n",
        "catboost_pred_lunch = model.predict(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "annUBspDjk1N"
      },
      "source": [
        "# optuna.visualization.plot_optimization_history(study)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTjw_Uw4mzsJ"
      },
      "source": [
        "# optuna.visualization.plot_parallel_coordinate(study)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbBQHfcOmz5m"
      },
      "source": [
        "# optuna.visualization.plot_param_importances(study)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apcZ571Zm0EM"
      },
      "source": [
        "# from catboost import Pool\n",
        "# feature_score = pd.DataFrame(list(zip(train_1[lunch_col].drop(columns=['중식계']).dtypes.index, model.get_feature_importance(Pool(train_1[lunch_col].drop(columns=['중식계']), label=train_1['중식계'])))),\n",
        "#                 columns=['Feature','Score'])\n",
        "\n",
        "# feature_score = feature_score.sort_values(by='Score', ascending=False, inplace=False, kind='quicksort', na_position='last')\n",
        "# plt.rcParams[\"figure.figsize\"] = (12,7)\n",
        "# ax = feature_score.plot('Feature', 'Score', kind='bar', color='c')\n",
        "# ax.set_title(\"Catboost Feature Importance Ranking\", fontsize = 14)\n",
        "# ax.set_xlabel('')\n",
        "\n",
        "# rects = ax.patches\n",
        "\n",
        "# labels = feature_score['Score'].round(2)\n",
        "\n",
        "# for rect, label in zip(rects, labels):\n",
        "#     height = rect.get_height()\n",
        "#     ax.text(rect.get_x() + rect.get_width()/2, height + 0.35, label, ha='center', va='bottom')\n",
        "\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThZ8S_DFfDbk"
      },
      "source": [
        "### 저녁"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQjRzzWSfE8s"
      },
      "source": [
        "dinner_col = ['휴가자', '출장자', '야근자', '재택근무자', '석식계', '석식메뉴_육류', '석식메뉴_난류', '석식메뉴_덮밥_국밥류', '석식메뉴_비빔밥_볶음밥류', '석식메뉴_국탕류', '석식메뉴_구이류', '석식메뉴_전류', '석식메뉴_튀김류', '석식메뉴_콩류', '석식메뉴_묵', '석식메뉴_생선_조개류', '석식메뉴_채소류', '석식메뉴_해조류', '석식메뉴_장류', '석식메뉴_만두', '석식메뉴_곡물가루', '석식메뉴_과일', '석식메뉴_김밥_초밥', '석식메뉴_절임류', '석식메뉴_면류', '석식메뉴_스튜', '석식메뉴_샐러드', '석식메뉴_우유', '석식메뉴_빵류', '석식메뉴_돼지고기', '석식메뉴_소고기', '석식메뉴_닭고기', '석식메뉴_오리고기', '식사가능인원', '일', '야근_가능', '연기준몇주째', '첫_출근일', '마지막_출근일', '주차', 'is_corona', 'fine_degree', 'rain_degree_dinner', 'discomfort_degree_dinner', 'before_holiday', 'after_holiday', '월일수','명절_이전_영업일', '국정감사','탄력근무_여부', '년_2016', '년_2017', '년_2018', '년_2019', '년_2020', '년_2021', '월_1', '월_2', '월_3', '월_4', '월_5', '월_6', '월_7', '월_8', '월_9', '월_10', '월_11', '월_12', '요일_0', '요일_1', '요일_2', '요일_3', '요일_4', '계절_0', '계절_1', '계절_2', '계절_3']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MegfZyz7cH7q"
      },
      "source": [
        "# def objective(trial: Trial) -> float:\n",
        "# #     params_catboost = {\n",
        "# #     'iterations': trial.suggest_int(\"iterations\", 1, 100000),\n",
        "# #     'learning_rate': trial.suggest_float(\"learning_rate\", 0.001, 0.01),\n",
        "# #     'eval_metric': 'MAE',\n",
        "# #     'random_seed': 42,\n",
        "# #     'logging_level': 'Silent',\n",
        "# #     'use_best_model': True,\n",
        "# #     'loss_function': 'MAE',\n",
        "# #     'od_type': 'Iter',\n",
        "# #     'od_wait': trial.suggest_int(\"od_wait\", 1, 10000),\n",
        "# #     'one_hot_max_size': trial.suggest_int(\"one_hot_max_size\", 1, 10000),\n",
        "# #     'l2_leaf_reg': trial.suggest_int(\"l2_leaf_reg\", 1, 15),\n",
        "# #     'depth': trial.suggest_int(\"l2_leaf_reg\", 1, 15),\n",
        "# #     'rsm': trial.suggest_float(\"rsm\", 0.1, 0.9),\n",
        "# #     'random_strength': trial.suggest_int(\"random_strength\", 1, 10000),\n",
        "# #     'bagging_temperature': trial.suggest_int(\"bagging_temperature\", 1, 10000),\n",
        "# # }\n",
        "#     params_catboost = {\n",
        "#         # \"objective\": 'Regression',\n",
        "#         'eval_metric' : 'MAE',\n",
        "#         'loss_function' : 'MAE',\n",
        "#         \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n",
        "#         \"depth\": trial.suggest_int(\"depth\", 1, 12),\n",
        "#         \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n",
        "#         \"bootstrap_type\": trial.suggest_categorical(\n",
        "#             \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n",
        "#         ),\n",
        "\n",
        "#         'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-1, log = True),\n",
        "#         'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-5, 1e-1, log = True),\n",
        "#         'min_child_samples': trial.suggest_int('min_child_samples', 2, 20),\n",
        "#         'random_strength': trial.suggest_float('random_strength', 0.05, 1, log = True)\n",
        "#     }\n",
        "\n",
        "#     if params_catboost[\"bootstrap_type\"] == \"Bayesian\":\n",
        "#         params_catboost[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n",
        "#     elif params_catboost[\"bootstrap_type\"] == \"Bernoulli\":\n",
        "#         params_catboost[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n",
        "    \n",
        "#     X_train, X_valid, y_train, y_valid = train_test_split(train_2[dinner_col].drop(columns=['석식계']), train_2['석식계'], test_size=0.2)\n",
        "\n",
        "#     model = CatBoostRegressor(**params_catboost)\n",
        "#     model.fit(\n",
        "#         X_train,\n",
        "#         y_train,\n",
        "#         cat_features=[],\n",
        "#         eval_set=(X_valid, y_valid),\n",
        "#         early_stopping_rounds=100,\n",
        "#         verbose=True,\n",
        "#     )\n",
        "\n",
        "#     from sklearn.metrics import mean_absolute_error\n",
        "#     catboost_pred = model.predict(X_valid)\n",
        "#     mean_absolute_error = (mean_absolute_error(y_valid, catboost_pred))\n",
        "    \n",
        "#     return mean_absolute_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETHjYhBtfm-S"
      },
      "source": [
        "# def objective(trial: Trial) -> float:\n",
        "#     params_catboost = {\n",
        "#     'iterations': trial.suggest_int(\"iterations\", 1, 100000),\n",
        "#     'learning_rate': trial.suggest_float(\"learning_rate\", 0.001, 0.01),\n",
        "#     'eval_metric': 'MAE',\n",
        "#     'loss_function': 'MAE',\n",
        "#     'random_seed': 42,\n",
        "#     'logging_level': 'Silent',\n",
        "#     'use_best_model': True,\n",
        "#     'od_type': 'Iter',\n",
        "#     'od_wait': trial.suggest_int(\"od_wait\", 1, 10000),\n",
        "#     'one_hot_max_size': trial.suggest_int(\"one_hot_max_size\", 1, 10000),\n",
        "#     'l2_leaf_reg': trial.suggest_int(\"l2_leaf_reg\", 1, 15),\n",
        "#     'depth': trial.suggest_int(\"l2_leaf_reg\", 1, 15),\n",
        "#     'rsm': trial.suggest_float(\"rsm\", 0.1, 0.9),\n",
        "#     'random_strength': trial.suggest_int(\"random_strength\", 1, 10000),\n",
        "#     'bagging_temperature': trial.suggest_int(\"bagging_temperature\", 1, 10000),\n",
        "# }\n",
        "    \n",
        "#     X_train, X_valid, y_train, y_valid = train_test_split(train_2[dinner_col].drop(columns=['석식계']), train_2['석식계'], test_size=0.2)\n",
        "\n",
        "#     model = CatBoostRegressor(**params_catboost)\n",
        "#     model.fit(\n",
        "#         X_train,\n",
        "#         y_train,\n",
        "#         cat_features=[],\n",
        "#         eval_set=(X_valid, y_valid),\n",
        "#         early_stopping_rounds=100,\n",
        "#         verbose=True,\n",
        "#     )\n",
        "\n",
        "#     from sklearn.metrics import mean_absolute_error\n",
        "#     catboost_pred = model.predict(X_valid)\n",
        "#     mean_absolute_error = (mean_absolute_error(y_valid, catboost_pred))\n",
        "    \n",
        "#     return mean_absolute_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJwiyeeTftoR"
      },
      "source": [
        "# sampler = TPESampler(seed=42)\n",
        "# study = optuna.create_study(\n",
        "#     study_name=\"catboost_parameter_opt\",\n",
        "#     direction=\"minimize\",\n",
        "#     sampler=sampler,\n",
        "# )\n",
        "# study.optimize(objective, n_trials=50)\n",
        "# print(\"Best Score:\", study.best_value)\n",
        "# print(\"Best trial:\", study.best_trial.params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5z8Gg_8Of7t-"
      },
      "source": [
        "# catboost params\n",
        "params = {'colsample_bylevel': 0.07483210682368195, 'depth': 8, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bernoulli',\n",
        "          'learning_rate': 0.01866828671939659, 'l2_leaf_reg': 0.031061104466436423, 'min_child_samples': 2, 'random_strength': 0.24750133579738517, 'subsample': 0.5114408837171173}\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(train_2[dinner_col].drop(columns=['석식계']), train_2['석식계'], test_size=0.2)\n",
        "model = CatBoostRegressor(**params)\n",
        "model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        cat_features=[],\n",
        "        eval_set=(X_valid, y_valid),\n",
        "        early_stopping_rounds=100,\n",
        "        verbose=True,\n",
        "    )\n",
        "\n",
        "catboost_pred_dinner = model.predict(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HagjY0XJ3mkh"
      },
      "source": [
        "## LightGBM 모델링"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBxZyCYsAgii"
      },
      "source": [
        "### 점심"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9KWl98kDcSS"
      },
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "import gc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1pUKSlHIskx"
      },
      "source": [
        "# def objective(trial: Trial) -> float:\n",
        "#     params_lgb = {\n",
        "#         \"random_state\": 42,\n",
        "#         \"verbosity\": -1,\n",
        "#         \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.5),\n",
        "#         \"n_estimators\": 10000,\n",
        "#         \"objective\": \"regression\",\n",
        "#         \"metric\": \"MAE\",\n",
        "#         \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 3e-5),\n",
        "#         \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 9e-2),\n",
        "#         \"max_depth\": trial.suggest_int(\"max_depth\", 1, 20),\n",
        "#         \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
        "#         'feature_fraction':trial.suggest_discrete_uniform('feature_fraction',0.5, 1, 0.01),\n",
        "#         'bagging_fraction':trial.suggest_discrete_uniform('bagging_fraction',0.5, 1, 0.01),\n",
        "#         \"subsample_freq\": trial.suggest_int(\"subsample_freq\", 1, 10),\n",
        "#         \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
        "#         \"max_bin\": trial.suggest_int(\"max_bin\", 200, 500),\n",
        "#     }\n",
        "#     X_train, X_valid, y_train, y_valid = train_test_split(train_1[lunch_col].drop(columns=['중식계']), train_1['중식계'], test_size=0.2)\n",
        "    \n",
        "#     model = LGBMRegressor(**params_lgb)\n",
        "#     model.fit(\n",
        "#         X_train,\n",
        "#         y_train,\n",
        "#         eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
        "#         early_stopping_rounds=100,\n",
        "#         verbose=False,\n",
        "#     )\n",
        "#     from sklearn.metrics import mean_absolute_error\n",
        "#     lgbm_pred = model.predict(X_valid)\n",
        "#     mean_absolute_error = (mean_absolute_error(y_valid, lgbm_pred))\n",
        "    \n",
        "#     return mean_absolute_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mrp2lCE23c2a"
      },
      "source": [
        "# sampler = TPESampler(seed=42)\n",
        "# study = optuna.create_study(\n",
        "#     study_name=\"lgbm_parameter_opt\",\n",
        "#     direction=\"minimize\",\n",
        "#     sampler=sampler,\n",
        "# )\n",
        "# study.optimize(objective, n_trials=100)\n",
        "# print(\"Best Score:\", study.best_value)\n",
        "# print(\"Best trial:\", study.best_trial.params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pObWaWW5IVE"
      },
      "source": [
        "params = {'learning_rate': 0.07268442128680935, 'reg_alpha': 2.2381047339569982e-05, 'reg_lambda': 0.0062625553020554565, 'max_depth': 9, 'num_leaves': 250,\n",
        "          'feature_fraction': 0.8500000000000001, 'bagging_fraction': 0.9199999999999999, 'subsample_freq': 9, 'min_child_samples': 75, 'max_bin': 322}\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(train_1[lunch_col].drop(columns=['중식계']), train_1['중식계'], test_size=0.2)\n",
        "model = LGBMRegressor(**params)\n",
        "model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        eval_set=(X_valid, y_valid),\n",
        "        early_stopping_rounds=100,\n",
        "        verbose=False,\n",
        "    )\n",
        "\n",
        "lgbm_pred_lunch = model.predict(test[lunch_col].drop(columns=['중식계']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhgJQepwAkEb"
      },
      "source": [
        "### 저녁"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xv9xpPauLNaW"
      },
      "source": [
        "# def objective(trial: Trial) -> float:\n",
        "#     params_lgb = {\n",
        "#         \"random_state\": 42,\n",
        "#         \"verbosity\": -1,\n",
        "#         \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.5),\n",
        "#         \"n_estimators\": 10000,\n",
        "#         \"objective\": \"regression\",\n",
        "#         \"metric\": \"MAE\",\n",
        "#         \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 3e-5),\n",
        "#         \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 9e-2),\n",
        "#         \"max_depth\": trial.suggest_int(\"max_depth\", 1, 20),\n",
        "#         \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
        "#         'feature_fraction':trial.suggest_discrete_uniform('feature_fraction',0.5, 1, 0.01),\n",
        "#         'bagging_fraction':trial.suggest_discrete_uniform('bagging_fraction',0.5, 1, 0.01),\n",
        "#         \"subsample_freq\": trial.suggest_int(\"subsample_freq\", 1, 10),\n",
        "#         \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
        "#         \"max_bin\": trial.suggest_int(\"max_bin\", 200, 500),\n",
        "#     }\n",
        "#     X_train, X_valid, y_train, y_valid = train_test_split(train_2[dinner_col].drop(columns=['석식계']), train_2['석식계'], test_size=0.2)\n",
        "    \n",
        "#     model = LGBMRegressor(**params_lgb)\n",
        "#     model.fit(\n",
        "#         X_train,\n",
        "#         y_train,\n",
        "#         eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
        "#         early_stopping_rounds=100,\n",
        "#         verbose=False,\n",
        "#     )\n",
        "#     from sklearn.metrics import mean_absolute_error\n",
        "#     lgbm_pred = model.predict(X_valid)\n",
        "#     mean_absolute_error = (mean_absolute_error(y_valid, lgbm_pred))\n",
        "    \n",
        "#     return mean_absolute_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OvGHDu5e3mX"
      },
      "source": [
        "# sampler = TPESampler(seed=42)\n",
        "# study = optuna.create_study(\n",
        "#     study_name=\"lgbm_parameter_opt\",\n",
        "#     direction=\"minimize\",\n",
        "#     sampler=sampler,\n",
        "# )\n",
        "# study.optimize(objective, n_trials=100)\n",
        "# print(\"Best Score:\", study.best_value)\n",
        "# print(\"Best trial:\", study.best_trial.params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVsAI_1Ye3hg"
      },
      "source": [
        "params = {'learning_rate': 0.2570179753119847, 'reg_alpha': 2.655816280486811e-05, 'reg_lambda': 0.0539252002807482, 'max_depth': 3, 'num_leaves': 101,\n",
        "          'feature_fraction': 0.62, 'bagging_fraction': 0.81, 'subsample_freq': 7, 'min_child_samples': 72, 'max_bin': 424}\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(train_2[dinner_col].drop(columns=['석식계']), train_2['석식계'], test_size=0.2)\n",
        "model = LGBMRegressor(**params)\n",
        "model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        eval_set=(X_valid, y_valid),\n",
        "        early_stopping_rounds=100,\n",
        "        verbose=False,\n",
        "    )\n",
        "\n",
        "lgbm_pred_dinner = model.predict(test[dinner_col].drop(columns=['석식계']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_8IMY2ze3Yf"
      },
      "source": [
        "## Xgboost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wp1mvs85NAPO"
      },
      "source": [
        "### 점심"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZQzrVBmAwiC"
      },
      "source": [
        "from xgboost import XGBRegressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiY0tsaoe28s"
      },
      "source": [
        "def objective(trial: Trial) -> float :\n",
        "    tree_method = ['exact','approx','hist']\n",
        "    boosting_list = ['gbtree', 'gblinear']\n",
        "    objective_list_reg = ['reg:linear', 'reg:gamma', 'reg:tweedie']\n",
        "    param_xgboost = {\n",
        "        'boosting':trial.suggest_categorical('boosting', boosting_list),\n",
        "        'tree_method':trial.suggest_categorical('tree_method', tree_method),\n",
        "        'max_depth':trial.suggest_int('max_depth', 2, 25),\n",
        "        'reg_alpha':trial.suggest_int('reg_alpha', 0, 5),\n",
        "        'reg_lambda':trial.suggest_int('reg_lambda', 0, 5),\n",
        "        'min_child_weight':trial.suggest_int('min_child_weight', 0, 5),\n",
        "        'gamma':trial.suggest_int('gamma', 0, 5),\n",
        "        'learning_rate':trial.suggest_loguniform('learning_rate',0.005,0.5),\n",
        "        'eval_metric': \"mae\",\n",
        "        'objective':trial.suggest_categorical('objective', objective_list_reg),\n",
        "        'colsample_bytree':trial.suggest_discrete_uniform('colsample_bytree', 0.1, 1, 0.01),\n",
        "        'colsample_bynode':trial.suggest_discrete_uniform('colsample_bynode', 0.1, 1, 0.01),\n",
        "        'colsample_bylevel':trial.suggest_discrete_uniform('colsample_bylevel', 0.1, 1, 0.01),\n",
        "        'subsample':trial.suggest_discrete_uniform('subsample', 0.5, 1, 0.05),\n",
        "        'nthread' : -1  \n",
        "      }\n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(train_1[lunch_col].drop(columns=['중식계']), train_1['중식계'], test_size=0.2)\n",
        "    \n",
        "    model = XGBRegressor(**param_xgboost)\n",
        "    model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
        "        early_stopping_rounds=100,\n",
        "        verbose=False,\n",
        "    )\n",
        "    from sklearn.metrics import mean_absolute_error\n",
        "    xgboost_pred = model.predict(X_valid)\n",
        "    mean_absolute_error = (mean_absolute_error(y_valid, xgboost_pred))\n",
        "    \n",
        "    return mean_absolute_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCIH8jGUAtwt"
      },
      "source": [
        "# sampler = TPESampler(seed=42)\n",
        "# study = optuna.create_study(\n",
        "#     study_name=\"xgboost_parameter_opt\",\n",
        "#     direction=\"minimize\",\n",
        "#     sampler=sampler,\n",
        "# )\n",
        "# study.optimize(objective, n_trials=100)\n",
        "# print(\"Best Score:\", study.best_value)\n",
        "# print(\"Best trial:\", study.best_trial.params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5a4vo8KAvQq"
      },
      "source": [
        "params = {'boosting': 'gblinear', 'tree_method': 'approx', 'max_depth': 2, 'reg_alpha': 0, 'reg_lambda': 4, 'min_child_weight': 3, 'gamma': 0,\n",
        "          'learning_rate': 0.2525106791852816, 'objective': 'reg:gamma', 'colsample_bytree': 0.84, 'colsample_bynode': 0.39, 'colsample_bylevel': 0.1, 'subsample': 0.8500000000000001}\n",
        "        \n",
        "X_train, X_valid, y_train, y_valid = train_test_split(train_1[lunch_col].drop(columns=['중식계']), train_1['중식계'], test_size=0.2)\n",
        "model = XGBRegressor(**params)\n",
        "model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        eval_set=[(X_valid, y_valid)],\n",
        "        early_stopping_rounds=100,\n",
        "        verbose=False,\n",
        "    )\n",
        "\n",
        "xgboost_pred_lunch = model.predict(test[lunch_col].drop(columns=['중식계']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFii1mkyRiP_"
      },
      "source": [
        "### 저녁"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ7CtWkFAvY_"
      },
      "source": [
        "def huber_approx_obj(preds, dtrain):\n",
        "    d = preds - dtrain #remove .get_labels() for sklearn\n",
        "    h = 1  #h is delta in the graphic\n",
        "    scale = 1 + (d / h) ** 2\n",
        "    scale_sqrt = np.sqrt(scale)\n",
        "    grad = d / scale_sqrt\n",
        "    hess = 1 / scale / scale_sqrt\n",
        "    return grad, hess\n",
        "\n",
        "def objective(trial: Trial) -> float :\n",
        "    tree_method = ['exact','approx','hist']\n",
        "    boosting_list = ['gbtree', 'gblinear']\n",
        "    objective_list_reg = ['reg:linear', 'reg:gamma', 'reg:tweedie', huber_approx_obj]\n",
        "    param_xgboost = {\n",
        "        'boosting':trial.suggest_categorical('boosting', boosting_list),\n",
        "        'tree_method':trial.suggest_categorical('tree_method', tree_method),\n",
        "        'max_depth':trial.suggest_int('max_depth', 2, 25),\n",
        "        'reg_alpha':trial.suggest_int('reg_alpha', 0, 5),\n",
        "        'reg_lambda':trial.suggest_int('reg_lambda', 0, 5),\n",
        "        'min_child_weight':trial.suggest_int('min_child_weight', 0, 5),\n",
        "        'gamma':trial.suggest_int('gamma', 0, 5),\n",
        "        'learning_rate':trial.suggest_loguniform('learning_rate',0.005,0.5),\n",
        "        'eval_metric': \"mae\",\n",
        "        'objective':trial.suggest_categorical('objective', objective_list_reg),\n",
        "        'colsample_bytree':trial.suggest_discrete_uniform('colsample_bytree', 0.1, 1, 0.01),\n",
        "        'colsample_bynode':trial.suggest_discrete_uniform('colsample_bynode', 0.1, 1, 0.01),\n",
        "        'colsample_bylevel':trial.suggest_discrete_uniform('colsample_bylevel', 0.1, 1, 0.01),\n",
        "        'subsample':trial.suggest_discrete_uniform('subsample', 0.5, 1, 0.05),\n",
        "        'nthread' : -1  \n",
        "      }\n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(train_2[dinner_col].drop(columns=['석식계']), train_2['석식계'], test_size=0.2)\n",
        "    \n",
        "    model = XGBRegressor(**param_xgboost)\n",
        "    model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
        "        early_stopping_rounds=100,\n",
        "        verbose=False,\n",
        "    )\n",
        "    from sklearn.metrics import mean_absolute_error\n",
        "    xgboost_pred = model.predict(X_valid)\n",
        "    mean_absolute_error = (mean_absolute_error(y_valid, xgboost_pred))\n",
        "    \n",
        "    return mean_absolute_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfZJIMWZAvbZ"
      },
      "source": [
        "# sampler = TPESampler(seed=42)\n",
        "# study = optuna.create_study(\n",
        "#     study_name=\"xgboost_parameter_opt\",\n",
        "#     direction=\"minimize\",\n",
        "#     sampler=sampler,\n",
        "# )\n",
        "# study.optimize(objective, n_trials=100)\n",
        "# print(\"Best Score:\", study.best_value)\n",
        "# print(\"Best trial:\", study.best_trial.params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tek2ajvZAvea"
      },
      "source": [
        "params = {'boosting': 'gbtree', 'tree_method': 'approx', 'max_depth': 25, 'reg_alpha': 3, 'reg_lambda': 2, 'min_child_weight': 5, 'gamma': 0,\n",
        "          'learning_rate': 0.4615619384348788, 'objective': 'reg:gamma', 'colsample_bytree': 0.5700000000000001, 'colsample_bynode': 0.97, 'colsample_bylevel': 0.78, 'subsample': 1.0}\n",
        "        \n",
        "X_train, X_valid, y_train, y_valid = train_test_split(train_2[dinner_col].drop(columns=['석식계']), train_2['석식계'], test_size=0.2)\n",
        "model = XGBRegressor(**params)\n",
        "model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        eval_set=[(X_valid, y_valid)],\n",
        "        early_stopping_rounds=100,\n",
        "        verbose=False,\n",
        "    )\n",
        "\n",
        "xgboost_pred_dinner = model.predict(test[dinner_col].drop(columns=['석식계']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsnRgKfHqEPv"
      },
      "source": [
        "## RandomForest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5FatEy6uL5D"
      },
      "source": [
        "### 점심"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5B_Hq3vpEA-"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIxG9hbapFbK"
      },
      "source": [
        "# def objective(trial: Trial) -> float :\n",
        "#     params_rf = {\n",
        "#         'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n",
        "#         'max_depth': trial.suggest_int('max_depth', 4, 50),\n",
        "#         'min_samples_split': trial.suggest_int('min_samples_split', 2, 150),\n",
        "#         'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 60),\n",
        "#         'random_state' : 42,\n",
        "#     }\n",
        "#     X_train, X_valid, y_train, y_valid = train_test_split(train_1[lunch_col].drop(columns=['중식계']), train_1['중식계'], test_size=0.2)\n",
        "    \n",
        "#     model = RandomForestRegressor(**params_rf)\n",
        "#     model.fit(\n",
        "#         X_train,\n",
        "#         y_train,\n",
        "#         # eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
        "#         # early_stopping_rounds=100,\n",
        "#         # verbose=False,\n",
        "#     )\n",
        "#     from sklearn.metrics import mean_absolute_error\n",
        "#     rf_pred = model.predict(X_valid)\n",
        "#     mean_absolute_error = (mean_absolute_error(y_valid, rf_pred))\n",
        "    \n",
        "#     return mean_absolute_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vM93vu_xpFXU"
      },
      "source": [
        "# sampler = TPESampler(seed=42)\n",
        "# study = optuna.create_study(\n",
        "#     study_name=\"rf_parameter_opt\",\n",
        "#     direction=\"minimize\",\n",
        "#     sampler=sampler,\n",
        "# )\n",
        "# study.optimize(objective, n_trials=100)\n",
        "# print(\"Best Score:\", study.best_value)\n",
        "# print(\"Best trial:\", study.best_trial.params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Zmcg4ZnpFTj"
      },
      "source": [
        "params = {'n_estimators': 900, 'max_depth': 32, 'min_samples_split': 6, 'min_samples_leaf': 3}\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(train_1[lunch_col].drop(columns=['중식계']), train_1['중식계'], test_size=0.2)\n",
        "model = RandomForestRegressor(**params)\n",
        "model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        # eval_set=[(X_valid, y_valid)],\n",
        "        # early_stopping_rounds=100,\n",
        "        # verbose=False,\n",
        "    )\n",
        "\n",
        "rf_pred_lunch = model.predict(test[lunch_col].drop(columns=['중식계']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pRUBQ4-uO3D"
      },
      "source": [
        "### 저녁"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubYB6BkHpFL-"
      },
      "source": [
        "# def objective(trial: Trial) -> float :\n",
        "#     params_rf = {\n",
        "#         'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n",
        "#         'max_depth': trial.suggest_int('max_depth', 4, 50),\n",
        "#         'min_samples_split': trial.suggest_int('min_samples_split', 2, 150),\n",
        "#         'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 60),\n",
        "#         'random_state' : 42,\n",
        "#     }\n",
        "#     X_train, X_valid, y_train, y_valid = train_test_split(train_2[dinner_col].drop(columns=['석식계']), train_2['석식계'], test_size=0.2)\n",
        "    \n",
        "#     model = RandomForestRegressor(**params_rf)\n",
        "#     model.fit(\n",
        "#         X_train,\n",
        "#         y_train,\n",
        "#         # eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
        "#         # early_stopping_rounds=100,\n",
        "#         # verbose=False,\n",
        "#     )\n",
        "#     from sklearn.metrics import mean_absolute_error\n",
        "#     rf_pred = model.predict(X_valid)\n",
        "#     mean_absolute_error = (mean_absolute_error(y_valid, rf_pred))\n",
        "    \n",
        "#     return mean_absolute_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "op5Q3RV2pFJH"
      },
      "source": [
        "# sampler = TPESampler(seed=42)\n",
        "# study = optuna.create_study(\n",
        "#     study_name=\"rf_parameter_opt\",\n",
        "#     direction=\"minimize\",\n",
        "#     sampler=sampler,\n",
        "# )\n",
        "# study.optimize(objective, n_trials=100)\n",
        "# print(\"Best Score:\", study.best_value)\n",
        "# print(\"Best trial:\", study.best_trial.params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMhAMaKzpFFx"
      },
      "source": [
        "params = {'n_estimators': 974, 'max_depth': 19, 'min_samples_split': 17, 'min_samples_leaf': 1}\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(train_2[dinner_col].drop(columns=['석식계']), train_2['석식계'], test_size=0.2)\n",
        "model = RandomForestRegressor(**params)\n",
        "model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        # eval_set=[(X_valid, y_valid)],\n",
        "        # early_stopping_rounds=100,\n",
        "        # verbose=False,\n",
        "    )\n",
        "\n",
        "rf_pred_dinner = model.predict(test[dinner_col].drop(columns=['석식계']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvg8ZOEUv8u7"
      },
      "source": [
        "## Extra Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eo7NZwZVv-iU"
      },
      "source": [
        "### 점심"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTYHb-oOv-tg"
      },
      "source": [
        "from sklearn.ensemble import ExtraTreesRegressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCXc8ziqv-3Q"
      },
      "source": [
        "def objective(trial: Trial) -> float :\n",
        "    params_extra = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n",
        "        'max_depth': trial.suggest_int('max_depth', 4, 50),\n",
        "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 150),\n",
        "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 60),\n",
        "        'random_state' : 42,\n",
        "    }\n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(train_1.drop(columns=['중식계']), train_1['중식계'], test_size=0.2)\n",
        "    \n",
        "    model = ExtraTreesRegressor(**params_extra)\n",
        "    model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        # eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
        "        # early_stopping_rounds=100,\n",
        "        # verbose=False,\n",
        "    )\n",
        "    from sklearn.metrics import mean_absolute_error\n",
        "    extra_pred = model.predict(X_valid)\n",
        "    mean_absolute_error = (mean_absolute_error(y_valid, extra_pred))\n",
        "    \n",
        "    return mean_absolute_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJF3CMkTv_Eb"
      },
      "source": [
        "sampler = TPESampler(seed=42)\n",
        "study = optuna.create_study(\n",
        "    study_name=\"extra_parameter_opt\",\n",
        "    direction=\"minimize\",\n",
        "    sampler=sampler,\n",
        ")\n",
        "study.optimize(objective, n_trials=100)\n",
        "print(\"Best Score:\", study.best_value)\n",
        "print(\"Best trial:\", study.best_trial.params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAEWAwa8v8qy"
      },
      "source": [
        "params = {'n_estimators': 908, 'max_depth': 24, 'min_samples_split': 20, 'min_samples_leaf': 12}\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(train_1[lunch_col].drop(columns=['중식계']), train_1['중식계'], test_size=0.2)\n",
        "model = ExtraTreesRegressor(**params)\n",
        "model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        # eval_set=[(X_valid, y_valid)],\n",
        "        # early_stopping_rounds=100,\n",
        "        # verbose=False,\n",
        "    )\n",
        "\n",
        "extra_pred_lunch = model.predict(test[lunch_col].drop(columns=['중식계']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYmVA-6jxTUn"
      },
      "source": [
        "### 저녁"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdHZeH9BxTQm"
      },
      "source": [
        "# def objective(trial: Trial) -> float :\n",
        "#     params_extra = {\n",
        "#         'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n",
        "#         'max_depth': trial.suggest_int('max_depth', 4, 50),\n",
        "#         'min_samples_split': trial.suggest_int('min_samples_split', 2, 150),\n",
        "#         'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 60),\n",
        "#         'random_state' : 42,\n",
        "#     }\n",
        "#     X_train, X_valid, y_train, y_valid = train_test_split(train_2[dinner_col].drop(columns=['석식계']), train_2['석식계'], test_size=0.2)\n",
        "    \n",
        "#     model = ExtraTreesRegressor(**params_extra)\n",
        "#     model.fit(\n",
        "#         X_train,\n",
        "#         y_train,\n",
        "#         # eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
        "#         # early_stopping_rounds=100,\n",
        "#         # verbose=False,\n",
        "#     )\n",
        "#     from sklearn.metrics import mean_absolute_error\n",
        "#     extra_pred = model.predict(X_valid)\n",
        "#     mean_absolute_error = (mean_absolute_error(y_valid, extra_pred))\n",
        "    \n",
        "#     return mean_absolute_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9U3HCNvixTMf"
      },
      "source": [
        "# sampler = TPESampler(seed=42)\n",
        "# study = optuna.create_study(\n",
        "#     study_name=\"extra_parameter_opt\",\n",
        "#     direction=\"minimize\",\n",
        "#     sampler=sampler,\n",
        "# )\n",
        "# study.optimize(objective, n_trials=100)\n",
        "# print(\"Best Score:\", study.best_value)\n",
        "# print(\"Best trial:\", study.best_trial.params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rgmer0wQxTHt"
      },
      "source": [
        "params = {'n_estimators': 383, 'max_depth': 49, 'min_samples_split': 16, 'min_samples_leaf': 6}\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(train_2[dinner_col].drop(columns=['석식계']), train_2['석식계'], test_size=0.2)\n",
        "model = ExtraTreesRegressor(**params)\n",
        "model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        # eval_set=[(X_valid, y_valid)],\n",
        "        # early_stopping_rounds=100,\n",
        "        # verbose=False,\n",
        "    )\n",
        "\n",
        "extra_pred_dinner = model.predict(test[dinner_col].drop(columns=['석식계']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlhoYSUxUUKe"
      },
      "source": [
        "## 제출 - Stacking Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dv55_aRnUiIb"
      },
      "source": [
        "sample_submission = pd.read_csv(path+'sample_submission.csv')\n",
        "submission = sample_submission.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQbaKYOPUmJH"
      },
      "source": [
        "submission['중식계'] = (catboost_pred_lunch+lgbm_pred_lunch+xgboost_pred_lunch+rf_pred_lunch+extra_pred_lunch)/5\n",
        "submission['석식계'] = (catboost_pred_dinner+lgbm_pred_dinner+xgboost_pred_dinner+rf_pred_dinner+extra_pred_dinner)/5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QmGsIMAUNQ-"
      },
      "source": [
        "from datetime import datetime\n",
        "now_tm = datetime.today().strftime('%Y-%m-%d-%H:%M:%S')\n",
        "\n",
        "sub_path = '/content/drive/MyDrive/DACON/Dacon_Industry_Meal/submit/'\n",
        "submission.to_csv(sub_path+now_tm+'.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjoU2-DLRwUu"
      },
      "source": [
        "## Flaml"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7yTWBAq56_4"
      },
      "source": [
        "import numpy as np\n",
        "import shap\n",
        "def get_columns(model, X_test, train, del_col_nm='석식계'):\n",
        "  explainer = shap.Explainer(model._model)\n",
        "  shap_values = explainer(X_test)\n",
        "  vals = np.abs(shap_values.values).mean(0)\n",
        "  cols = list(train.columns)\n",
        "  cols.remove(del_col_nm)\n",
        "  feature_importance = pd.DataFrame(list(zip(cols, vals)), columns=['col_name','feature_importance_vals'])\n",
        "  feature_importance.sort_values(by=['feature_importance_vals'], ascending=False, inplace=True)\n",
        "  return feature_importance[feature_importance['feature_importance_vals'] > 1]['col_name'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JARYWbqt6jvM"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def run_kfold(train, test, selected_cols=[], target_col='중식계'):\n",
        "    folds=KFold(n_splits=5, shuffle=True, random_state=2021)\n",
        "    outcomes=[]\n",
        "    model = None\n",
        "    sub=[]\n",
        "\n",
        "    X = None\n",
        "    X_test = None\n",
        "    if len(selected_cols) > 0:\n",
        "      X = np.array(train[selected_cols])\n",
        "      X_test = np.array(test[selected_cols])\n",
        "    else:\n",
        "      X = np.array(train.drop(columns=[target_col]))\n",
        "      cols = list(train.columns)\n",
        "      cols.remove(target_col)\n",
        "      X_test = np.array(test[cols])\n",
        "\n",
        "    y = np.array(train[target_col])\n",
        "    # 정규화\n",
        "    # standardScaler = StandardScaler()\n",
        "    # standardScaler.fit(X)\n",
        "    # X = standardScaler.transform(X)\n",
        "\n",
        "    # X_test = standardScaler.transform(X_test) # 정규화\n",
        "    cols_import = []\n",
        "\n",
        "    for n_fold, (train_index, val_index) in enumerate(folds.split(X)):\n",
        "        print(n_fold, 'fold started =========================================')\n",
        "        # X_train1, X_val = , X_train.loc[val_index]\n",
        "        y_train = y[train_index]\n",
        "        X_train = X[train_index]\n",
        "        # y_train1, y_val = y_train.loc[train_index], y_train.loc[val_index]\n",
        "        y_val = y[val_index]\n",
        "        X_val = X[val_index]\n",
        "        if n_fold == 0:\n",
        "          automl = AutoML()\n",
        "          automl_settings = {\n",
        "              \"time_budget\": 120,  # in seconds\n",
        "              \"metric\": 'mae',\n",
        "              \"task\": 'regression'\n",
        "          }\n",
        "          automl.fit(X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val, **automl_settings)\n",
        "          model = automl.model\n",
        "        else:\n",
        "          model.fit(X_train, y_train, eval_set=[(X_val, y_val)],\n",
        "                        eval_metric='mae', early_stopping_rounds=10) \n",
        "        \n",
        "        pred1 = model.predict(X_test)\n",
        "        if len(selected_cols) == 0:\n",
        "          cols_import += list(get_columns(model, X_test, train=train, del_col_nm=target_col))\n",
        "        sub.append(pred1)\n",
        "    return sub, model, X_test, set(cols_import)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rqhMdnC7qD4"
      },
      "source": [
        "my_submission, model, X_test, cols_import = run_kfold(train_1, test, selected_cols=[], target_col='중식계')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKBj0moG5Q5B"
      },
      "source": [
        "pred1 = np.mean(my_submission, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQ9AE_4e5S3n"
      },
      "source": [
        "my_submission, model, X_test, cols_import = run_kfold(train_2, test, selected_cols=[], target_col='석식계')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCkQhcmU5XF0"
      },
      "source": [
        "pred2 = np.mean(my_submission, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zicAX8FQHz_"
      },
      "source": [
        "# def run_kfold2(train, test):\n",
        "#     folds=KFold(n_splits=5, shuffle=True, random_state=2021)\n",
        "#     outcomes=[]\n",
        "#     model = None\n",
        "#     sub=[]\n",
        "\n",
        "#     X = np.array(train.drop(columns=['석식계']))\n",
        "#     y = np.array(train['석식계'])\n",
        "#     # 정규화\n",
        "#     standardScaler = StandardScaler()\n",
        "#     standardScaler.fit(X)\n",
        "#     X = standardScaler.transform(X)\n",
        "\n",
        "#     cols = list(train.columns)\n",
        "#     cols.remove('석식계')\n",
        "#     X_test = np.array(test[cols])\n",
        "#     X_test = standardScaler.transform(X_test) # 정규화\n",
        "\n",
        "#     # X = np.array(X)\n",
        "\n",
        "#     for n_fold, (train_index, val_index) in enumerate(folds.split(X)):\n",
        "#         print(n_fold, 'fold started =========================================')\n",
        "#         # X_train1, X_val = , X_train.loc[val_index]\n",
        "#         y_train = y[train_index]\n",
        "#         X_train = X[train_index]\n",
        "#         # y_train1, y_val = y_train.loc[train_index], y_train.loc[val_index]\n",
        "#         y_val = y[val_index]\n",
        "#         X_val = X[val_index]\n",
        "#         if n_fold == 0:\n",
        "#           automl = AutoML()\n",
        "#           automl_settings = {\n",
        "#               \"time_budget\": 240,  # in seconds\n",
        "#               \"metric\": 'mae',\n",
        "#               \"task\": 'regression'\n",
        "#           }\n",
        "#           # print(len(X_train1))\n",
        "#           # print(len(y_train1.to_numpy()))\n",
        "#           automl.fit(X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val, **automl_settings)\n",
        "#           model = automl.model\n",
        "#         else:\n",
        "#           model.fit(X_train, y_train, eval_set=[(X_val, y_val)],\n",
        "#                         eval_metric='mae', early_stopping_rounds=10) \n",
        "        \n",
        "#         pred1 = model.predict(X_test)\n",
        "#         sub.append(pred1)\n",
        "#     return sub\n",
        "\n",
        "# my_submission = run_kfold2(train_2, test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jt2ARcrQLdU"
      },
      "source": [
        "sample_submission = pd.read_csv(path+'sample_submission.csv')\n",
        "submission = sample_submission.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgN_9TE0QML8"
      },
      "source": [
        "submission['중식계'] = pred1\n",
        "submission['석식계'] = pred2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ej4FbP7QNXF"
      },
      "source": [
        "from datetime import datetime\n",
        "now_tm = datetime.today().strftime('%Y-%m-%d-%H:%M:%S')\n",
        "\n",
        "sub_path = '/content/drive/MyDrive/DACON/Dacon_Industry_Meal/submit/'\n",
        "submission.to_csv(sub_path+now_tm+'.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWZwi_LP8E1X"
      },
      "source": [
        "submission"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6U1sTx358yj-"
      },
      "source": [
        "# !pip install /content/drive/MyDrive/구내식당/water/dacon_submit_api-0.0.4-py3-none-any.whl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zO4bqf739GsW"
      },
      "source": [
        "from dacon_submit_api import dacon_submit_api \n",
        "\n",
        "result = dacon_submit_api.post_submission_file(\n",
        "'/content/drive/MyDrive/DACON/Dacon_Industry_Meal/submit/2021-07-09-00:17:13.csv.csv', \n",
        "'c7eb6960dc46f188d012c09ac2fbde1a0f90af05eeef92249719473834bb981e', \n",
        "'235743', \n",
        "'블메소리', \n",
        "'국정감사 컬럼 삭제')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LE_WTj5OCvsW"
      },
      "source": [
        "## Pycaret"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9H957l5l16L"
      },
      "source": [
        "feature_selection - 전진 단계별 선택법"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5lUYMDml50w"
      },
      "source": [
        "import statsmodels.api as sm\n",
        "## 전진 단계별 선택법 - 중식\n",
        "lunch_cols = train_1.columns.tolist()\n",
        "lunch_cols.remove('중식계')\n",
        "lunch_cols.remove('일자')\n",
        "variables = lunch_cols ## 설명 변수 리스트\n",
        " \n",
        "y = train_1['중식계'] ## 반응 변수\n",
        "selected_variables = [] ## 선택된 변수들\n",
        "sl_enter = 0.05\n",
        "sl_remove = 0.05\n",
        " \n",
        "sv_per_step = [] ## 각 스텝별로 선택된 변수들\n",
        "adjusted_r_squared = [] ## 각 스텝별 수정된 결정계수\n",
        "steps = [] ## 스텝\n",
        "step = 0\n",
        "while len(variables) > 0:\n",
        "    remainder = list(set(variables) - set(selected_variables))\n",
        "    pval = pd.Series(index=remainder) ## 변수의 p-value\n",
        "    ## 기존에 포함된 변수와 새로운 변수 하나씩 돌아가면서 \n",
        "    ## 선형 모형을 적합한다.\n",
        "    for col in remainder: \n",
        "        X = train_1[selected_variables+[col]]\n",
        "        X = sm.add_constant(X)\n",
        "        model = sm.OLS(y,X).fit()\n",
        "\n",
        "        pval[col] = model.pvalues[col]\n",
        " \n",
        "    min_pval = pval.min()\n",
        "    if min_pval < sl_enter: ## 최소 p-value 값이 기준 값보다 작으면 포함\n",
        "        selected_variables.append(pval.idxmin())\n",
        "        ## 선택된 변수들에대해서\n",
        "        ## 어떤 변수를 제거할지 고른다.\n",
        "        while len(selected_variables) > 0:\n",
        "            selected_X = train_1[selected_variables]\n",
        "            selected_X = sm.add_constant(selected_X)\n",
        "            selected_pval = sm.OLS(y,selected_X).fit().pvalues[1:] ## 절편항의 p-value는 뺀다\n",
        "            max_pval = selected_pval.max()\n",
        "            if max_pval >= sl_remove: ## 최대 p-value값이 기준값보다 크거나 같으면 제외\n",
        "                remove_variable = selected_pval.idxmax()\n",
        "                selected_variables.remove(remove_variable)\n",
        "            else:\n",
        "                break\n",
        "        \n",
        "        step += 1\n",
        "        steps.append(step)\n",
        "        adj_r_squared = sm.OLS(y,sm.add_constant(train_1[selected_variables])).fit().rsquared_adj\n",
        "        adjusted_r_squared.append(adj_r_squared)\n",
        "        sv_per_step.append(selected_variables.copy())\n",
        "    else:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7FQXO5kl6HA"
      },
      "source": [
        "fig = plt.figure(figsize=(10,10))\n",
        "fig.set_facecolor('white')\n",
        " \n",
        "font_size = 15\n",
        "plt.xticks(steps,[f'step {s}\\n'+'\\n'.join(sv_per_step[i]) for i,s in enumerate(steps)], fontsize=12)\n",
        "plt.plot(steps, adjusted_r_squared, marker='o')\n",
        "    \n",
        "plt.ylabel('Adjusted R Squared',fontsize=font_size)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJP7mDNAl6KH"
      },
      "source": [
        "train_1_tmp = train_1[selected_variables]\n",
        "train_1_tmp['중식계'] = train_1['중식계']\n",
        "train_1 = train_1_tmp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWqcYAsjl6OC"
      },
      "source": [
        "## 전진 단계별 선택법 - 석식\n",
        "dinner_cols = train_2.columns.tolist()\n",
        "dinner_cols.remove('석식계')\n",
        "dinner_cols.remove('일자')\n",
        "variables = dinner_cols ## 설명 변수 리스트\n",
        " \n",
        "y = train_2['석식계'] ## 반응 변수\n",
        "selected_variables_dinner = [] ## 선택된 변수들\n",
        "sl_enter = 0.05\n",
        "sl_remove = 0.05\n",
        " \n",
        "sv_per_step = [] ## 각 스텝별로 선택된 변수들\n",
        "adjusted_r_squared = [] ## 각 스텝별 수정된 결정계수\n",
        "steps = [] ## 스텝\n",
        "step = 0\n",
        "while len(variables) > 0:\n",
        "    remainder = list(set(variables) - set(selected_variables_dinner))\n",
        "    pval = pd.Series(index=remainder) ## 변수의 p-value\n",
        "    ## 기존에 포함된 변수와 새로운 변수 하나씩 돌아가면서 \n",
        "    ## 선형 모형을 적합한다.\n",
        "    for col in remainder: \n",
        "        X = train_2[selected_variables_dinner+[col]]\n",
        "        X = sm.add_constant(X)\n",
        "        model = sm.OLS(y,X).fit()\n",
        "\n",
        "        pval[col] = model.pvalues[col]\n",
        " \n",
        "    min_pval = pval.min()\n",
        "    if min_pval < sl_enter: ## 최소 p-value 값이 기준 값보다 작으면 포함\n",
        "        selected_variables_dinner.append(pval.idxmin())\n",
        "        ## 선택된 변수들에대해서\n",
        "        ## 어떤 변수를 제거할지 고른다.\n",
        "        while len(selected_variables_dinner) > 0:\n",
        "            selected_X = train_2[selected_variables_dinner]\n",
        "            selected_X = sm.add_constant(selected_X)\n",
        "            selected_pval = sm.OLS(y,selected_X).fit().pvalues[1:] ## 절편항의 p-value는 뺀다\n",
        "            max_pval = selected_pval.max()\n",
        "            if max_pval >= sl_remove: ## 최대 p-value값이 기준값보다 크거나 같으면 제외\n",
        "                remove_variable = selected_pval.idxmax()\n",
        "                selected_variables_dinner.remove(remove_variable)\n",
        "            else:\n",
        "                break\n",
        "        \n",
        "        step += 1\n",
        "        steps.append(step)\n",
        "        adj_r_squared = sm.OLS(y,sm.add_constant(train_2[selected_variables_dinner])).fit().rsquared_adj\n",
        "        adjusted_r_squared.append(adj_r_squared)\n",
        "        sv_per_step.append(selected_variables_dinner.copy())\n",
        "    else:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jc4NgqVEl-wE"
      },
      "source": [
        "fig = plt.figure(figsize=(10,10))\n",
        "fig.set_facecolor('white')\n",
        " \n",
        "font_size = 15\n",
        "plt.xticks(steps,[f'step {s}\\n'+'\\n'.join(sv_per_step[i]) for i,s in enumerate(steps)], fontsize=12)\n",
        "plt.plot(steps, adjusted_r_squared, marker='o')\n",
        "    \n",
        "plt.ylabel('Adjusted R Squared',fontsize=font_size)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4P4WL--4mAbw"
      },
      "source": [
        "train_2_tmp = train_2[selected_variables_dinner]\n",
        "train_2_tmp['석식계'] = train_2['석식계']\n",
        "train_2 = train_2_tmp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fh5eAD5nmBud"
      },
      "source": [
        "# test 데이터\n",
        "cols_lunch = selected_variables\n",
        "cols_dinner = selected_variables_dinner\n",
        "test_lunch = test[cols_lunch]\n",
        "test_dinner = test[cols_dinner]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLHhl6zpTxUy"
      },
      "source": [
        "reg = setup(data=train_1,\n",
        "            target='중식계',\n",
        "            numeric_imputation = 'mean',\n",
        "            normalize = True,\n",
        "            silent= True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDOqdaKWT6xm"
      },
      "source": [
        "best_5 = compare_models(sort='MAE', n_select=5)\n",
        "blended = blend_models(estimator_list= best_5, fold=5, optimize='MAE')\n",
        "pred_holdout = predict_model(blended)\n",
        "final_model = finalize_model(blended)\n",
        "pred1 = predict_model(final_model, test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCdlhjgnU39F"
      },
      "source": [
        "sample_submission = pd.read_csv(path+'sample_submission.csv')\n",
        "submission = sample_submission.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1fj6sjNVInL"
      },
      "source": [
        "submission['중식계'] = pred1.reset_index()['Label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KblKSlEIUTm"
      },
      "source": [
        "reg = setup(data=train_2,\n",
        "            target='석식계',\n",
        "            numeric_imputation = 'mean',\n",
        "            normalize = True,\n",
        "            silent= True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y42zsb9KIXPE"
      },
      "source": [
        "best_5 = compare_models(sort='MAE', n_select=5)\n",
        "blended = blend_models(estimator_list= best_5, fold=5, optimize='MAE')\n",
        "pred_holdout = predict_model(blended)\n",
        "final_model = finalize_model(blended)\n",
        "pred2 = predict_model(final_model, test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPmlmWLBIXGG"
      },
      "source": [
        "submission['석식계'] = pred2.reset_index()['Label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFfBt8X5VKNr"
      },
      "source": [
        "sub_path = '/content/drive/MyDrive/DACON/Dacon_Industry_Meal/submit/'\n",
        "best_submit = pd.read_csv(sub_path+'20210605_01_79.csv')\n",
        "df_82 = pd.read_csv(sub_path+'20210608_01_holiday_82.csv')\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "def show_mae(data) : \n",
        "    result = mean_absolute_error(best_submit['중식계'], data['중식계'])+mean_absolute_error(best_submit['석식계'], data['석식계'])\n",
        "    return display(result)\n",
        "\n",
        "show_mae(submission)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-RXYMOBZA3O"
      },
      "source": [
        "submission.to_csv(sub_path+'/20210619_02.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
>>>>>>> f39b472e0daa7e1c9c7149668657a10c70147de5
